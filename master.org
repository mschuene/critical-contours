#+T1ITLE:Masterarbeit 
#+AUTHOR: Maik Schünemann
#+email: maikschuenemann@gmail.com
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  en
#+OPTIONS:   H:10 num:t toc:nil :nil @:t ::t |:t ^:t -:t f:t *:t <:t d:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil todo:t pri:nil tags:not-in-toc tasks:nil 
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+OPTIONS: d:(not "exploration")
#+EXCLUDE_TAGS: exploration
#+EXCLUDE_TAGS: implementation
#+OPTIONS: d:(not "exploration" "implementation")
#+LINK_UP:   
#+LINK_HOME:
#+TAGS:  BlowerDoor(b) Suub(s) Uni(u) Home(h) Task(t) Note(n) Info(i) noexport(e)
#+TAGS: Changed(c) Project(p) Reading(r) Hobby(f) OpenSource(o) Meta(m) implementation(p)
#+SEQ_TODO: TODO(t) STARTED(s) WAITING(w) APPT(a) NEXT(n) | DONE(d) CANCELLED(c) DEFERRED(f) 
#+STARTUP:showall
#+LaTeX_CLASS: bachelorarbeit
#+LATEX_HEADER: \usepackage{amsmath,amscd,dsfont}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{bbm}
#+LATEX_HEADER: \usepackage{breqn}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{epstopdf}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{pdflscape}
#+LATEX_HEADER: \newtheorem{thm}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{lem}[thm]{Lemma}
#+LATEX_HEADER: \newtheorem{prop}[thm]{Proposition}
#+LATEX_HEADER: \newtheorem*{cor}{Corollary}
#+LATEX_HEADER: \theoremstyle{definition}
#+LATEX_HEADER: \newtheorem{defn}{Definition}[section]
#+LATEX_HEADER: \newtheorem{conj}{Conjecture}[section]
#+LATEX_HEADER: \newtheorem{exmp}{Example}[section]
#+LATEX_HEADER: \theoremstyle{remark}
#+LATEX_HEADER: \newtheorem*{rem}{Remark}
#+LATEX_HEADER: \newtheorem*{note}{Note}
#+LATEX_HEADER: \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
#+LATEX_HEADER: \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{titlepage}
\pagestyle{empty}
\begin{center}  
%\vspace*{1cm}  
    \includegraphics[width=0.3\textwidth]{Universitaet_Bremen.png} 
    \vspace*{1cm}
 \large 
\\
    {\Large\textbf{Master Thesis}}\\
    \vspace*{1cm}
    \textbf{Mathematics}\\
    \textit{Research Group Dynamical Systems and Geometry}\\
    \vspace{0.5cm}
    \textbf{Neurosciences}\\
    \textit{Department of Theoretical Neurophysics}\\
    \vspace{1cm}

    {\LARGE \textbf{Feature integration with critical dynamics}}\\
    {\large \textbf{numerical studies and mathematical analysis}}\\
    \vspace{1cm}
    \includegraphics[width=150px]{/home/kima/Dropbox/uni/master/images/titleimage.pdf}
    \vspace{1cm}
  \end{center}
\begin{center}
{\large
\begin{tabular}[h]{ll}
\doublespacing
    \textbf{Author:} &  Maik Schünemann\\
    \textit{E-Mail:} &  mschuene@uni-bremen.de\\
    \textit{Matr.-No.:} & 2926336 \\ 
    \textit{Address:} & Kirchbachstr. 201a \\
    & 28211 Bremen\\
\\
    \textbf{Thesis advisor Mathematics:} & Prof. Dr. Marc Keßeböhmer\\
  \\

    \textbf{Thesis advisor Neurosciences:} & Dr. Udo Ernst\\
  \\
    \textbf{Submission date:}  & \today
\end{tabular}}
\end{center}
\end{titlepage}

\newpage
\thispagestyle{empty}
\mbox{}


\begin{figure}[htp]
\centering
\includegraphics[width=1.0\textwidth]{/home/kima/Dropbox/uni/master/DeclarationsThesis_final.pdf}
\end{figure}
\clearpage{}
\thispagestyle{empty}
\newpage
\thispagestyle{empty}
\mbox{}
\tableofcontents
\pagenumbering{arabic}

* Setup of Development environment     :exploration:
  :PROPERTIES:
  :header-args: :tangle src/utils.py
  :END:

  #+begin_src ipython :session master :tangle no
  %matplotlib inline
  #+end_src 

  #+RESULTS:

  #+begin_src ipython :session master
  1+1
  #+end_src 

  #+RESULTS:
  : 2



  #+begin_src ipython :session master :tangle no :results output silent :exports code
     %load_ext memory_profiler
     %load_ext line_profiler
#     %matplotlib inline
     # %load_ext wurlitzer
     import sh
     try:
         sh.cd('src')
     except:
         pass
     from ehe import *;
     from utils import *;
     from plfit_cd import *;
     from ehe_subnetworks import *; 
  #+end_src 


  #+RESULTS:

     
  To have a nice edit and reload cycle without the need to manually
  compile the program, cppimport is used to load the module. It
  automatically detects file changes, however python has a known
  limitation that it cannot reload a native extension. In order to
  not having to restart the whole python process with the whole
  workspace in it, this helper function reloads the module under a
  different name.

  In order to efficiently run the c++ code on the cluster, an
  optimized blas library is installed on each machine and thus the
  native code should be compiled on each host befor it is run there.
  The home directory is synchronized between nodes in the cluster so
  to avoid interaction issues, a different output file is written for
  each node using its hostname as part of the suffix.

  #+begin_src ipython :session master
     import sys
     from wurlitzer import sys_pipes
     sys.path.insert(0,'./cpp')

     from cppimport import imp
     from time import gmtime, strftime
     import sys
     from cppimport.find import find_module_cpppath
     from cppimport.importer import setup_module_data
     from cppimport.importer import should_rebuild
     import os.path
     import os
     def load_module(fullname):
         name = find_module_cpppath(fullname)
         hostname = os.uname()[1].replace("-","_")
         suffix = hostname+strftime("%Y%m%d%H%M%S",gmtime())
         (d,fname) = os.path.split(name)
         rendered_file_prefix = '.rendered.'+fname.split(".cpp")[0]
         with open(name) as f:
             content = f.read()
             new_content = content.replace("__module_suffix__",suffix)
         last_suffixes = [f.split(rendered_file_prefix)[-1].split('.cpp')[0]
                          for f in os.listdir(d) if f.startswith(rendered_file_prefix)]
         last_suffixes = [s for s in last_suffixes if s.startswith(hostname)]
         if len(last_suffixes) != 0:
             last_suffix = max(last_suffixes)
             with open(d+"/"+fullname+last_suffix+'.cpp','w') as flr:
                 flr.write(content.replace("__module_suffix__",last_suffix))
             module_data = setup_module_data(fullname+last_suffix, name.split('.cpp')[0]+last_suffix+'.cpp')
             if not(should_rebuild(module_data)):
                 return imp(fullname+last_suffix)
             for key in ['ext_path','filepath']:
                 try:
                   os.remove(module_data[key])
                 except:
                   pass
             try:
                 pass
                 #os.remove(d+"/"+'.rendered.'+fullname+last_suffix+'.cpp')    
                 #os.remove(d+"/"+'.'+fullname+last_suffix+'.cpp.cppimporthash')
             except:
               pass
         with open(d+"/"+fullname+suffix+".cpp",'w') as f2:
             f2.write(new_content)
         print('reloading module')
         with sys_pipes():
             res = imp(fullname+suffix)
         return res
  #+end_src 

  #+RESULTS:

** Setup on the cluster machines

   The sun grid engine can be called with the qsub commands, providing
   different queues that consist of 67+ nodes with synchronized home
   folder between them. A number of programs can be run on the
   machines for example by the following script. These nodes are very
   heterongenous ranging from pretty old computers to 24 core
   fatbastards with 500gb

** start jobs on the cluster
   
   Mako template to start python scripts for this thesis using qsub.

   It takes care of generating appropriate directory structures etc,
   which are customizable by python variables.

   The source code always runs in the src directory and has common
   imports as well as the variables task_id and outdir.

   To fully support job arrays, facilities to post process the
   output of the instances of the job array are provided using a
   second qsub started process that is on hold until the job array
   has finished executing.
   
   #+BEGIN_SRC sh :tangle src/util/cluster_template.sh :eval no 
       #!/bin/bash
       cd "$(dirname "$0")"
       mkdir ${name}
       cd ${name}
       jobid=$(qsub -terse\
        -N ${name}\
       % if queue in ['short','short_64gb','long','long_64gb','maximus','maximus+long_64gb','maximus+long']:
        -q `cat /0/maik/queue_${queue}.txt`\
       % else:
        -q ${queue}\
       % endif
        -cwd `#execute in current directory`\
        -p ${priority}\
       % if mail is not None:
        -M ${mail}\
        -m a `#end abort suspend`\
       % endif
       % if max_threads > 1:
        -pe omp ${max_threads}\
       % endif
        -t ${task_ids}\
        <<EOT
       # source .bashrc since it isn't sourced by default
       source $HOME/.bashrc
       % if max_threads != 0:
       export OMP_NUM_THREADS=${max_threads}
       % endif
       export QT_QPA_PLATFORM=offscreen
       ${interpreter} <<EOF
       # common setup code
       import matplotlib
       matplotlib.use('Agg')
       import pickle 
       import os
       import sh
       sh.cd("/home/maik/master/src")
       outdir = "../avalanches/${name}"
       try:
           os.mkdir(outdir)
       except:
           pass
       print(os.environ,flush=True)
       task_id = int(os.environ.get('SGE_TASK_ID'))
       from ehe import * 
       from ehe_ana import * 
       from utils import * 
       from plfit_cd import * 
       from ehe_subnetworks import *
       ${content}
       EOF
       EOT
       )

       echo "submitted job with id $jobid"
       jid=`echo $jobid | cut -d. -f1`
       qsub\
        -N ${"postprocess"+name}\
       % if queue in ['short','short_64gb','long','long_64gb','maximus']:
        -q `cat /0/queue_${queue}.txt`\
       % else:
        -q ${queue}\
       % endif
        -cwd `#execute in current directory`\
        -p ${priority}\
       % if mail is not None:
        -M ${mail}\
        -m ea `#end abort suspend`\
       % endif
        -l h_rt=00:00:15 -hold_jid $jid <<EOT
       source $HOME/.bashrc
       ${interpreter} <<EOF
       % if postprocessing_content != '':
       # common setup code
       import pickle
       import os
       import sh
       sh.cd("/home/maik/master/src")
       outdir = "../avalanches/${name}"
       try:
           os.mkdir(outdir)
       except:
           pass
       print(os.environ,flush=True)
       from ehe import * 
       from utils import * 
       from plfit_cd import * 
       from ehe_subnetworks import *
       ${postprocessing_content}
       % endif
       EOF
       EOT

       qstat
   #+END_SRC

   #+RESULTS:

   This file is rendered with the mako template library
   #+begin_src ipython :session master
       from mako.template import Template
       from mako.runtime import Context
       from io import StringIO
       from os.path import basename
       from os.path import splitext

       def render_cluster_template(**opts):
           default_options={'queue':'long_64gb',
                            'mail':'maikschuenemann@gmail.com',
                            'output':'o',
                            'error':'e',
                            'priority':0,
                            'task_ids':'1',
                            'content':"print('hi')",
                            'max_threads':1,
                            'postprocessing_content':"",
                            'interpreter':'python',
                            'template_file':"./util/cluster_template.sh",
                            'output_file':None}
           options = default_options.copy()
           options.update(opts)
           if 'name' not in options and options['output_file'] is not None:
               options['name'] = splitext(basename(options['output_file']))[0]
           template = Template(strict_undefined=True,filename=options['template_file'])
           output = StringIO()
           ctx = Context(output,**options)
           template.render_context(ctx)
           if options['output_file'] is not None:
               with open(options['output_file'],"w") as f:
                   f.write(output.getvalue())
           return output.getvalue()
   #+end_src 

   #+RESULTS:

   In order to submit sun grid engine jobs in the server, the
   appropriate servername has to be chosen and in addition SGE_xxx
   environment variables have to be set.
     
   #+begin_src ipython :session master
       import sh
       def ssh_cluster(servername='server_inline'):
           return sh.ssh.bake("-t",servername,
           "export GE_CELL=neuro;export SGE_ROOT=/sge-root; export SGE_CLUSTER_NAME=OurCluster;cd /home/maik/master/src;")

       server= ssh_cluster('server')
       server_inline = ssh_cluster()
   #+end_src 

   #+RESULTS:


   Synchronize local repository with home directory on server
   #+begin_src ipython :session master
       def rsync_server(server='server_inline'):
           print('rsyncing server')
           print(sh.rsync('-rtvu','./',server+":/home/maik/master/src/"))
           #print(sh.rsync('-rtvu','../avalanches/',server+":/home/maik/master/avalanches/"))
           print('done')
   #+end_src 

   #+RESULTS:


   Execute python commands on the grid engine with this
   function. Created shell files that are run by the grid engine
   are stored in /src/cluster/<name>.sh if name is given and not indicated otherwise.
     
   #+begin_src ipython :session master
       def qsub(command="print('hi')",post_command='',name=None,outfile=None,servername='server_inline',execute=True,**options):
           ssh = ssh_cluster(servername)
           options['content'] = command
           options['postprocessing_content']=post_command
           if name is not None:
               options['name'] = name
               if outfile is None:
                   outfile = "./cluster/"+name+".sh"
                   options['output_file'] = outfile
           elif outfile is not None:
               options['output_file'] = outfile
           else:
               raise Exception('must be called with either name or outfile')
           rendered = render_cluster_template(**options)
           rsync_server(servername)
           if execute:
               print(ssh("sh "+options['output_file']))
   #+end_src 

   #+RESULTS:

   #+RESULTS:/

   Results are to be stored in the ./avalanches subdirectory on the
   server. This helper function transfers a result back to the local
   ./avalanches directory on

   #+begin_src ipython :session master 
        def scp_result(filename,servername='server_inline'):
           outfile = '../avalanches/'+filename 
           try:
              os.makedirs(os.path.dirname(outfile))
           except:
                pass
           print(sh.scp(servername+':/home/maik/master/avalanches/'+filename,outfile))
   #+end_src 

   #+RESULTS:



*** Post-Commands for Job Arrays

    To fully support job arrays, facilities to post process the
    output of the instances of the job array are provided using a
    second qsub started process that is on hold until the job array
    has finished executing.
      
    Probably the simplest use case of a job array is to split the
    generation of a large dataset over the nodes on the cluster.
    Each job writes it's data in a file with the suffix indicating
    it's task_id. $post-command-concat$ forms the complete array from
    the outputs of the instances of the job array and notes which
    parts are missing i.e. which instances of the job array have failed.
      
    #+begin_src ipython :session master
       import os
       from os.path import basename
       from os.path import splitext
       import numpy as np
       def post_command_concat(prefix,tid_range,axis=0):
           dname,bname = os.path.split(prefix)
           fname = splitext(bname)[0]
           arrays = [];
           failed = "";
           for tid in tid_range:
               try:
                   arrays.append(np.load(os.path.join(dname,fname+str(tid)+".npy")))
               except:
                   failed += str(tid)+"\n"
           post_array = np.concatenate(arrays,axis=axis)
           np.save(os.path.join(dname,fname+"concatenated.npy"),post_array)
           with open(os.path.join(dname,'failed_idx.txt'),'w') as f:
               f.write(failed)
    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master :tangle src/utils.py
        import os
        from os.path import basename
        from os.path import splitext
        from os.path import join
        import numpy as np

        def concat_detailed(avs_prefix,avs_inds_prefix,tid_range):
            dname,bname = os.path.split(avs_prefix)
            fname = splitext(bname)[0]
            fname_inds = splitext(basename(avs_inds_prefix))[0]
            concatenated_avs = []
            concatenated_avs_inds = []
            failed = ""
            for tid in tid_range:
                try:
                    avs_tid = np.load(join(dname,fname+str(tid)+".npy"))
                    avs_inds_tid = np.load(join(dname,fname_inds+str(tid)+".npy")) + len(concatenated_avs)
                    concatenated_avs.extend(avs_tid)
                    concatenated_avs_inds.extend(avs_inds_tid)
                except:
                    import traceback
                    traceback.print_exc()
                    failed += str(tid)+"\n"
            concatenated_avs = np.array(concatenated_avs)
            concatenated_avs_inds = np.array(concatenated_avs_inds)
            np.save(join(dname,fname+"concatenated.npy"),concatenated_avs)
            np.save(join(dname,fname_inds+"concatenated.npy"),concatenated_avs_inds)
            with open(os.path.join(dname,'failed_idx.txt'),'w') as f:
                f.write(failed)
            return (concatenated_avs,concatenated_avs_inds)

    #+end_src 

    #+RESULTS:





* Introduction

  The brain constantly receives informations about its environment
  from the sense organs. Perception requires that this sensory
  informations are integrated by the brain into a representation of
  the world citep:23358. According to Gestalt psychology, perception
  is governed in the visual system by Gestalt laws stating that
  localized stimuli are grouped together by principles such as proximity,
  similarity, common fate and good continuation
  citep:wertheimer1923untersuchungen,wagemans2012century.
  
  Particular examples of perceptual organization are (visual) grouping
  and figure-background organization. Grouping integrates the features
  of the visual scene into wholes (/Gestalten/), and figure-background
  organization assigns a particular interpretation to them
  [[cite:wagemans2012century][Section 3]].
  
  These processes are highly complex and require sophisticated computational
  capabilities from the underlying neurophysiological mechanisms
  and a high robustness to background noise. 

  Theoretical considerations show that in complex dynamical systems,
  computational power is maximized near a phase transition between
  chaos and order which is called the critical state. This was first demonstrated for Boolean networks
  and cellular automata
  citep:packard1988adaptation,LANGTON199012,KAUFFMAN1991467,kauffman1993origins
  and more recently for neural network models (see, for example,
  cite:realtimecomputationedgeofchaos,LEGENSTEIN2007323). 

  An intuition for this findingis given by the following
  considerations citep:shew2013functional. The dynamics of a
  subcritical (network) sytem are very unstructured and correlations
  between nodes decay exponentially thus information can not propagate
  far in the network. In a supercritical system however, initial
  activations quickly spread and engage the whole system, leading to
  'epilleptic' oscillatory states regardless of the initial
  conditions. The phase transition between these states where initial
  activation neither blows up nor dies down is termed the critical
  state. At such a phase transition, the system produces population
  events (for example avalanches in sandpile models or avalanches of
  spike propagation in neural network models) which are distributed
  according to a power law
  citep:bak1987self,eurich2002finite,levina2007dynamical,BUICE200953.

  Criticality in neural system is not only hypothesized in abstract
  neuron models but is observed in biological neural systems.
  cite:beggs2003neuronal measured power law avalanche statistics with
  slope \(-3/2\) in organotypic cultures from coronal slices of rat
  somatosensory cortex and acute slices using an \(8\times 8\)
  electrode array. After application of the
  \(\text{GABA}_{\text{A}}\)-receptor antagonist picrotoxin (weakening
  inhibition in the slice) epileptic discharges occurred indicative of
  a supercritical state. After 24hr recovery the avalanche size
  distribution returned to a power law of the same slope. This
  indicates that the neural system showed a form self organization
  towards this critical state. Since then evidence for criticality has
  been found in /vitro/ and in /vivo/
  citep:petermann2009spontaneous,ribeiro2010spike although some
  studies found evidence against criticality
  citep:bedard2006does,10.1371/journal.pone.0008982 or report
  marginal subcriticality
  citep:priesemann2014spike,tomen2014marginally. A simple conjecture
  would be that the brain is not always in a critical state but uses
  criticality only for certain tasks. Indeed, cite:hahn2017spontaneous
  provide evidence that different vigilance states are associated with
  different signs of criticality and they link critical states to
  synchronized cortical states.

  However, aside from maximizing measures of computational power like
  information entropy and dynamical range as described above, concrete
  proposals for potential benefits of criticality for a typical computaitonal task
  the brain has to solve are lacking.

  In this thesis we propose a link between feature integration and
  criticality: 

  In particular we show that it is possible to embed subnetworks
  representing combinations of features belonging to a particular
  'Gestalt' or 'figure' into a larger network of Eurich-Herrmann-Ernst
  neurons citep:eurich2002finite. This embedded subnetworks
  representing a perceptual figure display critical dynamics upon
  activation while subnetworks consisting of randomly chosen units not
  belonging to a particular figure (or the system as a whole) stay
  subcritical. We show that with this embedding algorithm we achieve a
  high capacity to embed a large number of figures with the desirable
  properties that neurons can simultaneously belong to several figures
  before the network becomes supercritical. In addition to well
  separated avalanche size statistics for these two modes of
  figure/target vs background activation fast classification is
  possible using a coincidence detector. We show with parametric
  studies that the findings are valid over a wide region of parameter
  space and robust to background noise and changes in the parameters
  of the embedding algorithm.

  The Eurich-Herrmann-Ernst (in the following abbreviated by EHE)
  model is of special interest since the coupling strength leading to
  critical power law avalanche size distributions are analytically
  derived in cite:eurich2002finite and extensions of this model have
  been successfully used as models for criticality in neural networks
  citep:levina2008mathematical,levina2007dynamical,levina2007criticality.

  In this thesis we extend the mathematical understanding of the
  EHE model. In particular we validate the derivations of the
  avalanche statistics in cite:eurich2002finite by proving ergodicity
  based on the theory developed in [[cite:levina2008mathematical][Chapter 7]].

  In addition we take a first step towards a comprehensive mathematical analysis of
  criticality in subnetworks by extending the mathematical analysis of
  the EHE model allowing for a general non-negative coupling matrix.
  We show that the qualitative properties of the EHE model dynamics
  generalize to this case and we extend the derivation of avalanche
  probabilities given in cite:eurich2002finite to this more general case.

  # Perception is highly nonlinear and what enters our
  # awareness is not just the sum of features represented in the sensory
  # input and the whole is grasped before its constituents enter
  # consciousness.
  

  # While the original physiological models used to explain this effects
  # were proven wrong in the 1950s
  # (cite:lashley1951examination,sperry1955visual) these gestalt
  # principles remain relevant and are studied using neurophysiological
  # experiments and computational models (for a review see
  # cite:wagemans2012century).


  This thesis consists of two main parts. 

  In chapter [[ref:chap:fics]] we provide evidence with simulation
  studies, that figure subnetworks, which display critical activation
  provide potential computational benefits for feature integration in
  the brain. Starting from a review of the EHE model used in this
  study we motivate and analyze the capacity of an algorithm
  constructing networks with embedded (critical) figure networks. We
  test the separability of avalanche size distributions between figure
  and background activation schemes as well as the separability using
  a coincidence detector in a simulated 2-alternative forced choice
  finding good performance over a wide region in parameter space which
  are qualitatively robust against changes in the embedding parameter
  and higher levels of background noise.


  In Chapter [[ref:chap:gen-ehe]] we generalize the mathematical
  formulation of the EHE model as a skew-product dynamical system
  [[cite:levina2008mathematical][Chapter 7]] allowing for a general
  nonnegative weight matrix. We are able to completely characterize
  the behavior of the dynamical system and show that the main
  qualitative properties of the homogeneous ehe system generalize to
  this more general setting. In addition we show that the EHE model
  can be understood as a skewed random walk on the torus and proof the
  ergodicity for the homogeneous case.



  
  # Starting from the
  #   description of the simplified EHE neuron model and how critical
  #   avalanche distributions emerge in this model (section [[ref:sec:pls]])
  #   we introduce a simple scheme to embed critical subnetworks, illustrate
  #   the generated weight matrices and show that it is possible to embed
  #   a large number of subnetworks so that on average each unit belongs
  #   to several subnetworks and investigate the maximal capacities of
  #   this approach in dependence of embedding parameters (section
  #   [[ref:sec:embedding]]).
  
  # We then test this approach with a simulated 2-alternate forced
  # choice task (section [[ref:sec:2afc-description]]) and observe well
  # separated avalanche size statistics for the /Target/ vs. the
  # /Distractor/ stimulus over a large region of parameter space with
  # the avalanche size statistic of the /Target/ remaining critical
  # (section [[ref:sec:2afc-avalanches]]). Going beyond observing separated
  # avalanche size statistics we show that a simple coincidence detector
  # integrating the number of synchronous events achieses high
  # performance in this 2-AFC task even in regimes where rate based
  # classifier would only achieve chance level (section
  # [[ref:sec:coincidence-detector-performance]]). Throughout, we peform
  # careful parametric studies to show the stability of this approach
  # with respect to embedding parameters, number and size of
  # subnetworks, amount of background noise and strength of inhibitory
  # connections (section [[ref:sec:last]]).

  # The avalanche size statistics for the EHE network can be calculated
  # analytically making it an interesting model to study aspects of
  # criticality from a theoretical perspective. In the second part of
  # this thesis we contribute an important step towards an analytical
  # understanding of the generalized EHE-Models used in the modeling
  # studies. In particular we will assess ergodic properties of the
  # system and derive the probability space of avalanches for any
  # nonnegative weight matrix.

  # Starting with a formulation of the generalized system (section
  # gen_ehe_physical) as a skew-product dynamical system (section
  # skew-product) we show that outside of a region with vanishing
  # density of states the system acts bijectively and has uniform
  # equilibrium density. In section bla this noninhabited region is
  # identified, and it is shown that the volume can be written as a sum
  # of subdeterminants. The bijectivity on the remaining system is shown
  # in region blablabla and the regions in the remaining phase space
  # leading upon receiving external input to avalanches are identified
  # in section blub. The bijectivity directly leads to uniform
  # equilibrium density. Going beyond that we perform a careful prove of
  # the topological transitivity of the original EHE model originally
  # proven in cite:levina2008mathematical. We show that this guarantees
  # the ergodicity of the system thus enabling to calculate the
  # avalanche probabilities by the volume of regions leading to them in
  # phase space. We conjecture the transitivity to be true for the
  # generalized Model under certain conditions of the weight matrix. The
  # probability space of avalanche statistics is then finally
  # constructed in section blblblbladkf. Numerically we study the
  # convergence of this theoretical avalanche statistics to empirical
  # statistics from model simulations. Finally we give a first
  # application of this framework derived here to rigorously derive the
  # closed form avalanche size distribution of the original EHE model
  # reported and derived already in cite:eurich2002finite.


* Simulation for Bernstein Conference                        :implementation:
  :PROPERTIES:
  :header-args: :tangle src/ehe_subnetworks.py
  :END:





  Either only one net active or equivalent random background units. 
      - 1000 units
      - 10000 avalanches
      - subnets of size 100
      - 50 ensembles of subnets

        very simple weight matrix - constant 100-units critical
        connectivity recurrentlyin each ensemble and \(-\beta \) weight
        between. \(\beta \) should be chosen in this way, that if one
        network of size 100 is activated, the combined inhibition
        represses all other units, but that randomly selected background
        units don't manage the samen

        #+begin_src ipython :session master
          def embed_ensembles(N_tot=1000,N_subunits=100,N_ensembles=50):
              return [np.random.choice(np.arange(N_tot),size=N_subunits,replace=False) for i in range(N_ensembles)]

          def simple_submatrix(N_tot,ensembles,beta=2,alpha_factor=1):
              W = np.zeros((N_tot,N_tot))
              for ens in ensembles:
                  W[np.ix_(ens,ens)] = ehe_critical_weights(len(ens))*alpha_factor
              N = len(ens)
              W[W == 0] = -beta*(1 - 1/np.sqrt(N))/N
              return W
        #+end_src 

        #+RESULTS:



      Simulate a two alternative forced choice task. For 10 realizations
      of subnetwork embeddings and 10 choices of activated subnetwork
      and random background elements, record model dynamics when
      presented an activated subnetwork (A) + K random background
      units and when only the same number of random background units are
      activated (B).
    
      #+begin_src ipython :session master
      def sim_2afc(N_s=100,N_e=50,K=0,beta=2,N_u=1000,num_ens=10,num_act=10,alpha_factor=1,background_contrast=1,figure_factor=1):
          ensembles = [embed_ensembles(N_tot=N_u,N_subunits=N_s,N_ensembles=N_e) for i in range(num_ens)]
          e = load_module('ehe_detailed').EHE()
          sim_res = {'ensembles':ensembles,'sim_res':[None]*num_ens,'N_s':100,'N_e':50,'K':0,'beta':2,'N_u':1000,'num_ens':10,'num_act':10}
          for i,ens in enumerate(ensembles):
              W = simple_submatrix(N_u,ens,beta=beta,alpha_factor=alpha_factor)
              figure_units = [ens[index] for index in np.random.choice(N_e,num_act,replace=False)] #ensembles does not have to be numpy array
              noise_units = [np.random.choice(list(set(range(N_u)) - set(fig_u)),K,replace=False) for fig_u in figure_units]
              background_units = [np.random.choice(list(set(range(N_u)) - set(noise_u)),N_s,replace=False) for noise_u in noise_units]
              # Simulate A: figure activation
              sim_res_a = []
              for fig_u,noise_u in zip(figure_units,noise_units):
                  external_weights = np.zeros(W.shape[0],dtype=int)
                  external_weights[noise_u] = 1
                  external_weights[np.random.choice(fig_u,int(figure_factor*len(fig_u)),replace=False)] = 1
                  #external_weights[np.hstack((np.array(fig_u),noise_u))] = 1
                  e.simulate_model_mat(np.random.random(N_u),10*N_u,W,deltaU,external_weights)
                  sim_res_a.append((e.avs,e.avs_inds,e.act_inds))
              # Simulate B: background activation
              sim_res_b = []
              for background_u,noise_u in zip(background_units,noise_units):
                  external_weights = np.zeros(W.shape[0],dtype=int)
                  external_weights[noise_u] = 1
                  external_weights[background_u] = background_contrast
                  e.simulate_model_mat(np.random.random(N_u),10*N_u,W,deltaU,external_weights)
                  sim_res_b.append((e.avs,e.avs_inds,e.act_inds))
              sim_res_i = {'figure_units':figure_units,'noise_units':noise_units,'background_units':background_units,
                           'sim_res_a':sim_res_a,'sim_res_b':sim_res_b,'W':W,'ens':ens,'background_contrast':background_contrast}        
              sim_res['sim_res'][i] = sim_res_i
          return sim_res
      #+end_src 

      #+RESULTS:

      Cluster Simulations!
    
      #+begin_src ipython :session master :eval no :tangle no
          qsub("""
          import itertools
          import pickle
          parameters =  [(Ns,Ne,50) for Ns in range(80,120,4) for Ne in range(40,71,2)]
          Ns,Ne,K = parameters[task_id - 1]
          sim_res = sim_2afc(N_s=Ns,N_e=Ne,K=K,beta=2,N_u=1000,num_ens=10,num_act=10)
          pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
          """,name='sim_2afc_parameter_search',servername='server_inline',queue='maximus+long',task_ids='1-160')
      #+end_src 


      #+begin_src ipython :session master :eval no :tangle no
      qsub("""
      import itertools
      import pickle
      parameters =  [(Ns,Ne,50) for Ns in range(80,120,4) for Ne in range(40,71,2)]
      Ns,Ne,K = parameters[task_id - 1]
      sim_res = sim_2afc(N_s=Ns,N_e=Ne,K=K,beta=2,N_u=1000,num_ens=10,num_act=10,alpha_factor=0.9)
      pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
      """,name='sim_2afc_parameter_search_0.9alphacrit',servername='server_inline',queue='maximus+long',task_ids='1-160')
      #+end_src 

      #+begin_src ipython :session master :eval no :tangle no
      qsub("""
      import itertools
      import pickle
      parameters =  [(Ns,Ne,50) for Ns in range(80,120,4) for Ne in range(40,71,2)]
      Ns,Ne,K = parameters[task_id - 1]
      sim_res = sim_2afc(N_s=Ns,N_e=Ne,K=K,beta=2,N_u=1000,num_ens=10,num_act=10,alpha_factor=1.1)
      pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
      """,name='sim_2afc_parameter_search_1.1alphacrit',servername='server',queue='maximus+long',task_ids='1-160')
      #+end_src 
    

      #+begin_src ipython :session master :eval no :tangle no
          qsub("""
          import itertools
          import pickle
          parameters =  [(Ns,Ne,300) for Ns in range(80,120,4) for Ne in range(40,71,2)]
          Ns,Ne,K = parameters[task_id - 1]
          sim_res = sim_2afc(N_s=Ns,N_e=Ne,K=K,beta=2,N_u=1000,num_ens=10,num_act=10)
          pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
          """,name='sim_2afc_parameter_search_k300',servername='server_inline',queue='maximus+long',task_ids='1-160')
      #+end_src 


      #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       import itertools
       import pickle
       parameters =  [(Ns,Ne,100) for Ns in range(80,120,4) for Ne in range(40,71,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = sim_2afc(N_s=Ns,N_e=Ne,K=K,beta=2,N_u=1000,num_ens=10,num_act=10)
       pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       """,name='sim_2afc_parameter_search_k100',servername='server_inline',queue='maximus',task_ids='1-160')
      #+end_src 


      #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       import itertools
       import pickle
       parameters =  [(Ns,Ne,100) for Ns in range(80,93,4) for Ne in range(70,95,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = sim_2afc(N_s=Ns,N_e=Ne,K=K,beta=2,N_u=1000,num_ens=10,num_act=10)
       pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       """,name='sim_2afc_parameter_search_k100_more_params',servername='server_inline',queue='maximus',task_ids='1-160')
      #+end_src 

      #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       import itertools
       import pickle
       parameters =  [(Ns,Ne,200) for Ns in range(80,120,4) for Ne in range(40,71,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = sim_2afc(N_s=Ns,N_e=Ne,K=K,beta=2,N_u=1000,num_ens=10,num_act=10)
       pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       """,name='sim_2afc_parameter_search_k200',servername='server_inline',queue='maximus',task_ids='1-160')
      #+end_src 


      :exploration:

Create ensembles for simulation
#+begin_src ipython :session master :eval no :tangle no
      pickle.dump([embed_ensembles(N_tot=10000,N_subunits=1000,N_ensembles=100) for i in range(10)],open('ensembles_1e4.pickle','wb'))
      pickle.dump(np.random.choice(100,10,replace=False),open('activated_ensembles_1e4.pickle','wb'))
      pickle.dump([np.random.choice(10000,1000,replace=False) for i in range(10)],open('background_units_1e4.pickle','wb'))
#+end_src 

#+RESULTS:

#+begin_src ipython :session master :tangle no :eval no
      qsub("""
      (ens_id,act_id) = [(ens_id,act_id) for ens_id in range(10) for act_id in range(10)][task_id-1]
      ensembles = pickle.load(open('ensembles_1e4.pickle','rb'))
      ens = ensembles[ens_id]
      activations = pickle.load(open('activated_ensembles_1e4.pickle','rb'))
      act = activations[act_id]
      N_tot = 10000
      W = simple_submatrix(N_tot,ens)
      e = load_module('ehe_detailed').EHE()
      e.simulate_model_mat(np.random.random(N_tot),10*N_tot,W,deltaU,np.array(ens[act]))
      np.save(outdir+"/avs"+str(task_id),e.avs)
      np.save(outdir+"/avs_inds"+str(task_id),e.avs_inds)
      """,servername='server',task_ids='1-100',queue='long_64gb',name='simple_submatrix_1e4',max_threads=4)

#+end_src 

#+begin_src ipython :session master :tangle no :eval no
          qsub("""
          (ens_id,act_id) = [(ens_id,act_id) for ens_id in range(10) for act_id in range(10)][task_id-1]
          ensembles = pickle.load(open('ensembles_1e4.pickle','rb'))
          ens = ensembles[ens_id]
          activations = pickle.load(open('background_units_1e4.pickle','rb'))
          act = activations[act_id] 
          N_tot = 10000
          W = simple_submatrix(N_tot,ens)
          e = load_module('ehe_detailed').EHE()
          e.simulate_model_mat(np.random.random(N_tot),10*N_tot,W,deltaU,act)
          np.save(outdir+"/avs"+str(task_id),e.avs)
          np.save(outdir+"/avs_inds"+str(task_id),e.avs_inds)
          """,servername='server_inline',task_ids='1-100',queue='long',name='simple_submatrix_random_1e4_new',max_threads=2)

#+end_src 

Auch für N=10000: 
#+begin_src ipython :session master :eval no :tangle no
      pickle.dump([embed_ensembles() for i in range(10)],open('ensembles.pickle','wb'))
      pickle.dump(np.random.choice(50,10,replace=False),open('activated_ensembles.pickle','wb'))
      pickle.dump([np.random.choice(1000,50,replace=False) for i in range(10)],open('background_units.pickle','wb'))
#+end_src 

#+begin_src ipython :session master :tangle no :eval no
      qsub("""
      (ens_id,act_id) = [(ens_id,act_id) for ens_id in range(10) for act_id in range(10)][task_id-1]
      ensembles = pickle.load(open('ensembles.pickle','rb'))
      ens = ensembles[ens_id]
      activations = pickle.load(open('background_units.pickle','rb'))
      act = activations[act_id] 
      N_tot = 1000
      W = simple_submatrix(N_tot,ens)
      e = load_module('ehe_detailed').EHE()
      e.simulate_model_mat(np.random.random(N_tot),10*N_tot,W,deltaU,act)
      np.save(outdir+"/avs"+str(task_id),e.avs)
      np.save(outdir+"/avs_inds"+str(task_id),e.avs_inds)
      """,servername='server',task_ids='1-100',queue='long_64gb',name='simple_submatrix_random')

#+end_src
      :END:

*** Abstract Bernstein Conference                               :exploration:
    
    Possible title:
    Critical subnetworks for figure-ground separation

    1. übergreifende Wissenschaftliche Fragestellung. 
       - Figure/ground separation bei Konturintegration anhand
         verschiedener dynamischen regimes (oder Phasen)

    2. EHE Modell bietet Zuganz zur Analyse von kritikalität.
       Fragestellung: Kann man kritische Lawinenstatistiken für diese
       Aufgabe verwenden?

    3. Dazu: Einbettung von Teilnetzwerken, welche mögliche figures
       repräsentieren in ein großes Netzwerk.
       - Rekurrente kritische Konnektivität innerhalb der \(N_e \) eingebetteten
         Teilnetzwerke der größe \(N_s \) und Inhibition zu allen restlichen.
       - Wenn die figure präsent ist, bekommen die zu dem entsprechenden
       Teilnetzwerk gehörenden Neuronen externen Input und die
       Lawinenstatistik in diesem Teilnetzwerk wird kritisch werden
       während die anderen Netzwerke subkritisch bleiben.
       - Die kritische Lawinenverteilung entsteht dabei nur wenn man
         ein Teilnetzwerk anregt und nicht die entsprechende Anzahl an
         zufällig ausgewählten Neuronen (Hintergrundrauschen)
       - Die Kritizität der Verteilung im aktivierten Teilnetz soll
         zeigt Robustheit gegenüber zusätzlich aktivierten hintergrund Neuronen \(K \).
       - Auswertung der Lawinenstatistiken in den aktivierten, bzw.
         nicht aktivierten sowie Hintergrundnetzwerken und
         Quantifizierung der Kritikalität mit statistischen goodness
         of fit tests
       - Aber: 'Phasenübergang' bei zu viel Überlapp in dem sich die
         Aktivität hochschaukelt und Lawinen nicht mehr enden.

    4. Ein auslese-Mechanismus zur Erkennung von kritischen Teilnetzen
       wird vorgestellt, welcher in einem gewissen
       beobachtungshorizont anhand der größten beoachteten
       Lawinengröße zwischen figure und background unterscheided.
       - Performance in Abhängigkeit der parameter
*** 2-afc coincidence classifier
    
    The simulations are performed with both figure and background
    activation with 10 realizations of the ensembles and 10 randomly
    selected activation subnetworks/background units. 

    The first step is to abstract the avalanche information from one
    simulation

    #+begin_src ipython :session master
      def extract_2afc_avalanche_sizes_and_times(sim_res):
          avalanche_informations = []
          for ensemble_res in sim_res['sim_res']:
              avalanche_informations.extend(zip(ensemble_res['sim_res_a'],ensemble_res['sim_res_b']))
          return avalanche_informations
    #+end_src 

    #+RESULTS:

    In the next step, the avalanches occuring in a given time horizon
    have to be extracted. For convenience, this step is implemented as
    a resampler in the scikit imblearn framework so that it can be
    used for example in automatic grid searches. 

    #+begin_src ipython :session master
      from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin
      from sklearn.naive_bayes import GaussianNB
      from imblearn.pipeline import Pipeline


      def extract_time_slices(avs,avs_inds,act_inds,t_hor,min_time,max_time=None,max_num_obs=int(1e4)):
          if max_time is None: 
              max_time = act_inds[-1]
          e = load_module('ehe_detailed').EHE()
          act_inds = np.array(act_inds[1:]) # act_inds_always starts with 0
          avs_inds = np.array(avs_inds)
          avs_inds_range = np.searchsorted(act_inds,[min_time,max_time])
          avs_range = avs_inds[avs_inds_range-[0,1]]
          avs_sizes,avs_durations = e.get_avs_size_and_duration(avs[slice(*avs_range)],
                                      avs_inds[slice(*avs_inds_range)] - avs_inds[avs_inds_range[0]])
          start_time,end_time = min_time,act_inds[avs_inds_range[1]-1] # end exclusive
          t_hor_multiples = np.arange(int(start_time/t_hor)+1,int(end_time/t_hor)+1+1)#multiples of t_hor lying in the time range
          splitpoints = np.searchsorted(act_inds[slice(*avs_inds_range)],t_hor_multiples*t_hor)
          # split the avs_sizes at each point where time becomes another multiple of t_hor
          # with searchsorted this also handles 0 observations in a time frame
          observations = np.split(avs_sizes,splitpoints)
          obs = observations if act_inds[-1] >= max_time else observations[-1] # potentially exclude last incompete observation
          return obs[:max_num_obs]

      class TimeHorizonAvalancheExtractor(BaseEstimator):

          def __init__(self,t_hor=100,max_time=110000,min_time=10000):
              self.t_hor = t_hor
              self.max_time = max_time

          def fit(self,trials,y=None):
              pass

          def sample(self,avalanches_2afc,labels=None):
              X,y = ([],[])
              e = load_module('ehe_detailed').EHE()
              for ((avs_1,avs_inds_1,act_inds_1),(avs_2,avs_inds_2,act_inds_2)) in avalanches_2afc:
                  X.append((extract_time_slices(avs_1,avs_inds_1,act_inds_1,self.t_hor,self.min_time,self.max_time),
                            extract_time_slices(avs_2,avs_inds_2,act_inds_2,self.t_hor,self.min_time,self.max_time)))

    #+end_src 

    #+RESULTS:

    The proposed classifier gets two such observations as input and
    has to make a decision which one was generated under figure and
    which one under background stimulation. The classifier bases it's
    decision on the number of avalanches in the observations that are
    bigger than or equal a threshold \(s_0 \). The performance in the 2afc
    design can then be calculated based on the resulting thresholds
    distributions, for each sample how many observations were above
    threshold.

    #+begin_src ipython :session master
      def thresholds(observations,s_0):
          return [np.sum(obs >= s_0) for obs in observations]
    #+end_src 

    #+RESULTS:

**** Exploration of 2afc statistics                             :exploration:

     #+begin_src ipython :session master :tangle no :eval no
       sim_res = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/sim_2afc_parameter_search/sim_res108,48,50.pickle','rb'))
       avs_2afc = extract_2afc_avalanche_sizes_and_times(sim_res)
       (avs1,avs_inds1,act_inds1),(avs2,avs_inds2,act_inds2) = avs_2afc[66]
     #+end_src 

     #+RESULTS:

***** Inter avalanche time
      The inter avalanche time between figure and background activation does not show much differences

      #+begin_src ipython :session master :file ./images/20170820_084813_2591gTi.png :tangle no 
        plt.figure(figsize=(12,8))
        plt.hist([np.diff(avs_inds1),np.diff(avs_inds2)],bins=np.arange(100),histtype='step')
      #+end_src 

      #+RESULTS:
      [[file:./images/20170820_084813_2591gTi.png]]

      Number of observed avalanches in a 20 timestep window figure vs background
      #+begin_src ipython :session master :file ./images/20170821_112344_259168K.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      observations1_20 = extract_time_slices(avs1,avs_inds1,act_inds1,20,10000,max_time)
      observations2_20 = extract_time_slices(avs2,avs_inds2,act_inds2,20,10000,max_time)
      plt.figure(figsize=(12,8))
      plt.hist([[len(obs) for obs in observations1_20],[len(obs) for obs in observations2_20]],histtype='step')
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168K.png]]

      #+begin_src ipython :session master :file ./images/20170821_112344_259168K234.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      observations1_100 = extract_time_slices(avs1,avs_inds1,act_inds1,100,10000,max_time)
      observations2_100 = extract_time_slices(avs2,avs_inds2,act_inds2,100,10000,max_time)
      plt.figure(figsize=(12,8))
      plt.hist([[len(obs) for obs in observations1_100],[len(obs) for obs in observations2_100]],histtype='step')
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168K234.png]]
    
      #+begin_src ipython :session master :file ./images/20170821_112344_259168K234dkaj.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      observations1_1000 = extract_time_slices(avs1,avs_inds1,act_inds1,1000,10000,max_time)
      observations2_1000 = extract_time_slices(avs2,avs_inds2,act_inds2,1000,10000,max_time)
      plt.figure(figsize=(12,8))
      plt.hist([[len(obs) for obs in observations1_1000],[len(obs) for obs in observations2_1000]],histtype='step',bins=30)
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168K234dkaj.png]]

***** firing rates in observed time window

      #+begin_src ipython :session master :file ./images/20170821_112344_259168ölkjK234dka234j.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      observations1_20 = extract_time_slices(avs1,avs_inds1,act_inds1,20,10000,max_time)
      observations2_20 = extract_time_slices(avs2,avs_inds2,act_inds2,20,10000,max_time)
      plt.figure(figsize=(12,8))
      plt.hist([[np.sum(obs) for obs in observations1_20],[np.sum(obs) for obs in observations2_20]],histtype='step')
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168ölkjK234dka234j.png]]


      #+begin_src ipython :session master :file ./images/20170821_112344_259168ölkjK234dka234j23424.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      observations1_100 = extract_time_slices(avs1,avs_inds1,act_inds1,100,10000,max_time)
      observations2_100 = extract_time_slices(avs2,avs_inds2,act_inds2,100,10000,max_time)
      plt.figure(figsize=(12,8))
      plt.hist([[np.sum(obs) for obs in observations1_100],[np.sum(obs) for obs in observations2_100]],histtype='step')
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168ölkjK234dka234j23424.png]]

      #+begin_src ipython :session master :file ./images/20170821_112344_259168K234dka234j.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      observations1_1000 = extract_time_slices(avs1,avs_inds1,act_inds1,1000,10000,max_time)
      observations2_1000 = extract_time_slices(avs2,avs_inds2,act_inds2,1000,10000,max_time)
      plt.figure(figsize=(12,8))
      plt.hist([[np.sum(obs) for obs in observations1_1000],[np.sum(obs) for obs in observations2_1000]],histtype='step')
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168K234dka234j.png]]
    
***** thresholds distributions for t_hor=100

      #+begin_src ipython :session master :tangle no :eval confirm
      all_observations1_100= []
      all_observations2_100= []
      for (avs1,avs_inds1,act_inds1),(avs2,avs_inds2,act_inds2) in avs_2afc: 
          max_time = min(act_inds1[-1],act_inds2[-1])
          all_observations1_100.extend(extract_time_slices(avs1,avs_inds1,act_inds1,1000,10000,max_time))
          all_observations2_100.extend(extract_time_slices(avs2,avs_inds2,act_inds2,1000,10000,max_time))
      #+end_src 

      #+RESULTS:

      #+begin_src ipython :session master :file ./images/20170821_112344_259168ölkjK234dka234adölfkj2j.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      s_0 = 1
      plt.figure(figsize=(12,8))
      thds1 = thresholds(all_observations1_100,s_0)
      thds2 = thresholds(all_observations2_100,s_0)
      acc = acc_2afc(thds2,thds1)
      plt.hist([thds1,thds2],histtype='step',label=['figure','background'])
      plt.legend()
      plt.title('thredholds distribution for t_hor='+str(100)+", s_0="+str(s_0)+" accuracy is "+str(acc))
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168ölkjK234dka234adölfkj2j.png]]


      #+begin_src ipython :session master :file ./images/20170821_112344_259168ölkjK234dka234adölfkj2342j.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      s_0 = 5
      plt.figure(figsize=(12,8))
      thds1 = thresholds(all_observations1_100,s_0)
      thds2 = thresholds(all_observations2_100,s_0)
      acc = acc_2afc(thds2,thds1)
      plt.hist([thds1,thds2],histtype='step',label=['figure','background'])
      plt.legend()
      plt.title('thredholds distribution for t_hor='+str(100)+", s_0="+str(s_0)+" accuracy is "+str(acc))
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168ölkjK234dka234adölfkj2342j.png]]

      
      #+begin_src ipython :session master :file ./images/20170821_112344_259168ölkjK234dka234adölfkj2342j234.png :tangle no
      max_time = min(act_inds1[-1],act_inds2[-1])
      s_0 = 10
      plt.figure(figsize=(12,8))
      thds1 = thresholds(all_observations1_100,s_0)
      thds2 = thresholds(all_observations2_100,s_0)
      acc = acc_2afc(thds2,thds1)
      plt.hist([thds1,thds2],histtype='step',label=['figure','background'])
      plt.legend()
      plt.title('thredholds distribution for t_hor='+str(100)+", s_0="+str(s_0)+" accuracy is "+str(acc))
      #+end_src 

      #+RESULTS:
      [[file:./images/20170821_112344_259168ölkjK234dka234adölfkj2342j234.png]]

**** Old: Evaluation of 2afc statistics for classifier and baseline lengths and rates :exploration:

     #+begin_src ipython :session master
       import numpy as np
       def evaluate_sim_res(sim_res,t_hors=np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int),
                            s_0s = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int),
                            min_time=10000,ensact='all'):
           plt.ioff()
           accuracies_thresholds = np.empty((len(t_hors),len(s_0s)))
           accuracies_rates = np.empty((len(t_hors),len(s_0s)))
           hists_thresholds = np.empty((len(t_hors),len(s_0s)),dtype=object)
           fig_thresholds = np.empty((len(t_hors),len(s_0s)),dtype=object)
           avs_2afc = extract_2afc_avalanche_sizes_and_times(sim_res)
           if ensact != 'all':
               ens,act = ensact 
               avs_2afc = [avs_2afc[10*ens+act]]
           fig_rates,fig_num_obs,hists_rates,hists_num_obs = [],[],[],[]
           for i,t_hor in enumerate(t_hors):
               print('i',i,flush=True)
               all_observations1,all_observations2 = [],[]
               for k,((avs1,avs_inds1,act_inds1),(avs2,avs_inds2,act_inds2)) in enumerate(avs_2afc):
                   print('k',k,flush=True)
                   max_time = min(np.max(act_inds1),np.max(act_inds2))
                   if max_time > min_time:
                       obs1 = extract_time_slices(avs1,avs_inds1,act_inds1,t_hor,min_time,max_time)
                       obs2 = extract_time_slices(avs2,avs_inds2,act_inds2,t_hor,min_time,max_time)
                       ncobs = min(len(obs1),len(obs2))
                       all_observations1.extend(obs1[:ncobs])
                       all_observations2.extend(obs2[:ncobs])
               r1,r2 = ([np.sum(obs) for obs in all_observations1],[np.sum(obs) for obs in all_observations2])
               bc = max(np.mean(r1)/np.mean(r2),1) # background contrast
               # now do it again scaled by the calculated background contrast
               all_observations1 = []
               all_observations2 = []
               for k,((avs1,avs_inds1,act_inds1),(avs2,avs_inds2,act_inds2)) in enumerate(avs_2afc):
                   print('k',k,flush=True)
                   max_time = min(np.max(act_inds1),np.max(act_inds2))
                   if max_time > min_time:
                       obs1 = extract_time_slices(avs1,avs_inds1,act_inds1,t_hor,min_time,int(max_time/bc))
                       obs2 = extract_time_slices(avs2,avs_inds2,act_inds2,int(t_hor*bc),min_time,max_time)
                       ncobs = min(len(obs1),len(obs2))
                       all_observations1.extend(obs1[:ncobs])
                       all_observations2.extend(obs2[:ncobs])
               print('extracted all observations',flush=True)
               plt.figure(figsize=(12,8))
               num_obs1,num_obs2 = ([len(obs) for obs in all_observations1],[len(obs) for obs in all_observations2])
               mi,ma = int(min(np.min(num_obs1),np.min(num_obs2))),int(max(np.max(num_obs1),np.max(num_obs2)))
               hist_num_obs = plt.hist([num_obs1,num_obs2],histtype='step',bins=np.arange(mi,ma+1),label=['figure','background'])
               acc_num_obs = acc_2afc(num_obs2,num_obs1)
               plt.title('number avalanches in observation time trame for t_hor='+str(t_hor)+" accuracy is "+str(acc_num_obs)+', bc '+str(bc))
               plt.legend()
               fig_num_obs.append(plt.gcf())
               print('num_obs figure and hist',flush=True)
               plt.figure(figsize=(12,8))
               rates1,rates2 = ([np.sum(obs) for obs in all_observations1],[np.sum(obs) for obs in all_observations2])
               acc_rates = acc_2afc(rates2,rates1)
               accuracies_rates[i,:] = acc_rates
               hist_rates = plt.hist([[r/t_hor for r in rates1],[r/t_hor for r in rates2]],histtype='step',label=['figure','background'])
               plt.title('firing rates in time horizon for t_hor='+str(t_hor)+" accuracy is "+str(acc_rates)+', bc ' + str(bc))
               plt.legend()
               fig_rates.append(plt.gcf())
               print('firing rates figure and hists',flush=True)
               for j,s_0 in enumerate(s_0s):
                   print('j',j,flush=True)
                   plt.figure(figsize=(12,8))
                   thds1,thds2 = (thresholds(all_observations1,s_0),thresholds(all_observations2,s_0))
                   mi,ma = int(min(np.min(thds1),np.min(thds2))),int(max(np.max(thds1),np.max(thds2)))
                   acc = acc_2afc(thds2,thds1)
                   accuracies_thresholds[i,j] = acc
                   hists_thresholds[i,j] = plt.hist([thds1,thds2],histtype='step',bins=np.arange(mi,ma+1),label=['figure','background'])
                   plt.title('thredholds distribution for t_hor='+str(t_hor)+", s_0="+str(s_0)+" accuracy is "+str(acc)+', bc ' + str(bc))
                   plt.legend()
                   fig_thresholds[i,j] = plt.gcf()
           acc_figure,(ax1,ax2) = plt.subplots(2,1,figsize=(16,8))
           im1 = ax1.imshow(accuracies_thresholds,origin='lower')
           im2 = ax2.imshow(accuracies_rates,origin='lower')
           acc_figure.colorbar(im1,ax=ax1)
           acc_figure.colorbar(im2,ax=ax2)
           plt.ion()
           return {'accuracies_thresholds':accuracies_thresholds,
                   'accuracies_rates':accuracies_rates,
                   'hists_thresholds':hists_thresholds,
                   'fig_thresholds':fig_thresholds,
                   'avs_2afc':avs_2afc,
                   'fig_rates':fig_rates,
                   'fig_num_obs':fig_num_obs,
                   'hists_rates':hists_rates,
                   'hists_num_obs':hists_num_obs,
                   'acc_figure':acc_figure}
     #+end_src 

     #+RESULTS:


     Now test this on the cluster
     #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       parameters = [(Ns,Ne,50) for Ns in range(80,120,4) for Ne in range(40,71,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = pickle.load(open(outdir+'/../sim_2afc_parameter_search/sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle","rb"))
       params = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int)
       res = evaluate_sim_res(sim_res)
       pickle.dump(res,open(outdir+'/evaluated_sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       for p,fig in zip(params,res['fig_rates']):
           fig.savefig(outdir+'/rate_fig_'+str(Ns)+","+str(Ne)+","+str(K)+'rate '+str(p)+'.png')
       for p,fig in zip(params,res['fig_num_obs']):
           fig.savefig(outdir+'/num_obs_fig_'+str(Ns)+","+str(Ne)+","+str(K)+' num_obs '+str(p)+'.png')
       [I,J] = res['fig_thresholds'].shape
           for i in range(I):
               for j in range(J):
                   res['fig_thresholds'][i,j].savefig(outdir+'/thresholds_fig'++str(Ns)+","+str(Ne)+","+str(K)+' t_hor='+str(params[i])+', s_0='+str(params[j])+'.png')
       res['acc_figure'].savefig(outdir+'/acc_figure'+str(Ns)+","+str(Ne)+","+str(K)+'.png')
       """,name='evaluate_sim_2afc_parameter_search',queue='maximus+long',task_ids='1-160',servername='server')
     #+end_src 


     Extract the figures again because they had the same name ....
     #+begin_src ipython :session master :tangle no :eval no
            qsub("""
            parameters = [(Ns,Ne,50) for Ns in range(80,120,4) for Ne in range(40,71,2)]
            Ns,Ne,K = parameters[task_id - 1]
            res = pickle.load(open(outdir+'/../evaluate_sim_2afc_parameter_search/evaluated_sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle","rb"))
            params = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int)
            for p,fig in zip(params,res['fig_rates']):
                fig.savefig(outdir+'/rate_fig_'+str(Ns)+","+str(Ne)+","+str(K)+'rate '+str(p)+'.png')
            for p,fig in zip(params,res['fig_num_obs']):
                fig.savefig(outdir+'/num_obs_fig_'+str(Ns)+","+str(Ne)+","+str(K)+' num_obs '+str(p)+'.png')
            [I,J] = res['fig_thresholds'].shape
            for i in range(I):
                for j in range(J):
                    res['fig_thresholds'][i,j].savefig(outdir+'/thresholds_fig'+str(Ns)+","+str(Ne)+","+str(K)+' t_hor='+str(params[i])+', s_0='+str(params[j])+'.png')
            res['acc_figure'].savefig(outdir+'/acc_figure'+str(Ns)+","+str(Ne)+","+str(K)+'.png')
            """,name='figures_evaluate_sim_2afc_parameter_search',queue='maximus+long',task_ids='1-160',servername='server_inline')
     #+end_src 

     Now for bigger K
     #+begin_src ipython :session master :tangle no :eval no
         qsub("""
         parameters = [(Ns,Ne,300) for Ns in range(80,120,4) for Ne in range(40,71,2)]
         Ns,Ne,K = parameters[task_id - 1]
         sim_res = pickle.load(open(outdir+'/../sim_2afc_parameter_search_k300/sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle","rb"))
         params = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int)
         res = evaluate_sim_res(sim_res)
         pickle.dump(res,open(outdir+'/evaluated_sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
         for p,fig in zip(params,res['fig_rates']):
             fig.savefig(outdir+'/rate_fig_'+str(Ns)+","+str(Ne)+","+str(K)+'rate '+str(p)+'.png')
         for p,fig in zip(params,res['fig_num_obs']):
             fig.savefig(outdir+'/num_obs_fig_'+str(Ns)+","+str(Ne)+","+str(K)+' num_obs '+str(p)+'.png')
         [I,J] = res['fig_thresholds'].shape
         for i in range(I):
             for j in range(J):
                 res['fig_thresholds'][i,j].savefig(outdir+'/thresholds_fig'++str(Ns)+","+str(Ne)+","+str(K)+' t_hor='+str(params[i])+', s_0='+str(params[j])+'.png')
         res['acc_figure'].savefig(outdir+'/acc_figure'+str(Ns)+","+str(Ne)+","+str(K)+'.png')
         """,name='evaluate_sim_2afc_parameter_search_k300',queue='maximus+long',task_ids='1-160',servername='server')
     #+end_src 
*** Phase space regions
    produce plots that show some evaluation of a single simulation
    result as 2d plot of the phase space 

    #+begin_src ipython :session master
      import pickle

      ext_dir = '/media/kima/50AC0F1379302ADD'

      def sr_params2name_func(params):
          return 'sim_res'+','.join([str(c) for c in params])+'.pickle'

      def phase_space_matrix(sim_res_func,sim_res_dir,parameter_array,params2name_func,dtype=float,error_value=-1):
          def inner(params):
              print('params',params)
              sim_res = None
              try:
                  sim_res = pickle.load(open(sim_res_dir+'/'+params2name_func(params),'rb'))
              except:
                  print('file not found or not loadable '+sim_res_dir+'/'+params2name_func(params),flush=True)
                  print('for params '+str(params),flush=True)
              return error_value if sim_res is None else sim_res_func(sim_res)
          ret = np.empty(parameter_array.shape,dtype=dtype)
          it = np.nditer(ret,flags=['multi_index'])
          while not it.finished:
              ret[it.multi_index] = inner(parameter_array[it.multi_index])
              it.iternext()
          return ret


    #+end_src 

    #+RESULTS:

    first simulation of phase spase
    #+begin_src ipython :session master :tangle no :eval no
      sim_res_parameter_array = np.array([[(Ns,Ne,200) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
      def num_avs_recorded(sim_res):
          return sum([len(sim_res['sim_res'][ens]['sim_res_a'][act][1]) +
                      len(sim_res['sim_res'][ens]['sim_res_b'][act][1]) for ens in range(10) for act in range(10)])
      psm = phase_space_matrix(num_avs_recorded,'/media/kima/50AC0F1379302ADD1/sim_2afc_parameter_search_k200',sim_res_parameter_array,sr_params2name_func)

      # plt.imshow(psm,origin='lower')
      # plt.colorbar()

      # plt.title('criticality_regions indicated by total number of recorded avalanches')
      # plt.label('N_s in range(80,120,4)')
      #plt.ylabel('N_e in range(40,71,2)')
    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master :tangle no :eval no
       sim_res_parameter_array = np.array([[(Ns,Ne,200) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
       def num_avs_recorded(sim_res):
           return sum([len(sim_res['sim_res'][ens]['sim_res_a'][act][1]) +
                       len(sim_res['sim_res'][ens]['sim_res_b'][act][1])for ens in range(10) for act in range(10)])
       psm = phase_space_matrix(num_avs_recorded,ext_dir+'/sim_2afc_parameter_search_k200',sim_res_parameter_array,sr_params2name_func)

       plt.imshow(psm,origin='lower')
       plt.colorbar()

       plt.title('criticality_regions indicated by total number of recorded avalanches')
       plt.xlabel('N_s in range(80,120,4)')
       plt.ylabel('N_e in range(40,71,2)')
    #+end_src 

    #+begin_src ipython :session master :tangle no :eval no
        sim_res_parameter_array = np.array([[(Ns,Ne,300) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
        def num_avs_recorded(sim_res):
            return sum([len(sim_res['sim_res'][ens]['sim_res_a'][act][1]) +
                        len(sim_res['sim_res'][ens]['sim_res_b'][act][1])for ens in range(10) for act in range(10)])
        psm = phase_space_matrix(num_avs_recorded,ext_dir+'/sim_2afc_parameter_search_k300',sim_res_parameter_array,sr_params2name_func)

        plt.imshow(psm,origin='lower')
        plt.colorbar()

        plt.title('criticality_regions indicated by total number of recorded avalanches')
        plt.xlabel('N_s in range(80,120,4)')
        plt.ylabel('N_e in range(40,71,2)')
    #+end_src 


    
    #+begin_src ipython :session master :tangle no :eval no
            qsub("""
            ext_dir = outdir+"/.."

            def esr_params2name_func(params):
                return 'evaluated_sim_res'+','.join([str(c) for c in params])+'.pickle'

            sim_res_parameter_array = np.array([[(Ns,Ne,100) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
            def best_accuracy(esr):
                return np.max(esr['accuracies_thresholds'][-1])
            psm = phase_space_matrix(best_accuracy,ext_dir+'/evaluate_sim_2afc_parameter_search_background_contrast_k100',sim_res_parameter_array,esr_params2name_func,error_value=0.4)

            plt.imshow(psm,origin='lower')
            plt.colorbar()

            plt.title('criticality_regions indicated by total number of recorded avalanches')
            plt.xlabel('N_s in range(80,120,4)')
            plt.ylabel('N_e in range(40,71,2)')
            plt.savefig('best_accuracy_k100.png')
            pickle.dump(plt.gcf(),open(outdir+"/best_accuracy_k100.pickle","wb"))
            """,name='best_accuracy_k100',servername='server_inline',queue='maximus')
    #+end_src 


    #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       ext_dir = outdir+"/.."

       def esr_params2name_func(params):
           return 'evaluated_sim_res'+','.join([str(c) for c in params])+'.pickle'

       sim_res_parameter_array = np.array([[(Ns,Ne,200) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
       def best_accuracy(esr):
           return np.max(esr['accuracies_thresholds'][-1])
       psm = phase_space_matrix(best_accuracy,ext_dir+'/evaluate_sim_2afc_parameter_search_background_contrast_k200',sim_res_parameter_array,esr_params2name_func,error_value=0.4)

       plt.imshow(psm,origin='lower')
       plt.colorbar()

       plt.title('criticality_regions indicated by total number of recorded avalanches')
       plt.xlabel('N_s in range(80,120,4)')
       plt.ylabel('N_e in range(40,71,2)')
       plt.savefig('best_accuracy_k200.png')
       pickle.dump(plt.gcf(),open(outdir+"/best_accuracy_k200.pickle","wb"))
       """,name='best_accuracy_k200',servername='server_inline',queue='maximus')
    #+end_src 


    #+begin_src ipython :session master :tangle no :eval no
            qsub("""
            ext_dir = outdir+"/.."

            def esr_params2name_func(params):
                return 'evaluated_sim_res'+','.join([str(c) for c in params])+'.pickle'

            sim_res_parameter_array = np.array([[(Ns,Ne,100) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
            def best_threshold(esr):
                params = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int)
                return params[np.argmax(esr['accuracies_thresholds'][-1])]
            psm = phase_space_matrix(best_threshold,ext_dir+'/evaluate_sim_2afc_parameter_search_background_contrast_k100',sim_res_parameter_array,esr_params2name_func,error_value=0.4)

            plt.imshow(psm,origin='lower')
            plt.colorbar()

            plt.title('criticality_regions indicated by total number of recorded avalanches')
            plt.xlabel('N_s in range(80,120,4)')
            plt.ylabel('N_e in range(40,71,2)')
            plt.savefig('best_accuracy_k100.png')
            pickle.dump(plt.gcf(),open(outdir+"/best_accuracy_k100.pickle","wb"))
            """,name='best_threshold_k100',servername='server_inline',queue='maximus')
    #+end_src 



    #+begin_src ipython :session master :tangle no :eval no
            qsub("""
            ext_dir = outdir+"/.."

            def esr_params2name_func(params):
                return 'evaluated_sim_res'+','.join([str(c) for c in params])+'.pickle'

            sim_res_parameter_array = np.array([[(Ns,Ne,200) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
            def best_threshold(esr):
                params = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int)
                return params[np.argmax(esr['accuracies_thresholds'][-1])]
            psm = phase_space_matrix(best_threshold,ext_dir+'/evaluate_sim_2afc_parameter_search_background_contrast_k200',sim_res_parameter_array,esr_params2name_func,error_value=0.4)

            plt.imshow(psm,origin='lower')
            plt.colorbar()

            plt.title('criticality_regions indicated by total number of recorded avalanches')
            plt.xlabel('N_s in range(80,120,4)')
            plt.ylabel('N_e in range(40,71,2)')
            plt.savefig('best_accuracy_k200.png')
            pickle.dump(plt.gcf(),open(outdir+"/best_accuracy_k200.pickle","wb"))
            """,name='best_threshold_k200',servername='server_inline',queue='maximus')
    #+end_src 
*** Control parameter for phase space
    
    #+begin_src ipython :session master
      def num_avs_matrix(sim_res):
          """returns a matrix M_{i,j} = (a_{i,j},b_{i,j}) of the number of recorded avalanches
          for ensemble i and activation j of simulation a and b"""
          num_ens,num_act = (sim_res['num_ens'],sim_res['num_act'])
          M = np.empty((num_ens,num_act),dtype=object)
          for i,sr in enumerate(sim_res['sim_res']):
              for j,((_,avs_inds_a,_),(_,avs_inds_b,_)) in enumerate(zip(sr['sim_res_a'],sr['sim_res_b'])):
                  M[i,j] = (len(avs_inds_a),len(avs_inds_b))
          return M


      def inds2subnet_dict(ens):
          d = defaultdict(lambda :[])
          for i,e in enumerate(ens):
              for unit in e:
                  d[unit].append(i)
          return d

      def pairwise_overlap(ens):
          Ovl = np.empty([len(ens)]*2,dtype=object)
          for i,e in enumerate(ens):
              for j,e2 in enumerate(ens):
                  Ovl[i,j] = len(set(e)-set(e2))
          return Ovl

      import networkx
      def tograph(W,only_pos=True):
          """converts W to a undirected (only_pos=True) networkx matrix"""


    #+end_src

    #+begin_src ipython :session master :eval no :tangle no
      sim_res = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/sim_res100,60,200.pickle','rb'))
      M = num_avs_matrix(sim_res)
      ens1 = sim_res['sim_res'][0]['ens']
      W1 = sim_res['sim_res'][0]['W']
      fig11 = sim_res['sim_res'][0]['figure_units'][0]
      noise11 = sim_res['sim_res'][0]['noise_units'][0]

      ens2 = sim_res['sim_res'][1]['ens']
      W2 = sim_res['sim_res'][1]['W']
      fig21 = sim_res['sim_res'][1]['figure_units'][0]
      noise21 = sim_res['sim_res'][1]['noise_units'][0]

      act_units11 = np.hstack((fig11,noise11))
      act_ind1 = np.zeros((1000,))
      act_ind1[act_units11] = 1


      act_units21 = np.hstack((fig21,noise21))
      act_ind2 = np.zeros((1000,))
      act_ind2[act_units21] = 1

      ens7 = sim_res['sim_res'][6]['ens']
      W7 = sim_res['sim_res'][6]['W']
      fig79 = sim_res['sim_res'][6]['figure_units'][8]
      noise79 = sim_res['sim_res'][6]['noise_units'][8]
      background79 = sim_res['sim_res'][6]['background_units'][8]

      act_units79 = np.hstack((fig79,noise79))
      act_ind7 = np.zeros((1000,))
      act_ind7[act_units79] = 1

      actb_units79 = np.hstack((noise79,background79))
      actb_ind7 = np.zeros((1000,))
      actb_ind7[actb_units79] = 1

      # to debug this, record for the overflowing avalanche the sizes of
      # consecutive avalanche steps

    #+end_src 

    #+RESULTS:


    try to identify differences between supsupcritical and running
    weight matrices using graph analysis tools

    #+begin_src ipython :session master
      import networkx as  nx
      import networkx.algorithms.approximation as app
      import networkx.algorithms as alg
      def create_graph(W,undirected=True):
          G = nx.Graph()
          G.add_nodes_from(range(W.shape[0]))
          for i in range(W.shape[0]):
              for j in range(W.shape[1]):
                  if i!=j and W[i,j] > 0:
                      G.add_edge(i,j)
          return G
    #+end_src 

    #+RESULTS:

*** Avalanche statistics in figure and background simulations
    see directly the avalanche statistics for the 2afc simulations
    #+begin_src ipython :session master 
      def evaluate_avalanche_plots(sim_res):
          plt.ioff()
          e = load_module('ehe_detailed').EHE()
          N_s,N_e,K = sim_res['N_s'],sim_res['N_e'],50
          all_avs_sizes_a,all_avs_sizes_b,all_avs_sizes_ref = [],[],[]
          num_ens,num_act = 10,10
          figures = np.empty((10,10),dtype=object)
          for ens in range(num_ens):
              for act in range(num_act):
                  f,ax = plt.subplots(1,1,figsize=(16,10))
                  avs_a,avs_inds_a,act_inds_a = sim_res['sim_res'][ens]['sim_res_a'][act]
                  avs_sizes_a,avs_durations_a = e.get_avs_size_and_duration(avs_a,avs_inds_a)
                  avs_b,avs_inds_b,act_inds_b = sim_res['sim_res'][ens]['sim_res_b'][act]
                  avs_sizes_b,avs_durations_b = e.get_avs_size_and_duration(avs_b,avs_inds_b)
                  e.simulate_model_const(np.random.random(N_s),len(avs_sizes_a),(1-1/np.sqrt(N_s))/N_s,deltaU)
                  avs_sizes_ref = e.avs_sizes
                  loglogplot(avs_sizes_a,ax=ax,label='avalanche sizes figure')
                  loglogplot(avs_sizes_b,ax=ax,label='avalanche sizes background')
                  loglogplot(avs_sizes_ref,ax=ax,label='reference N_s critical distribution')
                  ax.set_title('avs distributions 2afc NS='+str(N_s)+', Ne='+str(N_e)
                               + ', K='+str(50)+', ensemble,act='+str(ens)+','+str(act))
                  all_avs_sizes_a.extend(avs_sizes_a)
                  all_avs_sizes_b.extend(avs_sizes_b)
                  all_avs_sizes_ref.extend(avs_sizes_ref)
                  ax.legend()
                  figures[ens,act] = f
          f,ax = plt.subplots(1,1,figsize=(16,10))
          loglogplot(avs_sizes_a,ax=ax,label='avalanche sizes figure')
          loglogplot(avs_sizes_b,ax=ax,label='avalanche sizes background')
          loglogplot(avs_sizes_ref,ax=ax,label='reference N_s critical distribution')
          ax.set_title('all avs distributions 2afc NS='+str(N_s)+', Ne='+str(N_e)
                       + ', K='+str(50))
          ax.legend()
          plt.ion()
          return {'all_avalanche_figure':f,'ens+act_figures':figures}


      def evaluate_avalanche_plots_2(sim_res,N_s,N_e,K):
          #plt.ioff()
          e = load_module('ehe_detailed').EHE()
          all_avs_sizes_a,all_avs_sizes_b,all_avs_sizes_ref = [],[],[]
          num_ens,num_act = 10,10
          figures = np.empty((10,10),dtype=object)
          for ens in [9]:
              for act in [9]:
                  f,ax = plt.subplots(1,1,figsize=(16,10))
                  avs_a,avs_inds_a,act_inds_a = sim_res['sim_res'][ens]['sim_res_a'][act]
                  avs_sizes_a,avs_durations_a = e.get_avs_size_and_duration(avs_a,avs_inds_a)
                  avs_b,avs_inds_b,act_inds_b = sim_res['sim_res'][ens]['sim_res_b'][act]
                  avs_sizes_b,avs_durations_b = e.get_avs_size_and_duration(avs_b,avs_inds_b)
                  e.simulate_model_const(np.random.random(N_s),len(avs_sizes_a),(1-1/np.sqrt(N_s))/N_s,deltaU)
                  avs_sizes_ref = e.avs_sizes
                  # loglogplot(avs_sizes_a,ax=ax,label='avalanche sizes figure')
                  # loglogplot(avs_sizes_b,ax=ax,label='avalanche sizes background')
                  # loglogplot(avs_sizes_ref,ax=ax,label='reference N_s critical distribution')
                  # ax.set_title('avs distributions 2afc NS='+str(N_s)+', Ne='+str(N_e)
                  #              + ', K='+str(50)+', ensemble,act='+str(ens)+','+str(act))
                  all_avs_sizes_a.extend(avs_sizes_a)
                  all_avs_sizes_b.extend(avs_sizes_b)
                  all_avs_sizes_ref.extend(avs_sizes_ref)
                  # ax.legend()
                  # figures[ens,act] = f
          # f,ax = plt.subplots(1,1,figsize=(16,10))
          # loglogplot(avs_sizes_a,ax=ax,label='avalanche sizes figure')
          # loglogplot(avs_sizes_b,ax=ax,label='avalanche sizes background')
          # loglogplot(avs_sizes_ref,ax=ax,label='reference N_s critical distribution')
          # ax.set_title('all avs distributions 2afc NS='+str(N_s)+', Ne='+str(N_e)
          #              + ', K='+str(50))
          # ax.legend()
          # plt.ion()
          return (all_avs_sizes_a,all_avs_sizes_b,all_avs_sizes_ref)


      def evaluate_avalanche_plots_3(sim_res,N_s,N_e,K):
          #plt.ioff()
          e = load_module('ehe_detailed').EHE()
          all_avs_sizes_a,all_avs_sizes_b,all_avs_sizes_ref = [],[],[]
          num_ens,num_act = 10,10
          figures = np.empty((10,10),dtype=object)
          for ens in [9]:
              for act in [9]:
                  f,ax = plt.subplots(1,1,figsize=(16,10))
                  avs_a,avs_inds_a,act_inds_a = sim_res['sim_res'][ens]['sim_res_a'][act]
                  avs_sizes_a,avs_durations_a = e.get_avs_size_and_duration(avs_a,avs_inds_a)
                  avs_b,avs_inds_b,act_inds_b = sim_res['sim_res'][ens]['sim_res_b'][act]
                  avs_sizes_b,avs_durations_b = e.get_avs_size_and_duration(avs_b,avs_inds_b)
                  e.simulate_model_const(np.random.random(N_s),len(avs_sizes_a),(1-1/np.sqrt(N_s))/N_s,deltaU)
                  avs_sizes_ref = e.avs_sizes
                  # loglogplot(avs_sizes_a,ax=ax,label='avalanche sizes figure')
                  # loglogplot(avs_sizes_b,ax=ax,label='avalanche sizes background')
                  # loglogplot(avs_sizes_ref,ax=ax,label='reference N_s critical distribution')
                  # ax.set_title('avs distributions 2afc NS='+str(N_s)+', Ne='+str(N_e)
                  #              + ', K='+str(50)+', ensemble,act='+str(ens)+','+str(act))
                  all_avs_sizes_a.extend(avs_sizes_a)
                  all_avs_sizes_b.extend(avs_sizes_b)
                  all_avs_sizes_ref.extend(avs_sizes_ref)
                  # ax.legend()
                  # figures[ens,act] = f
          # f,ax = plt.subplots(1,1,figsize=(16,10))
          # loglogplot(avs_sizes_a,ax=ax,label='avalanche sizes figure')
          # loglogplot(avs_sizes_b,ax=ax,label='avalanche sizes background')
          # loglogplot(avs_sizes_ref,ax=ax,label='reference N_s critical distribution')
          # ax.set_title('all avs distributions 2afc NS='+str(N_s)+', Ne='+str(N_e)
          #              + ', K='+str(50))
          # ax.legend()
          # plt.ion()
          return (all_avs_sizes_a,all_avs_sizes_b,all_avs_sizes_ref)


      def draw_all_avs_plot(avs_sizes_a,avs_sizes_b,avs_sizes_ref,N_s,N_e,K,beta):
          f,ax = plt.subplots(1,1,figsize=(5,3))
          loglogplot(avs_sizes_a,ax=ax,label=r'target')
          loglogplot(avs_sizes_b,ax=ax,label=r'distractor')
          loglogplot(avs_sizes_ref,ax=ax,label=r'reference')
          ax.set_title(r'$N_s='+str(N_s)+'$, $N_e='+str(N_e)
                       + '$, $K='+str(K)+r'$, $\beta ='+str(beta)+'$')
          ax.legend()
          return f,ax


      plt.rc('text', usetex=True)
      plt.rc('font', family='serif')


      def draw_subplots(parameters):
          for (Ns,Ne,K,beta) in parameters:
              sim_res = pickle.load(open('../avalanches/sim_res_avalanche_plots/sim_res'+str(Ns)+','+str(Ne)+','+str(K)+'.pickle','rb'))
              (a,b,ref) = evaluate_avalanche_plots_2(sim_res,Ns,Ne,K)
              f,ax = draw_all_avs_plot(a,b,ref,Ns,Ne,K,beta)
              f.savefig('all_avs'+str(Ns)+','+str(Ne)+','+str(K)+','+str(beta)+'.pdf')

      parameters = [(80,46,100,1),
                    (80,44,100,1),
                    (80,40,100,1),
                    (100,54,200,2),
                    (100,50,200,2),
                    (100,44,200,2)]



      def draw_subplots_2(parameters):
          for (Ns,Ne,K,beta) in parameters:
              sim_res = pickle.load(open('../avalanches/normal_sim_res/sim_res'+str(Ns)+','+str(Ne)+','+str(K)+'.pickle','rb'))
              (a,b,ref) = evaluate_avalanche_plots_3(sim_res,Ns,Ne,K)
              f,ax = draw_all_avs_plot(a,b,ref,Ns,Ne,K,beta)
              f.savefig('all_avs'+str(Ns)+','+str(Ne)+','+str(K)+','+str(beta)+'.pdf')

      parameters_2 = [(100,44,100,2),
                      (100,54,100,2),
                      (100,58,100,2)]



      def modify_hist(f,s_0,acc,name):
          f.set_figsize=(3,2)
          f.axes[0].set_title(r'Threshold distributions $T=1000$,$s_0='+str(s_0)+'$,accuracy$='+str(acc)+"$")
          hl = f.axes[0].get_legend_handles_labels()
          f.axes[0].legend(hl[0],['distractor','target'])
          f.axes[0].set_xlabel(r"num avs$ > s_0$")
          f.savefig(name,dpi=300)


      def do_modifications(esr):
          s_0s = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int)
          modify_hist(esr['fig_thresholds'][-1][0],1,0.002,"hist1000,1.pdf")
          modify_hist(esr['fig_thresholds'][-1][np.where(s_0s == 24)[0][0]],1,0.797,"hist1000,24.pdf")
          modify_hist(esr['fig_thresholds'][-1][np.where(s_0s == 30)[0][0]],1,0.817,"hist1000,30.pdf")
          modify_hist(esr['fig_thresholds'][-1][np.where(s_0s == 45)[0][0]],1,0.765,"hist1000,45.pdf")


      def redraw_hist(hist_data,s_0,acc,name):
          f,ax = plt.subplots(figsize=(6,4))
          bars = hist_data[0]
          bins = hist_data[1]
          data = [np.concatenate([np.ones(int(bar[i])) * bins[i] for i in range(len(bar))]) for bar in bars]
          ax.hist(data,histtype='step',bins=bins - 0.5,label=['target','distractor'],linewidth=2)
          ax.set_title(r'Threshold distributions $T=1000$,$s_0='+str(s_0)+'$,accuracy$='+str(acc)+"$")
          ax.set_xlabel(r"number of avalanche sizes bigger than $s_0$")
          ax.legend()
          f.savefig(name,dpi=300)
          return f,ax


      def do_redraw_hist(esr):
          s_0s = np.concatenate(([1,2,4,6,8,10,12,14,16,18],np.logspace(np.log10(20),3,20))).astype(int)
          redraw_hist(esr['hists_thresholds'][-1][0],1,0.002,"hist1000,1.pdf")
          redraw_hist(esr['hists_thresholds'][-1][np.where(s_0s == 24)[0][0]],24,0.797,"hist1000,24.pdf")
          redraw_hist(esr['hists_thresholds'][-1][np.where(s_0s == 30)[0][0]],30,0.817,"hist1000,30.pdf")
          redraw_hist(esr['hists_thresholds'][-1][np.where(s_0s == 45)[0][0]],45,0.765,"hist1000,45.pdf")
    #+end_src 

    #+RESULTS:

    #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       parameters = [(Ns,Ne,50) for Ns in range(80,120,4) for Ne in range(40,71,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = pickle.load(open(outdir+'/../sim_2afc_parameter_search/sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle","rb"))
       res = evaluate_avalanche_plots(sim_res)
       pickle.dump(res,open(outdir+'/evaluated_sim_res_avalanche_plots'+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       for i in range(10):
           for j in range(10): 
               res['ens+act_figures'][i,j].savefig(outdir+'/avs_plots'+str(Ns)+","+str(Ne)+","+str(K)+' ens,act='+str(i)+','+str(j)+'.png')
       res['all_avalanche_figure'].savefig(outdir+'/all_avs_plots'+str(Ns)+","+str(Ne)+","+str(K)+'.png')
       """,name='evaluate_sim_2afc_avalanche_statistics',queue='maximus+long',task_ids='1-160',servername='server')
    #+end_src 


    #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       parameters = [(Ns,Ne,300) for Ns in range(80,120,4) for Ne in range(40,71,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = pickle.load(open(outdir+'/../sim_2afc_parameter_search_k300/sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle","rb"))
       res = evaluate_avalanche_plots(sim_res)
       pickle.dump(res,open(outdir+'/evaluated_sim_res_avalanche_plots'+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       for i in range(10):
           for j in range(10): 
               res['ens+act_figures'][i,j].savefig(outdir+'/avs_plots'+str(Ns)+","+str(Ne)+","+str(K)+' ens,act='+str(i)+','+str(j)+'.png')
       res['all_avalanche_figure'].savefig(outdir+'/all_avs_plots'+str(Ns)+","+str(Ne)+","+str(K)+'.png')
       """,name='evaluate_sim_2afc_avalanche_statistics_k300',queue='maximus+long',task_ids='1-160',servername='server')
    #+end_src 


    #+begin_src ipython :session master 
           def evaluate_population_activity(sim_res):
               e = load_module('ehe_detailed').EHE()
               N_s,N_e,K= sim_res['N_s'],sim_res['N_e'],sim_res['K']
               all_pop_act_a,all_pop_act_b = [],[]
               pop_acts_a = []; pop_acts_b = []
               num_ens,num_act = 5,5
               figures = np.empty((10,10),dtype=object)
               for ens in range(num_ens):
                   for act in range(num_act):
                       f,(ax_a,ax_b) = plt.subplots(2,1,figsize=(16,20))
                       avs_a,avs_inds_a,act_inds_a = sim_res['sim_res'][ens]['sim_res_a'][act]
                       avs_sizes_a,avs_durations_a = e.get_avs_size_and_duration(avs_a,avs_inds_a)
                       avs_b,avs_inds_b,act_inds_b = sim_res['sim_res'][ens]['sim_res_b'][act]
                       avs_sizes_b,avs_durations_b = e.get_avs_size_and_duration(avs_b,avs_inds_b)
                       pop_act_a = np.zeros(act_inds_a[-1])
                       pop_act_a[act_inds_a[1:-1]] = avs_sizes_a
                       ax_a.plot(pop_act_a)
                       ax_a.set_title('population activity figure Ns='+str(N_s)+', Ne='+str(N_e)
                                    + ', K='+str(K)+', ensemble,act='+str(ens)+','+str(act))
                       pop_act_b = np.zeros(act_inds_b[-1])
                       pop_act_b[act_inds_b[1:-1]] = avs_sizes_b
                       ax_b.plot(pop_act_b)
                       ax_b.set_title('population activity background Ns='+str(N_s)+', Ne='+str(N_e)
                                    + ', K='+str(K)+', ensemble,act='+str(ens)+','+str(act))
                       all_pop_act_a.extend(pop_act_a)
                       all_pop_act_b.extend(pop_act_b)
                       pop_acts_a.append(pop_act_a)
                       pop_acts_b.append(pop_act_b)
               # f,(ax_a,ax_b) = plt.subplots(2,1,figsize=(16,20))
               # ax_a.plot(all_pop_act_a)
               # ax_a.set_title('all pop act figure NS='+str(N_s)+', Ne='+str(N_e) + ', K='+str(K))
               # ax_b.plot(all_pop_act_b)
               # ax_b.set_title('all pop act background NS='+str(N_s)+', Ne='+str(N_e) + ', K='+str(K))
               return {'all_pop_act_a':all_pop_act_a,'all_pop_act_b':all_pop_act_b,'pop_acts_a':pop_acts_a,'pop_acts_b':pop_acts_u}
    #+end_src 



    #+RESULTS:


    #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       parameters = [(Ns,Ne,50) for Ns in range(80,120,4) for Ne in range(40,71,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = pickle.load(open(outdir+'/../sim_2afc_parameter_search/sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle","rb"))
       res = evaluate_population_activity(sim_res)
       pickle.dump(res,open(outdir+'/evaluated_sim_res_population_activity'+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       for i in range(10):
           for j in range(10): 
               res['ens+act_figures'][i,j].savefig(outdir+'/pop_act_plots'+str(Ns)+","+str(Ne)+","+str(K)+' ens,act='+str(i)+','+str(j)+'.png')
       res['all_pop_act_figure'].savefig(outdir+'/all_pop_act_plots'+str(Ns)+","+str(Ne)+","+str(K)+'.png')
       """,name='evaluate_sim_2afc_population_activity',queue='long_64gb',task_ids='1-160',servername='server')
    #+end_src 

    #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       parameters = [(Ns,Ne,300) for Ns in range(80,120,4) for Ne in range(40,71,2)]
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = pickle.load(open(outdir+'/../sim_2afc_parameter_search_k300/sim_res'+str(Ns)+","+str(Ne)+","+str(K)+".pickle","rb"))
       res = evaluate_population_activity(sim_res)
       pickle.dump(res,open(outdir+'/evaluated_sim_res_population_activity'+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       for i in range(10):
           for j in range(10): 
               res['ens+act_figures'][i,j].savefig(outdir+'/pop_act_plots'+str(Ns)+","+str(Ne)+","+str(K)+' ens,act='+str(i)+','+str(j)+'.png')
       res['all_pop_act_figure'].savefig(outdir+'/all_pop_act_plots'+str(Ns)+","+str(Ne)+","+str(K)+'.png')
       """,name='evaluate_sim_2afc_population_activity_k300',queue='long_64gb',task_ids='1-160',servername='server')
    #+end_src 
 
*** coactivation in target stimulus
    extract avalanche size distribution of activated subnetworks!

    #+begin_src ipython :session master
      def coactivated_avs_distribs(sim_res,ens_idx,act_idx):
          avs,avs_inds,act_inds = sim_res['sim_res'][ens_idx]['sim_res_a'][act_idx]
          e = load_module('ehe_detailed').EHE()
          ensemble = sim_res['ensembles'][ens_idx]
          spiking_patterns = e.get_spiking_patterns(avs,avs_inds)
          units_in_av = [sum(sp,[]) for sp in spiking_patterns]
          fig = set(sim_res['sim_res'][ens_idx]['figure_units'][act_idx])
          avs_sizes_ens = [([sum(u in e for u in avs_u) for avs_u in units_in_av],len(fig.intersection(e))) for e in ensemble]
          return avs_sizes_ens
    
    #+end_src 

    #+RESULTS:

*** Non 2afc simulation                                         :exploration:
**** Analyze old Simulation results                             :exploration:
     
     
     1. Einlesen der avs und avs_inds Dateien
     2. Aus Task_id das ensemble und aktivierte subnet heraussuchen
     3. Lawinenstatistiken in alles Teilnetzwerken zeichnen lassen
        
     #+begin_src ipython :session master
       def analyze_results(task_id,suffix=''):
           # load dataset
           avs,avs_inds = [np.load("../avalanches/simple_submatrix"+suffix+"/"+s+str(task_id)+".npy") for s in ['avs','avs_inds']]
           # load ensemble and activation information
           (ens_id,act_id) = [(ens_id,act_id) for ens_id in range(10) for act_id in range(10)][task_id-1]
           ensembles = pickle.load(open('ensembles'+suffix+'.pickle','rb'))
           ens = ensembles[ens_id]
           activations = pickle.load(open('activated_ensembles'+suffix+'.pickle','rb'))
           act = activations[act_id]
           # look at avalanche size statistics in each subnet
           e = load_module('ehe_detailed').EHE()
           detailed_sub_avs = []
           sub_avs = []
           for ensemble in ens:
               (sub_avs_detailed,sub_avs_inds) = e.subnetwork_avalanches(avs,avs_inds,set(ensemble))
               (sub_avs_size,sub_avs_duration) = e.get_avs_size_and_duration(sub_avs_detailed,sub_avs_inds)
               detailed_sub_avs.append((sub_avs_detailed,sub_avs_inds))
               sub_avs.append((sub_avs_size,sub_avs_duration))
           return {"ens":ens,"act":act,"sub_avs":sub_avs,"detailed_sub_avs":detailed_sub_avs,'avs':avs,'avs_inds':avs_inds}


       def analyze_results_random(task_id,suffix=''):
           # load dataset
           avs,avs_inds = [np.load("../avalanches/simple_submatrix_random"+suffix+"/"+s+str(task_id)+".npy") for s in ['avs','avs_inds']]
           # load ensemble and activation information
           (ens_id,act_id) = [(ens_id,act_id) for ens_id in range(10) for act_id in range(10)][task_id-1]
           ensembles = pickle.load(open('ensembles'+suffix+'.pickle','rb'))
           ens = ensembles[ens_id]
           activations = pickle.load(open('background_units'+suffix+'.pickle','rb'))
           act = activations[act_id]
           # look at avalanche size statistics in each subnet
           e = load_module('ehe_detailed').EHE()
           detailed_sub_avs = []
           sub_avs = []
           for ensemble in ens:
               (sub_avs_detailed,sub_avs_inds) = e.subnetwork_avalanches(avs,avs_inds,set(ensemble))
               (sub_avs_size,sub_avs_duration) = e.get_avs_size_and_duration(sub_avs_detailed,sub_avs_inds)
               detailed_sub_avs.append((sub_avs_detailed,sub_avs_inds))
               sub_avs.append((sub_avs_size,sub_avs_duration))
           (act_avs_detailed,act_avs_inds) = e.subnetwork_avalanches(avs,avs_inds,set(act))
           (act_avs_size,act_avs_duration) = e.get_avs_size_and_duration(act_avs_detailed,act_avs_inds)
           return {"ens":ens,"act":act,"sub_avs":sub_avs,"detailed_sub_avs":detailed_sub_avs,'avs':avs,'avs_inds':avs_inds,
                   'act_detailed':(act_avs_detailed,act_avs_inds),'act_avs':(act_avs_size,act_avs_duration)}



       def plot_avs_subnets(res):
           f,ax = plt.subplots(figsize=(16,12))
           for sub_avs,sub_avd in res['sub_avs']:
               loglogplot(sub_avs,ax=ax)
           act_avs,act_avd = res['sub_avs'][res['act']]
           #plot reference power law to expect from recurrent ehe network in just the activated subnetwork
           e = load_module('ehe_detailed').EHE()
           N_sub = len(res['ens'][res['act']])
           e.simulate_model_const(np.random.random(N_sub),len(act_avs),(1-1/np.sqrt(N_sub))/N_sub,deltaU)
           loglogplot(e.avs_sizes,ax=ax)


       def plot_avs_subnets_random(res):
           f,ax = plt.subplots(figsize=(16,12))
           for sub_avs,sub_avd in res['sub_avs']:
               loglogplot(sub_avs,ax=ax)
           act_avs,act_avd = res['act_avs']
           loglogplot(act_avs,ax=ax)
           # plot reference power law to expect from recurrent ehe network in just the activated subnetwork
           e = load_module('ehe_detailed').EHE()
           N_sub = len(res['act'])
           e.simulate_model_const(np.random.random(N_sub),len(act_avs),(1-1/np.sqrt(N_sub))/N_sub,deltaU)
           loglogplot(e.avs_sizes,ax=ax)



       def analyze_all_results(suffix=''):
           # produziere 1 Bild. bei dem die aktivierte Kurve die Mittelung
           # über alle aktiven kurven in den 100 realisierungen ist und die
           # nichtaktivierte Kurve ebenfalls über alle nichtaktivierten
           # kurven aller realisierungen gemittelt ist
           mean_avs_activated = []
           mean_avs_nonactivated = []
           N_sub = 0
           for task_id in range(1,101):
               res_tid = analyze_results(task_id)
               sub_avs = res_tid['sub_avs']
               N_sub = len(res_tid['ens'][0])
               mean_avs_nonactivated += sum([sub_avs[i][0] for i in range(len(sub_avs)) if i != res_tid['act']],[])
               mean_avs_activated.extend(sub_avs[res_tid['act']][0])
           f,ax = plt.subplots(figsize=(16,12))
           loglogplot(mean_avs_activated,ax=ax)
           loglogplot(mean_avs_nonactivated,ax=ax)
           e = load_module('ehe_detailed').EHE()
           e.simulate_model_const(np.random.random(N_sub),len(mean_avs_activated),(1-1/np.sqrt(N_sub))/N_sub,deltaU)
           return {'mean_avs_activated':mean_avs_activated,'mean_avs_nonactivated':mean_avs_nonactivated,'f':f,'ax':ax}
     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master :file ./images/20170726_103236_1950AXK.png :tangle no :eval no
    res = analyze_results(45)
    plot_avs_subnets(res)
     #+end_src 

     #+RESULTS:
     [[file:./images/20170726_103236_1950AXK.png]]


     #+begin_src ipython :session master :file ./images/20170726_105914_1950BYR.png :tangle no :eval no
  res_r = analyze_results_random(66)
  plot_avs_subnets_random(res_r)
     #+end_src 

     #+RESULTS:
     [[file:./images/20170726_105914_1950BYR.png]]

     Wie sieht das für N=10000 aus? 

     #+begin_src ipython :session master :file ./images/20170726_112949_1950c7M.png :tangle no :eval no
    res = analyze_results(32,suffix='_1e4')
    plot_avs_subnets(res)
     #+end_src 

     #+RESULTS:
     [[file:./images/20170726_112949_1950c7M.png]]


     Ok hier scheint alles zu Funktionieren. Für evtl. Plots bei der Konferenz braucht es aber noch der Mittelung...
     Erstmal mit Udo reden und dann gucken ob auch alles bei 10000 klappt. Dauert noch bis Simulation zu ende ist. 


     #+begin_src ipython :session master :file ./images/20170726_132145_1950xin.png :tangle no :eval no
  analyze_all_results()
     #+end_src 

     #+RESULTS:
     [[file:./images/20170726_132145_1950xin.png]]

**** Old Parameter Search.                                      :exploration:
     - 60 Simulationen in A und 60 Simulationen in B. Jede Simulation 
       dauert ca. 3.5 cpu Stunden

     Daher schreibe Simulation als einen Job!

     Schön für ein Poster bei der Konferenz wäre es anhand der
     Lawinenstatistiken entscheiden zu können, ob eine figure aktiviert
     ist oder ob die aktivierten Neuronen hintergrundrauschen darstellen.

     Von da her ist es besser beide Situation in einer Simulation an
     einer Realisierung der Eingebetteten Subnetzwerke zu machen

     #+begin_src ipython :session master
       import itertools
       def run_simulation(Ns,Ne,K,Nu=1000,Nl=10000,M_ens=10,M_act=10,beta=2):
           ensembles = [embed_ensembles(N_tot=Nu,N_subunits=Ns,N_ensembles=Ne) for i in range(M_ens)]
           acts = np.random.choice(Ne,M_act,replace=False)
           sim_res = []
           mean_avs_activated = []
           mean_avs_nonactivated = []
           sim_idx = 0
           for (ens,act) in itertools.product(ensembles,acts):
               W = simple_submatrix(Nu,ens,beta=beta)
               e = load_module('ehe_detailed').EHE()
               background = np.random.choice(list(set(range(Nu))-set(ens[act])),K,replace=False)
               e.simulate_model_mat(np.random.random(Nu),10*Nu,W,deltaU,
                                    np.hstack((np.array(ens[act]),background)))
               print('simulation '+str(sim_idx),flush=True)
               avs,avs_inds,act_inds = e.avs,e.avs_inds,e.act_inds
               sub_avs = []
               for i,ensemble in enumerate(ens):
                   (sub_avs_detailed,sub_avs_inds) = e.subnetwork_avalanches(avs,avs_inds,set(ensemble))
                   if len(sub_avs_inds) > 0:
                       (sub_avs_size,sub_avs_duration) = e.get_avs_size_and_duration(sub_avs_detailed,sub_avs_inds)
                       sub_avs.append((sub_avs_size,sub_avs_duration))
                       if i == act:
                           mean_avs_activated.extend(sub_avs_size)
                       else:
                           mean_avs_nonactivated.extend(sub_avs_size)
               print('analysis '+str(sim_idx),flush=True)
               # Also random assignment...
               sim_res.append({"avs":avs,"avs_inds":avs_inds,'ens':ens,'act':act,'act_inds':act_inds,'sub_avs':sub_avs,
                               "background":background})
           f,ax = plt.subplots(figsize=(16,12))
           loglogplot(mean_avs_activated,ax=ax)
           loglogplot(mean_avs_nonactivated,ax=ax)
           e = load_module('ehe_detailed').EHE()
           print("start_const_simulation",flush=True)
           e.simulate_model_const(np.random.random(Ns),len(mean_avs_activated),(1-1/np.sqrt(Ns))/Ns,deltaU)
           loglogplot(e.avs_sizes,ax=ax)
           ax.set_title('Ns='+str(Ns)+",Ne="+str(Ne)+",K="+str(K)+",beta="+str(beta))
           return {'mean_avs_activated':mean_avs_activated,'mean_avs_nonactivated':mean_avs_nonactivated,"sim_res":sim_res,"f":f,"ax":ax}
     #+end_src


     #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       import itertools
       import pickle
       parameters = ([(100,int(Ne),0) for Ne in np.logspace(np.log10(10),np.log10(200),20)] + 
                     [(int(Ns),50,0)  for Ns in np.logspace(np.log10(50),np.log10(500),20)] +
                     [(100,50,int(K)) for K in  np.logspace(np.log10(100),np.log10(500),20)])
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = run_simulation(Ns,Ne,K)
       sim_res['parameters'] = (Ns,Ne,K)
       pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       sim_res['f'].savefig(outdir+"/plot"+str(Ns)+","+str(Ne)+","+str(K)+".png")
       """,name='simple_submatrix_parameter_seach',servername='server',queue='maximus+long',task_ids='1-60')
     #+end_src 
     #+end_src 

    
     #+RESULTS:

     #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       import itertools
       import pickle
       parameters = ([(100,int(Ne),0) for Ne in np.logspace(np.log10(10),np.log10(200),20)] + 
                     [(int(Ns),50,0)  for Ns in np.logspace(np.log10(50),np.log10(500),20)] +
                     [(100,50,int(K)) for K in  np.logspace(np.log10(100),np.log10(500),20)])
       Ns,Ne,K = parameters[task_id - 1]
       sim_res = run_simulation(Ns,Ne,K)
       sim_res['parameters'] = (Ns,Ne,K)
       pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
       sim_res['f'].savefig(outdir+"/plot"+str(Ns)+","+str(Ne)+","+str(K)+".png")
       """,name='simple_submatrix_parameter_seach',servername='server',queue='maximus+long',task_ids='1-60')
     #+end_src 

    
     #+begin_src ipython :session master
      import itertools
      def run_simulation_random(Ns,Ne,Nu=1000,Nl=10000,M_ens=10,M_act=10):
          ensembles = [embed_ensembles(N_tot=Nu,N_subunits=Ns,N_ensembles=Ne) for i in range(M_ens)]
          backgrounds = [np.random.choice(Nu,Ns,replace=False) for i in range(M_act)]
          sim_res = []
          mean_avs_activated = []
          mean_avs_nonactivated = []
          sim_idx = 0
          for (ens,background) in itertools.product(ensembles,backgrounds):
              W = simple_submatrix(Nu,ens)
              e = load_module('ehe_detailed').EHE()
              e.simulate_model_mat(np.random.random(Nu),10*Nu,W,deltaU,np.array(background))
              print('simulation '+str(sim_idx),flush=True)
              avs,avs_inds,act_inds = e.avs,e.avs_inds,e.act_inds
              sub_avs = []
              for i,ensemble in enumerate(ens):
                  (sub_avs_detailed,sub_avs_inds) = e.subnetwork_avalanches(avs,avs_inds,set(ensemble))
                  if len(sub_avs_inds) > 0:
                      (sub_avs_size,sub_avs_duration) = e.get_avs_size_and_duration(sub_avs_detailed,sub_avs_inds)
                      sub_avs.append((sub_avs_size,sub_avs_duration))
                      mean_avs_nonactivated.extend(sub_avs_size)
              (background_avs_detailed,background_avs_inds) = e.subnetwork_avalanches(avs,avs_inds,set(background))
              if len(background_avs_inds) > 0:
                  (background_avs_size,background_avs_duration) = e.get_avs_size_and_duration(background_avs_detailed,background_avs_inds)
                  mean_avs_activated.extend(background_avs_size)
              print('analysis '+str(sim_idx),flush=True)
              sim_res.append({"avs":avs,"avs_inds":avs_inds,'ens':ens,'background':background,'act_inds':act_inds,'sub_avs':sub_avs,
                              "background":background})
          f,ax = plt.subplots(figsize=(16,12))
          loglogplot(mean_avs_activated,ax=ax)
          loglogplot(mean_avs_nonactivated,ax=ax)
          e = load_module('ehe_detailed').EHE()
          print("start_const_simulation",flush=True)
          e.simulate_model_const(np.random.random(Ns),max(len(mean_avs_activated),10*Nu),(1-1/np.sqrt(Ns))/Ns,deltaU)
          loglogplot(e.avs_sizes,ax=ax)
          ax.set_title('Ns='+str(Ns)+",Ne="+str(Ne))
          return {'mean_avs_activated':mean_avs_activated,'mean_avs_nonactivated':mean_avs_nonactivated,"sim_res":sim_res,"f":f,"ax":ax}
     #+end_src

     #+RESULTS:
    

     #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       import itertools
       import pickle
       parameters = ([(100,int(Ne)) for Ne in np.logspace(np.log10(10),np.log10(200),20)] + 
                     [(int(Ns),50)  for Ns in np.logspace(np.log10(50),np.log10(500),20)])
       Ns,Ne = parameters[task_id - 1]
       sim_res = run_simulation_random(Ns,Ne)
       sim_res['parameters'] = (Ns,Ne)
       pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+".pickle",'wb'))
       sim_res['f'].savefig(outdir+"/plot"+str(Ns)+","+str(Ne)+".png")
       """,name='simple_submatrix_random_parameter_seach',servername='server_inline',queue='maximus+long',task_ids='1-40')
     #+end_src 


     genaueres sampling beim Phasenübergang

     #+begin_src ipython :session master :eval no :tangle no
           qsub("""
           import itertools
           import pickle
           parameters = ([(100,int(Ne),0) for Ne in np.linspace(50,70,21)] + 
                         [(int(Ns),50,0)  for Ns in np.linspace(100,120,21)])
           Ns,Ne,K = parameters[task_id - 1]
           sim_res = run_simulation(Ns,Ne,K,M_ens=20,M_act=20)
           sim_res['parameters'] = (Ns,Ne,K)
           pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+","+str(K)+".pickle",'wb'))
           sim_res['f'].savefig(outdir+"/plot"+str(Ns)+","+str(Ne)+","+str(K)+".png")
           """,name='simple_submatrix_parameter_seach_detailed',servername='server',queue='maximus+long',task_ids='1-42')
     #+end_src 

     #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       import itertools
       import pickle
       parameters = ([(100,int(Ne)) for Ne in np.linspace(50,70,21)] + 
                     [(int(Ns),50)  for Ns in np.linspace(100,120,21)])
       Ns,Ne = parameters[task_id - 1]
       sim_res = run_simulation_random(Ns,Ne)
       sim_res['parameters'] = (Ns,Ne)
       pickle.dump(sim_res,open(outdir+"/sim_res"+str(Ns)+","+str(Ne)+".pickle",'wb'))
       sim_res['f'].savefig(outdir+"/plot"+str(Ns)+","+str(Ne)+".png")
       """,name='simple_submatrix_random_parameter_seach_detailed',servername='server',queue='maximus+long',task_ids='1-42')
     #+end_src 

**** old classifier to detect presence of an activated figure   :exploration:

     Approach: 
     - distinguish between network and background by waiting for a large enough avalanche
     - But, can only wait for a certain time, i.e a certain number of external activation steps
    
     User scikit learn classifier interface. 
     Input: 
     - ein sim_res dictionary 
     Output: 
     - True -> figure aktiviert, False -> nur background noise

       #+begin_src ipython :session master
         from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin
         from sklearn.naive_bayes import GaussianNB
         from imblearn.pipeline import Pipeline

         class TimeHorizonAvalancheExtractor(BaseEstimator):
             """Takes the avalanches during the simulation as input and transforms them to
                the largest avalanche size in the observed external input time steps num_obs"""

             def  __init__(self,t_hor=100,max_size=100000):
                 self.t_hor = t_hor
                 self.max_size=max_size

             def fit(self,trials,y=None):
                 return self

             def sample(self,sim_results,labels=None):
                 X,y = ([],[])
                 for sim_res in sim_results:
                     # split the simulation results into bins of t_hor external time steps
                     # see if it is a background or a figure activation
                     figure_simulation = 'act' in sim_res.keys()
                     avs,avs_inds,act_inds = (sim_res['avs'],sim_res['avs_inds'],sim_res['act_inds'])
                     e = load_module('ehe_detailed').EHE()
                     avs_sizes,avs_durations = e.get_avs_size_and_duration(avs,avs_inds)
                     duration_between_avs = np.diff(act_inds)
                     time = np.cumsum(duration_between_avs)
                     t_hor_rem = (time/self.t_hor).astype(int)
                     splitpoints = np.searchsorted(t_hor_rem,np.arange(1,t_hor_rem[-1]))
                     # split the avs_sizes at each point where time becomes another multiple of t_hor
                     observations = np.split(avs_sizes,splitpoints)
                     X.extend(observations)
                     y.extend([figure_simulation]*len(observations))
                     if len(y) > self.max_size:
                         return (X,y)
                 return (X,y)

             def fit_sample(self, X, y=None):
                 return self.fit(X,y).sample(X,y)

         class MaxAvalancheExtractor(BaseEstimator,TransformerMixin):
             """ Transforms the given avalanche sizes to their maximum value"""

             def __init__(self):
                 pass

             def transform(self,X,y=None):
                 return [[np.max(np.hstack(([0],avs_sizes)))] for avs_sizes in X]

             def fit(self,X,y=None):
                 print("hi from fit, self is ",self)
                 return self

             #def fit_transform(self,X,y=None):
             #    print("hi from fit_transform, self is ",self)
             #    return self.fit(X,y).transform(X,y)

         def figure_detector(t_hor):
             return Pipeline([('extract__hor',TimeHorizonAvalancheExtractor(t_hor)),
                              ('max_avs_size',MaxAvalancheExtractor()),
                              ('classification',GaussianNB())])
       #+end_src 

       #+RESULTS:

**** old ROC analysis of classifier                             :exploration:
    
     #+begin_src ipython :session master
       from sklearn.metrics import roc_curve, auc
       import sklearn.metrics as metrics
       def plot_roc(fpr,tpr,auc=None,ax=None):
           lw = 2
           ax = plt.subplots()[1] if ax is None else ax
           auc = auc if auc is not None else metrics.auc(fpr,tpr)
           ax.plot(fpr, tpr, color='darkorange',
                lw=lw, label='ROC curve (area = ' + str(auc) + " )")
           ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
           ax.set_xlim([0.0, 1.0])
           ax.set_ylim([0.0, 1.05])
           ax.set_xlabel('False Positive Rate')
           ax.set_ylabel('True Positive Rate')
           ax.legend(loc="lower right")
     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master
       def extract_data(t_hor,sim_res_a,sim_res_b,exclude_beginning=1000,max_size=None):
           print('max_size',max_size)
           avalanches_a,labels_a = TimeHorizonAvalancheExtractor(t_hor,max_size=max_size).fit_sample(sim_res_a)
           avalanches_b,labels_b = TimeHorizonAvalancheExtractor(t_hor,max_size=max_size).fit_sample(sim_res_b)
           start_idx = int(np.ceil(exclude_beginning/t_hor))
           min_size = min(len(avalanches_a),len(avalanches_b))
           # achtung geschummelt, nicht unbedingt über gleiche realisierung gegangen etc....
           # X,y = ([],[])
           # 
           # for a,b in zip(avalanches_a[start_idx:],avalanches_b[start_idx:]):
           #     pos = np.random.rand() > 0.5
           #     X.append((a,b) if pos  else (b,a))
           #     y.append(0 if pos else 1)
           # return (X,y)
           if max_size is None:
               max_size = min_size
           return (avalanches_a[start_idx:min(min_size,max_size)],avalanches_b[start_idx:min(min_size,max_size)])

       def get_dists(avs_a,avs_b,s_0):
           return ([len(av_a[av_a >= s_0]) for av_a in avs_a],
                   [len(av_b[av_b >= s_0]) for av_b in avs_b])


       def rate_dists(avs_a,avs_b):
           return ([np.sum(av_a) for av_a in avs_a],[np.sum(av_b) for av_b in avs_b])

       def acc_2afc(samples1,samples2):
           ma = max(np.max(samples1),np.max(samples2))
           mi = min(np.min(samples1),np.min(samples2))
           h1 = np.cumsum(np.histogram(samples1,np.arange(mi,ma+2))[0]/len(samples1))
           h2 = np.cumsum(np.histogram(samples2,np.arange(mi,ma+2))[0]/len(samples2))
           x = np.hstack((1,1-h1))
           y = np.hstack((1,1-h2))
           Perf = np.sum(-np.diff(x)*(y[1:]+y[:-1])/2)
           return Perf

       def get_acc_2afc(t_hor,s_0s,sim_res_a,sim_res_b,max_size=None):
           print('start',flush=True)
           avs_a,avs_b = extract_data(t_hor,sim_res_a,sim_res_b,max_size=max_size)
           print('extracted',len(avs_a),len(avs_b),flush=True)
           accs = []
           rate_dist_na,rate_dist_nb = rate_dists(avs_a,avs_b)
           rate_acc = acc_2afc(rate_dist_nb,rate_dist_na)
           for s_0 in s_0s:
               if s_0 > t_hor:
                   accs.append(0)
                   continue
               dist_na,dist_nb = get_dists(avs_a,avs_b,s_0)
               print('dists na and nb',len(avs_a),len(avs_b),flush=True)
               accs.append(acc_2afc(dist_nb,dist_na))
           return accs,rate_acc



     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master :tangle no
       for i in range(100):
           print('start',i,flush=True)
           accs,rate_acc = get_acc_2afc(i+1,range(1,101),sim_res_a,sim_res_b,max_size=100000)
           print('rate_acc',rate_acc)
           print('accs',accs)
           data_matrix[i,:] = accs
           rate_matrix[i,:] = rate_acc
           print('i',i)
     #+end_src 




     #+RESULTS:

     Neuer Klassifikator, explizit für 2afc
     #+begin_src ipython :session master 
       class Classifier2AFC(BaseEstimator,ClassifierMixin):
           def __init__(self,s_0):
               self.s_0 = s_0;

           def fit(self,X,y=None):
               return self # in future fit s_0 here

           def predict(self,X):
               y = []
               for (avs_0,avs_1) in X:
                  if len(avs_0[avs_0 > self.s_0]) > len(avs_1[avs_1 > self.s_0]):
                      y.append(0)
                  #elif len(avs_0[avs_0 > self.s_0]) == len(avs_1[avs_1 > self.s_0]):
                  #    y.append(0 if np.random.rand() > 0.5 else 1)
                  else: 
                      y.append(1)
               return y

     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master
     import numpy as np
     def roc_analysis(X,y,thresholds=np.arange(0,11)):
         tpr,fpr = [],[]
         for th in thresholds: 
             print('threshold ',th)
             cl = Classifier2AFC(th)
             y_pred = cl.predict(X)
             cm = metrics.confusion_matrix(y,y_pred)
             tp,fp = cm[1,1],cm[0,1]
             tpr.append(tp/len(y))
             fpr.append(fp/len(y))
         return tpr,fpr
     #+end_src 

     #+RESULTS:





     #+RESULTS:

**** old Summary of Simulation Results                          :exploration:

*** Avalanche statistics of embedded subnetworks and randomly activated background units :exploration:

    An ensemble of \(N_e=50 \) subnetworks, each of size \(N_s=100 \)
    are embedded into a network of \(N_u=1000\) total units. The
    assignment of units to subnetworks is done by uniformly selecting
    without replacement \(N_s\) of the \(N_u\) units for each
    subnetwork independently of each other. 

    The avalanche statistics of this subnetworks are studied under two configurations: 
    A) A randomly selected subnetwork receives external input
    B) N_s randomly selected (background) units receive external input.

    Situation A models an activation of an embedded figure, while in
    situation B the same number of units is activated, but in random
    positions, which resembles background activation against which
    the model should be robust. The task is to discriminate between
    these situations, i.e. to detect the presence of an activated figure.

    For the configuration \(N_s=103,N_e=50\), the resulting avalanche
    distributions are displayed in figure [[ref:fig:avs103_50_A]] and [[ref:fig:avs103_50_B]].

    The green line shows the reference avalanche distribution of a
    critical EHE network of size \(N_s\). The blue line resembles the
    subavalanche distribution of the activated subnetwork in
    situation A and the subavalache distribution of the randomly
    assigned background units that receive external input in
    situation B. The subavalanche distribution in the orange line is
    the avalanche distribution of all non-activated subnetworks taken together.

    The figures show mean values over \(10 \) randomly selected
    ensembles and \(10 \) randomly selected subnetworks in Condition
    A and \(10 \) randomly selected \(N_s\) background units in Condition B

     
    #+CAPTION: \ref{fig:avs103_50_A} Condition A for \(N_s=103,N_e=50\)
    [[file:avalanches/figures/plot103,50,0.png]]
    #+CAPTION: \ref{fig:avs103_50_B} Condition B for \(N_s=103,N_e=50\)
    [[file:avalanches/figures/plot103,50.png]]


     
***** Dependence on the the number of units per subnet
     
      Holding \(N_e=50 \) constant and modifying the number of units
      per subnets \(N_s \), up to ca. \(N_s=116 \) the resulting
      avalanche distribution look qualitatifely similar to the case
      \(N_s=103,N_e=50 \). However, for values \(N_s \geq 116\), the
      subnetworks become supsupcritical and the simulation was stopped. 

      #+CAPTION: Condition A for \(N_s=116,N_e=50 \)
      [[file:avalanches/figures/plot116,50,0.png]]

      #+CAPTION: Condition B for \(N_s=116,N_e=50 \)
      [[file:avalanches/figures/plot116,50.png]]

      #+CAPTION: Condition A for \(N_s=131,N_e=50 \)
      [[file:avalanches/figures/plot131,50,0.png]]

      #+CAPTION: Condition B for \(N_s=131,N_e=50 \)
      [[file:avalanches/figures/plot131,50,0.png]]

      
***** Dependence on the number of ensembles

      Similarly, the performance degrades when increasing the size of
      the ensemble \(N_e \) while keeping \(N_s=50 \). The transition
      to the supsupcritical regeme is ca. at the point \(N_e=66 \)

      #+CAPTION: Condition A for \(N_s=100,N_e=66 \)
      [[file:avalanches/figures/plot100,66,0.png]]

      #+CAPTION: Condition B for \(N_s=100,N_e=66 \)
      [[file:avalanches/figures/plot100,66.png]]

      #+CAPTION: Condition A for \(N_s=100,N_e=77 \)
      [[file:avalanches/figures/plot100,77,0.png]]

      #+CAPTION: Condition B for \(N_s=100,N_e=77 \)
      [[file:avalanches/figures/plot100,77,0.png]]


***** Robustness against additional background Noise in Condition A

      In addition to condition A, now \(K \) randomly selected
      background units (that don't belong to the activated
      subnetworks) are activated and the influence on the avalanche
      distribution of the activated subnetwork is studied. This
      framework turns out to be pretty robust against additional
      background activation. In the condition \(N_s=100,N_e=50\) the
      resulting distributions are shown for \(K=100,223,442 \)

      
      #+CAPTION: Condition A for \(N_s=100,N_e=50,K=100 \)
      [[file:avalanches/figures/plot100,50,100.png]]

      #+CAPTION: Condition A for \(N_s=100,N_e=50,K=233 \)
      [[file:avalanches/figures/plot100,50,233.png]]

      #+CAPTION: Condition A for \(N_s=100,N_e=50,K=442 \)
      [[file:avalanches/figures/plot100,50,422.png]]


**** Discriminating between situations A and B by a readout mechanism

     A readout mechanism to discrimininate between the two situations
     is could use the biggest avalanche size observed in a certain
     time interval to classify whether there is a figure present or if
     the activated units are background noise. This approach is investigated in this section. 
     
     For two situations, one easy (A: \(N_s=100,N_e=53 \) B:
     (\(N_s=100,N_e=51\))) and one near the supsupcritical regeme (A:
     \(N_s=100,N_e=64\) B: \(N_s=100,N_e=61 \)), the distribution of
     the maximum observed avalanche in a timeframe of \(t_{\text{hor}}
     = 1000,100,2000\) external activation steps are plotted, together
     with the decision boundary of a trained naive bayes classifier.

     #+begin_src ipython :session master :tangle no :eval no
       sim_res_background = pickle.load(open('../avalanches/only_sim_res100,53.pickle','rb'))
       sim_res_figure = pickle.load(open('../avalanches/only_sim_res100,51,0.pickle','rb'))[:100]
       fd = figure_detector(1000)
       fd.fit(sim_res_background+sim_res_figure)
     #+end_src 

    

     #+begin_src ipython :session master :tangle no :cache yes :file ./images/20170728_151343_132102jQ.png :tangle no
     fd = figure_detector(1000)
     max_avs_figure = Pipeline(fd.steps[:-1]).fit_transform(sim_res_figure)
     max_avs_background = Pipeline(fd.steps[:-1]).fit_transform(sim_res_background)
     f,ax1 = plt.subplots(figsize=(16,12))
     ax1.hist([[x[0] for x in max_avs_figure],[x[0] for x in max_avs_background]],bins=np.arange(100),histtype='step')
     fd.fit(sim_res_figure + sim_res_background)
     ax1.plot([1000*int(fd.named_steps['classification'].predict([[i]])) for i in range(100)])
     #+end_src 

     #+RESULTS[bb6ad94b3e004707f208eeaf2e1fc5322f389125]:
     [[file:./images/20170728_151343_132102jQ.png]]


     #+begin_src ipython :session master :tangle no :cache yes :file ./images/20170728_151343_132102234jQ.png :tangle no
     fd = figure_detector(1000)
     max_avs_figure = Pipeline(fd.steps[:-1]).fit_transform(sim_res_figure)
     max_avs_background = Pipeline(fd.steps[:-1]).fit_transform(sim_res_background)[:len(max_avs_figure)]
     f,ax1 = plt.subplots(figsize=(16,12))
     ax1.hist([[x[0] for x in max_avs_figure],[x[0] for x in max_avs_background]],bins=np.arange(100),histtype='step')
     fd.named_steps['classification'].fit(max_avs_figure + max_avs_background,[True]*len(max_avs_figure)+[False]*len(max_avs_background))
     ax1.plot([1000*int(fd.named_steps['classification'].predict([[i]])) for i in range(100)])
     #+end_src 

     #+RESULTS[ed8825519343265e7d6888af58c39747dea8e048]:
     [[file:./images/20170728_151343_132102234jQ.png]]


     #+begin_src ipython :session master :tangle no :cache yes :file ./images/20170728_151343_132102jQ234234.png :tangle no
     fd = figure_detector(100)
     max_avs_figure = Pipeline(fd.steps[:-1]).fit_transform(sim_res_figure)
     max_avs_background = Pipeline(fd.steps[:-1]).fit_transform(sim_res_background)
     f,ax1 = plt.subplots(figsize=(16,12))
     ax1.hist([[x[0] for x in max_avs_figure],[x[0] for x in max_avs_background]],bins=np.arange(100),histtype='step')
     fdnamed_steps['classification'].fit(sim_res_figure + sim_res_background)
     ax1.plot([200000*int(fd.named_steps['classification'].predict([[i]])) for i in range(100)])
     #+end_src 

     #+RESULTS[b33c992ca18104c7483f75b3b0952c7c47666b27]:
     [[file:./images/20170728_151343_132102jQ234234.png]]


     #+begin_src ipython :session master :tangle no :cache yes :file ./images/20170728_151343_132102jQ234.png :tangle no
     fd = figure_detector(2000)
     max_avs_figure = Pipeline(fd.steps[:-1]).fit_transform(sim_res_figure)
     max_avs_background = Pipeline(fd.steps[:-1]).fit_transform(sim_res_background)
     f,ax1 = plt.subplots(figsize=(16,12))
     ax1.hist([[x[0] for x in max_avs_figure],[x[0] for x in max_avs_background]],bins=np.arange(100),histtype='step')
     fd.fit(sim_res_figure + sim_res_background)
     ax1.plot([5000*int(fd.named_steps['classification'].predict([[i]])) for i in range(100)])
     #+end_src 

     #+RESULTS[0799382130fe70677d85933a9fa0c0a0b002c858]:
     [[file:./images/20170728_151343_132102jQ234.png]]

     Now for situation (A: \(N_s=100,N_e=64\) B: \(N_s=100,N_e=61 \))

     #+begin_src ipython :session master :tangle no :eval no
       sim_res_background_hard = pickle.load(open('../avalanches/only_sim_res100,61.pickle','rb'))
       sim_res_figure_hard = pickle.load(open('../avalanches/only_sim_res100,64,0.pickle','rb'))[:100]
     #+end_src 



     #+begin_src ipython :session master :tangle no :cache yes :file ./images/20170728_151343_132102j2342Q.png :tangle no
     fd = figure_detector(1000)
     max_avs_figure = Pipeline(fd.steps[:-1]).fit_transform(sim_res_figure_hard)
     max_avs_background = Pipeline(fd.steps[:-1]).fit_transform(sim_res_background_hard)
     f,ax1 = plt.subplots(figsize=(16,12))
     ax1.hist([[x[0] for x in max_avs_figure],[x[0] for x in max_avs_background]],bins=np.arange(100),histtype='step')
     fd.fit(sim_res_figure_hard + sim_res_background_hard)
     ax1.plot([100*int(fd.named_steps['classification'].predict([[i]])) for i in range(100)])
     #+end_src 

     #+RESULTS[d5ac0cf931707fe9ae2371d0ab13d21761a1babd]:
     [[file:./images/20170728_151343_132102j2342Q.png]]


     #+begin_src ipython :session master :tangle no :cache yes :file ./images/20170728_151343_13210w342j2342Q.png :tangle no
     fd = figure_detector(100)
     max_avs_figure = Pipeline(fd.steps[:-1]).fit_transform(sim_res_figure_hard)
     max_avs_background = Pipeline(fd.steps[:-1]).fit_transform(sim_res_background_hard)
     f,ax1 = plt.subplots(figsize=(16,12))
     ax1.hist([[x[0] for x in max_avs_figure],[x[0] for x in max_avs_background]],bins=np.arange(100),histtype='step')
     fd.fit(sim_res_figure_hard + sim_res_background_hard)
     ax1.plot([80000*int(fd.named_steps['classification'].predict([[i]])) for i in range(100)])
     #+end_src 

     #+RESULTS[7a481f66d45c5792bd406cc4ee7f597a2de26470]:
     [[file:./images/20170728_151343_13210w342j2342Q.png]]


     #+begin_src ipython :session master :tangle no :cache yes :file ./images/20170728_151343_13210jkjw342j2342Q.png :tangle no
     fd = figure_detector(2000)
     max_avs_figure = Pipeline(fd.steps[:-1]).fit_transform(sim_res_figure_hard)
     max_avs_background = Pipeline(fd.steps[:-1]).fit_transform(sim_res_background_hard)
     f,(ax1,ax2) = plt.subplots(2,1,figsize=(16,12))
     ax1.hist([[x[0] for x in max_avs_figure],[x[0] for x in max_avs_background]],bins=np.arange(100))
     fd.fit(sim_res_figure_hard + sim_res_background_hard)
     ax2.plot([int(fd.named_steps['classification'].predict([[i]])) for i in range(100)])
     #+end_src 

     #+RESULTS[ced1bc9ed8068ec1b800089ab4ab373d70aa0885]:
     [[file:./images/20170728_151343_13210jkjw342j2342Q.png]]

     For reference the code for the classifier, which is written as simple pipeline using scikit learn: 

     #+begin_src ipython :session master
        from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin
        from sklearn.naive_bayes import GaussianNB
        from imblearn.pipeline import Pipeline

        class TimeHorizonAvalancheExtractor(BaseEstimator):
            """Takes the avalanches during the simulation as input and transforms them to
               the largest avalanche size in the observed external input time steps num_obs"""

            def  __init__(self,t_hor=100):
                self.t_hor = t_hor

            def fit(self,trials,y=None):
                return self

            def sample(self,sim_results,labels=None):
                X,y = ([],[])
                for sim_res in sim_results:
                    # split the simulation results into bins of t_hor external time steps
                    # see if it is a background or a figure activation
                    figure_simulation = 'act' in sim_res.keys()
                    avs,avs_inds,act_inds = (sim_res['avs'],sim_res['avs_inds'],sim_res['act_inds'])
                    e = load_module('ehe_detailed').EHE()
                    avs_sizes,avs_durations = e.get_avs_size_and_duration(avs,avs_inds)
                    duration_between_avs = np.diff(act_inds)
                    time = np.cumsum(duration_between_avs)
                    t_hor_rem = (time/self.t_hor).astype(int)
                    splitpoints = np.searchsorted(t_hor_rem,np.arange(1,t_hor_rem[-1]))
                    # split the avs_sizes at each point where time becomes another multiple of t_hor
                    observations = np.split(avs_sizes,splitpoints)
                    X.extend(observations)
                    y.extend([figure_simulation]*len(observations))
                return (X,y)

            def fit_sample(self, X, y=None):
                return self.fit(X,y).sample(X,y)

        class MaxAvalancheExtractor(BaseEstimator,TransformerMixin):
            """ Transforms the given avalanche sizes to their maximum value"""
            def __init__(self):
                pass

            def transform(self,X,y=None):
                return [[np.max(np.hstack(([0],avs_sizes)))] for avs_sizes in X]

            def fit(self,X,y=None):
                print("hi from fit, self is ",self)
                return self

        def figure_detector(t_hor):
            return Pipeline([('extract_t_hor',TimeHorizonAvalancheExtractor(t_hor)),
                             ('max_avs_size',MaxAvalancheExtractor()),
                             ('classification',GaussianNB())])
     #+end_src 

** Non 2afc approaches for connectivity matrices                :exploration:

*** Critical network with nonconstant connectivity matrix    :implementation:

    Almost simplest case, two weights \(w_1 \) and \(w_2 \) which are
    assigned to overlap- and non-overlap units respectively. Thus
    the columns of W are either constant \(w_1 \) or \(w_2 \). 

    Hypothesis if each unit on average receives the same input from
    other neurons as in the critical normal ehe model, it itself will
    display critical avalanche statistics.

    #+begin_src ipython :session master
      def simple_overlap_matrix(N,overlap_inds,w_ovl):
          W = np.zeros((N,N))
          N_alpha_crit = (1 - 1/np.sqrt(N))
          w_nonovl = (N_alpha_crit - len(overlap_inds)*w_ovl)/(N - len(overlap_inds))
          W[:,overlap_inds] = w_ovl;
          W[:,np.setdiff1d(np.arange(N),overlap_inds)] = w_nonovl
          return W
    
    #+end_src 
*** Implementation                                           :implementation:

    #+begin_src ipython :session master :file ./images/20170608_165257_2271UmN2.png :tangle no :eval no
   ehe = load_module('ehe_detailed')
   avs_indices = np.load("../avalanches/const_crit_avalanches_detailed_10000/inds1.npy")
   avs_detailed = np.load("../avalanches/const_crit_avalanches_detailed_10000/avs1.npy")
   (avs_sizes,avs_durations) = ehe.get_avs_size_and_duration(avs_detailed,avs_indices)
   loglogplot(avs_sizes)
    #+end_src 

    #+RESULTS:
    [[file:./images/20170608_165257_2271UmN2.png]]

*** Weight matrices for subnetworks
   

    The images in this section show the empirical distribution of
    avalanche sizes in the specified subnetwork (blue) and a simulated
    power law of the expected distribution with the same number of
    observations in green.
   
**** const critical weights in just one subnetwork
     - Hypothesis:
       - Critical in this subnetwork
       - subcritical in networks overlapping
   
    
     :implementation:
     #+begin_src ipython :session master
       import numpy as np
       def const_critical_subnetwork_weights(N_whole,inds_sub,w = None):
           W = np.zeros((N_whole,N_whole))
           inds = np.array(inds_sub)
           if w is None:
               w = (1 - 1/np.sqrt(len(inds)))/len(inds)
           W[np.ix_(inds,inds)] = w
           return W
     #+end_src 

     #+RESULTS:

     #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       ehe = load_module('ehe_detailed')
       N_whole = 10000;
       N_sub = 1000;
       inds_sub = list(range(N_sub))
       W = const_critical_subnetwork_weights(N_whole,inds_sub)
       print(W)
       avs,avs_inds = ehe.simulate_model_mat(np.random.random(N_whole),10*N_sub,W,deltaU)
       np.save(outdir+"/avs"+str(task_id),avs)
       np.save(outdir+"/avs_inds"+str(task_id),avs_inds)
       """,name='const_crit_subnetwork1e31e4',servername='server',task_ids='1-10')
     #+end_src 




     #+RESULTS:


     #+begin_src ipython :session master :file ./images/20170618_132247_3982MxR.png :tangle no :eval no
     ehe = load_module('ehe_detailed')
     avs_detailed = np.load('../avalanches/const_crit_subnetwork1e31e4/avsconcatenated.npy')
     avs_indices = np.load('../avalanches/const_crit_subnetwork1e31e4/avs_indsconcatenated.npy')
     f,(ax1,ax2) = plt.subplots(2,1,figsize=(16,12))
     ax1.set_title('avalanche distribution subnetwork')
     (sub_avs_detailed,sub_avs_indices) = ehe.subnetwork_avalanches(avs_detailed,avs_indices,set(range(1000)))
     (sub_avs_sizes,sub_avs_durations) = ehe.get_avs_size_and_duration(sub_avs_detailed,sub_avs_indices)
     loglogplot(sub_avs_sizes,ax=ax1)
     loglogplot(draw_sample_discrete(1,1000,3/2,N=len(sub_avs_sizes)),ax=ax1)
     ax2.set_title('avalanche distribution whole network')
     (avs_sizes,avs_durations) = ehe.get_avs_size_and_duration(avs_detailed,avs_indices)
     loglogplot(avs_sizes,ax=ax2)
     loglogplot(draw_sample_discrete(1,1000,3/2,N=len(avs_sizes)),ax=ax2)

     #+end_src 

     #+RESULTS:
     [[file:./images/20170618_132247_3982MxR.png]]
     :END:


     [[file:./images/20170618_132247_3982MxR.png]]

**** const critical weights in non overlapping subnetworks
     - Hypothesis:
       same as above


     :implementation:
     #+begin_src ipython :session master :tangle no :eval no
           qsub("""
           ehe = load_module('ehe_detailed')
           N_sub = 1000;
           N_whole = 10*N_sub;
           inds_sub = list(range(N_sub))
           inds_sub2 = list(range(3*N_sub,4*N_sub))
           W = const_critical_subnetwork_weights(N_whole,inds_sub)
           W += const_critical_subnetwork_weights(N_whole,inds_sub2)
           print(W)
           avs,avs_inds = ehe.simulate_model_mat(np.random.random(N_whole),10*N_sub,W,deltaU)
           np.save(outdir+"/avs"+str(task_id),avs)
           np.save(outdir+"/avs_inds"+str(task_id),avs_inds)
           """,name='const_crit_subnetwork1e31e4_nonoverlapping',servername='server',task_ids='11-30')
     #+end_src 

     #+begin_src ipython :session master :file ./images/20170618_132247_3982MxR2.png :tangle no :eval no
     ehe = load_module('ehe_detailed')
     avs_detailed = np.load('../avalanches/const_crit_subnetwork1e31e4_nonoverlapping/avsconcatenated.npy')
     avs_indices = np.load('../avalanches/const_crit_subnetwork1e31e4_nonoverlapping/avs_indsconcatenated.npy')
     f,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(16,12))
     ax1.set_title('avalanche distribution subnetwork')
     (sub_avs_detailed,sub_avs_indices) = ehe.subnetwork_avalanches(avs_detailed,avs_indices,set(range(1000)))
     (sub_avs_sizes,sub_avs_durations) = ehe.get_avs_size_and_duration(sub_avs_detailed,sub_avs_indices)
     loglogplot(sub_avs_sizes,ax=ax1)
     loglogplot(draw_sample_discrete(1,1000,3/2,N=len(sub_avs_sizes)),ax=ax1)
     ax2.set_title('avalanche distribution subnetwork2')
     (sub_avs_detailed,sub_avs_indices) = ehe.subnetwork_avalanches(avs_detailed,avs_indices,set(range(3000,4000)))
     (sub_avs_sizes,sub_avs_durations) = ehe.get_avs_size_and_duration(sub_avs_detailed,sub_avs_indices)
     loglogplot(sub_avs_sizes,ax=ax2)
     loglogplot(draw_sample_discrete(1,1000,3/2,N=len(sub_avs_sizes)),ax=ax2)
     ax3.set_title('avalanche distribution whole network')
     (avs_sizes,avs_durations) = ehe.get_avs_size_and_duration(avs_detailed,avs_indices)
     loglogplot(avs_sizes,ax=ax3)
     loglogplot(draw_sample_discrete(1,1000,3/2,N=len(avs_sizes)),ax=ax3)

     #+end_src 

     #+RESULTS:
     [[file:./images/20170618_132247_3982MxR2.png]]
     :END:
     [[file:./images/20170618_132247_3982MxR2.png]]

**** regular overlapping networks from Nergis
     - Hypothesis: 
       - Inhibition required to manage crosstalk

        
     :implementation:
     First normal overlap without inhibition 50%
     #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       ehe = load_module('ehe_detailed')
       N_sub = 1000;
       N_whole = 10*N_sub;
       inds_sub = list(range(N_sub))
       inds_sub2 = list(range(int(0.5*N_sub),int(1.5*N_sub)))
       W = const_critical_subnetwork_weights(N_whole,inds_sub)
       W[W==0] += const_critical_subnetwork_weights(N_whole,inds_sub2)[W==0]
       print(W)
       avs,avs_inds = ehe.simulate_model_mat(np.random.random(N_whole),10*N_sub,W,deltaU)
       np.save(outdir+"/avs"+str(task_id),avs)
       np.save(outdir+"/avs_inds"+str(task_id),avs_inds)
       """,name='const_crit_subnetwork1e31e4_overlapping50',servername='server',task_ids='1-10',
           post_command="concat_detailed(outdir+'/avs',outdir+'/avs_inds',range(1,11))")
     #+end_src 


     10%
     #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       ehe = load_module('ehe_detailed')
       N_sub = 1000;
       N_whole = 10*N_sub;
       inds_sub = list(range(N_sub))
       inds_sub2 = list(range(int(0.9*N_sub),int(1.9*N_sub)))
       W = const_critical_subnetwork_weights(N_whole,inds_sub)
       W[W==0] += const_critical_subnetwork_weights(N_whole,inds_sub2)[W==0]
       print(W)
       avs,avs_inds = ehe.simulate_model_mat(np.random.random(N_whole),10*N_sub,W,deltaU)
       np.save(outdir+"/avs"+str(task_id),avs)
       np.save(outdir+"/avs_inds"+str(task_id),avs_inds)
       """,name='const_crit_subnetwork1e31e4_overlapping10',servername='server',task_ids='1-10',
           post_command="concat_detailed(outdir+'/avs',outdir+'/avs_inds',range(1,11))")
     #+end_src
     :END:
 
     Results: Overlap of critical networks without inhibition leads to
     supercritical network where avalanche sizes grow unbounded!

**** Gaussian connectivity
     - See whether gaussian with small noise amplitude around critical value is ok

       #+begin_src ipython :session master :eval no :tangle no
         qsub("""
         import time
         N = 10000;
         W = (0.8 +np.random.normal(0,0.1,(N,N)))/N
         num_avs = int(1e7)
         ehe = load_module('ehe_detailed')
         e = ehe.EHE();
         start = time.time()
         e.simulate_model_mat(np.random.random(N),num_avs,W,deltaU);
         end = time.time()
         print("elapsed time ",end-start)
         np.save(outdir+"/avs"+str(task_id),e.avs)
         np.save(outdir+"/avs_inds"+str(task_id),e.avs_inds)
         """,name='avs_gauss',max_threads=8,servername='server_inline',queue='maximus',execute=False)
       #+end_src 


     #+begin_src ipython :session master :eval no :tangle no
            qsub("""
            import time
            N = 10000;
            W = (0.8 +np.random.normal(0,0.1,(N,N)))/N
            num_avs = int(1e7)
            ehe = load_module('ehe_detailed')
            e = ehe.EHE();
            start = time.time()
            e.simulate_model_mat(np.random.random(N),10000,W,deltaU);
            end = time.time()
            print("elapsed time ",end-start)
            np.save(outdir+"/avs"+str(task_id),e.avs)
            np.save(outdir+"/avs_inds"+str(task_id),e.avs_inds)
            """,name='gauss_fast',servername='server_inline',queue='maximus+long64_gb',
            post_command="concat_detailed(outdir+'/avs',outdir+'/avs_inds',range(1,2))",
                 task_ids='1')
     #+end_src 

     #+begin_src ipython :session master
             from matplotlib.backends.backend_pdf import PdfPages
             import matplotlib.pyplot as plt

             def multipage(filename, figs=None, dpi=200):
                 pp = PdfPages(filename)
                 if figs is None:
                     figs = [plt.figure(n) for n in plt.get_fignums()]
                 for fig in figs:
                     fig.savefig(pp, format='pdf')
                 pp.close()

     #+end_src 

* Feature integration with critical subnetworks\label{chap:fics}
  
  In this chapter we describe in detail the simulation studies used to
  evaluate whether embedded figure networks which display critical
  activation offer good computational abilities for feature integration.

  After reviewing the dynamical regimes of the generalized EHE model
  and the avalanche size distributions resulting from a the homogenous
  system citep:eurich2002finite in section [[ref:seq:ehe-introduction]] we
  introduce the problem of constructing a weight matrix with embedded
  critical subnetworks in section [[ref:sec:embedding]] and analyze the
  transition to supercriticality of the model in dependence of number
  and size of embedded feature subnetworks.

  In section [[ref:sec:separability]] we show that the models with
  embedded subnetworks exhibit well separated dynamics depending on
  whether a full feature subnetwork is activated (figure activation)
  or a randomly chosen network (distractor activation).
  We start by introducing the simplified network model used in this
  study, which is a generalization of the EHE-model (cite:eurich2002finite).

  We say a network of EHE-model units is in the critical state, if the
  avalanche size distribution is close to a power law. In the critical
  state, (avalanche) events occur on all possible spatial and temporal
  scales, leading to the observation of power laws where large events
  are much more probable than in exponential distributions.
  
** The generalized EHE model \label{seq:ehe-introduction}
   In this thesis we will analyse and derive the probability
   distribution for the spiking behavior of a generalization of the EHE
   model proposed in cite:eurich2002finite. The dynamics of the system
   is determined by

   \begin{align}
   \label{eq:ehe-dynamics}
   \tilde{u}_i(t+1) = u_i(t) + I_i^{\text{ext}}(t) + I_i^{\text{int}}(t)\\
   u_i(t+1) = 
   \begin{cases}
     \tilde{u}_i(t+1) \mbox{ if } \tilde{u}_i(t + 1) < 1 \\
     \tilde{u}_i(t+1) - 1 \mbox{ otherwise.}
   \end{cases}
   \end{align}

   We call \(u_i \) the activation value of unit \(i=1\ldots N \),
   with starting value in \([0,1]\) for all units. Throughout this
   thesis, \(N\) represents the number of neurons in the network (the
   system size).

   If a unit crosses the threshold i.e \(\tilde{u}_i > 1\), internal
   activation \(I_j^{\text{int}}\) is given to all units \(j = 1\ldots
   N\).
   When using this system as a model for neural networks,
   the internal input corresponds to a unit giving input to other neurons
   by sending a spike to postsynaptic neurons.

   
   \(I_i^{\text{ext}} \) represents external activation which is given in each time step
   to a randomly chosen unit, which is assumed to occur at a much
   slower time scale and represents, for example sensory input given to the
   population of neurons.

   The EHE model explicitly assumes an /infinite separation of
   timescales/, so that external input is only given to a unit after
   all internal activations have died out. If the external input
   causes a unit to cross the threshold, an avalanche of internal
   inputs occurr: The activated unit resets its energy and internal
   activation is given to all connected units. This may push some of
   these units above the threshold which in turn send internal
   activation to the neurons connected to them. 

   In order to model this behavior, we introduce the vector \(A(t)\)
   which indicates which units are above the threshold.
   \[   A_i(t) = \delta[\tilde{u}_i(t) > 1] \text{, }\]
   where \(\delta[cond]\) is \(1\) if the condition is true and \(0\)
   otherwise. The spreading of internal activation to the connected
   units is mediated by a weight matrix \(W = (w_{ij})_{i,j=1,\ldots,N}\) with \(w_{ij}\)
   representing the internal activation given to unit \(i\) when unit \(j\)
   crossed the threshold.
   \[I^{\text{int}}(t) = W A(t-1)\].  
   External input is applied to the randomly chosen unit \(r(t)\),
   where \(r(t) \sim \text{UNI}(\mathcal{M}), \mathcal{M} \subseteq
   \mathcal{N}\) and only when no internal activation is given to the system:
   \[I_i^{\text{ext}}(t) = \delta \left [r(t) = i \wedge |A(t)|_1 = 0\right ] \Delta U  \text{ .}\]
   If a unit crosses the threshold, an avalanche starts. The avalanche
   duration \(D(t)\) for the avalanche started by external input at
   time \(t - 1\) is defined as 
   \[D(t) := \delta[|A(t-1)|_1 = 0]\inf_{i\in
   \mathbb{N}}|A(t + i)|_1 = 0 \text{ .}\]
   The size of the avalance is given by the number of units
   crossing the threshold 
   \[L(t) := \delta[|A(t-1)|_1 = 0]\sum_{\tau = t}^{t + D(t)}|A(\tau)|_1 \text{ .}\]
   Note that due to the assumption of infinite separation of
   timescales \(t\) does not linearly represent time but merely steps
   in the simulation. All steps \(t\) with \(|A(t)|_1\neq 0\) are on
   the fast timescale. The time \(T\) measured on the slow time scale
   of the external input is given by \(t-\sum_{\tau = 0}^tD(\tau) \)
   if \(|A(t)|_1 = 0\). In the following we will use the word steps to
   denote a step on the slow time scale. In chapter [[ref:chap:gen-ehe]]
   we use a different approach and model this system explicitly using
   the separation of timescales as a discrete time skew-product
   dynamical system. 

   This dynamical system can be seen as a simplified model of a neural
   network. The activation value \(u_i\) represents the membrane
   potential of a neuron with 0 representing the resting potential. If
   which is above the resting potential (0) and induces a spike when
   it crosses a threshold, here set to \(1\). The spike is send to the
   other connected neurons. Neurons \(u_i\) is connected to neuron
   \(u_j\) if \(w_{ji} \neq 0\) and when \(u_i\) spikes, it sends
   internal input to \(u_j\) that increments its activation value by
   \(w_{ji}\). 
   
   However in this model neurons are /non-leaky/, i.e. their membrane
   potential does not decay to zero over time when receives no other
   activation. Additionally, after a unit spiked the membrane
   potential is not resetted to the resting state but wraps around
   without dissipation. We also note that when allowing (\(w_{ij} <
   0)\), no lower bound is given for the activation value of the
   units. In this thesis we did not check the results for robustness
   against weaker assumptions and a more realistic model, for example
   a network of integrate- and fire neurons. 

   # However
   # [[cite:levina2008mathematical][Sections 5.6,5.7]] report succesful bb 

   
  
*** Avalanche size distributions in the homogeneous EHE Model \label{sec:pls} 
  
    The model published in cite:eurich2002finite, here called the
    homogeneous EHE model is recovered with the choice of \(W =
    (\frac{\alpha}{N})_{i,j \in \{1,\ldots,N\}} \). Even with a
    constant coupling matrix, depending on the choice of \(\alpha \) it
    is capable of generating different regimes of avalanche statistics,
    which range from an exponential, subcritical distribution through
    the critical point, where the avalanche size distribution is a power law
    for almost the complete system size, to a supercritical and
    multipeaked regime. Increasing \(\alpha \) even further, avalanches
    can become infinitely large. This behaviour is displayed in figure [[ref:fig:ehe]].

    #+ATTR_LATEX: width:500px
    #+CAPTION: \label{fig:ehe} Probability distributions of avalanche sizes, \(P(x, N, \alpha)\), and avalanche 
    #+CAPTION: durations, \(p_{d}(x, N, \alpha)\) , in the subcritical (a; \(\alpha\) = 0.8), critical (b; \(\alpha\) = 0.99), supra-critical (c; \(\alpha\) = 0.999), and multi-peaked (d; \(\alpha\) = 0.99997) regime. (a-c) Solid lines and symbols denote the analytical and the numerical results for the avalanche size distributions, respectively. In (d), the solid line shows the numerically calculated avalanche size distribution. The dashed lines in (a-d) show the numerically evaluated avalanche duration distributions. In all cases, the presented curves are temporal averages over 107 avalanches with N = 10000, and \(\Delta U\) = 0.022. Figure and caption taken from cite:eurich2002finite.
    [[/home/kima/Dropbox/uni/master//images/20171125_210542_2721P0p.png]]

    cite:eurich2002finite derived the closed form expression
    [[eqref:eq:closed-form-size-ehe]] for the avalanche size distribution
    using considerations involving the equilibrium density of the phase
    space under the assumption of ergodicity. These arguments will be extended for
    general matrices in chapter [[ref:chap:gen-ehe]] and the ergodicity
    will be shown for the homogenous case. 

    The analytically derived avalanche size density perfectly matches the
    avalanche density obtained through model simulations for the
    regimes where avalanche sizes cannot exceed the system size.

    \begin{align}
    \label{eq:closed-form-size-ehe}
    p(L,N,\alpha) = L^{L-2}{N-1 \choose L-1}\left ( \frac{\alpha}{N} \right )^{L-1}\left (1 - L \frac{\alpha}{N}\right )^{N-L-1} \frac{N(1-\alpha)}{N-(N-1)\alpha} \text{ for } 1 \leq L \leq N 
    \end{align}

    In the limit \(N \rightarrow \infty \), large avalanches are
    distributed according to a power law: the local exponent of the
    avalanche size distribution converges to \(-\frac{3}{2} \) for \(L \rightarrow \infty \) [[cite:eurich2002finite][Eq. (9)]]

    \begin{align*}
    \gamma = \lim_{L\rightarrow \infty }\lim_{\alpha\rightarrow 1}\lim_{N\rightarrow \infty}\frac{\ln \frac{p(L,N,\alpha)}{p(L+1,N,\alpha)}}{\ln \frac{L}{L+1}} = -\frac{3}{2} \text{ .}
    \end{align*}

    However, we can only perform simulations on finite size networks.
    For this case, the avalanche distribution is not a real power law
    but can be seen to have a long stretch where it fits to a log log slope of \(-\frac{3}{2}\).

    To find the coupling strength where the avalanche statistics most
    closely resembles a power law, cite:eurich2002finite
    performed a numerical search for the parameter
    \(\alpha_{\text{crit}}(N) \) minimizing the \emph{distance} to an
    ideal power law \(\tilde{p}(L,N) =
    \frac{L^{-\frac{3}{2}}}{\sum_{L=1}^NL^{-\frac{3}{2}}} \) for system
    sizes ranging from \(10^2 \) to \(10^7 \). This \emph{distance}
    between distributions was measured by the symmetric version of the
    Kullback-Leibler divergence \(D_{KL}^{\text{sym}}(P,Q) =
    \sum_i(P(i)-Q(i))\left (\ln(P(i)) - \ln(Q(i))\right )\) between
    \(p(L,N,\alpha)\) and \(\tilde{p}(L,N)\).

    Using the symmetric KL-Divergence one has a divergence measure to
    evaluate how different two distributions are. The KL divergence is
    not a metric (since the triangular inequality does not hold) but
    it is nonnegative and zero if and only if its two arguments are
    the same. It is often used due to its relevance in information
    theoretic terms: While D_{KL}(P,Q) measures the expected number of
    extra bits required to code samples from P using a code optimized
    for Q rather than the code optimized for P
    [[cite:cover2006elements][Section 2.3]],
    \(D_{KL}^{\text{sym}}(P,Q)\) represents the average number of
    extra bits if one distribution is coded with respect to the other
    one.

    For finite size \(N\) the system minimizes the distance to a power
    law for the /critical coupling weight/
    \[\alpha_{\text{crit}}(N) = \frac{1- \frac{1}{\sqrt{N}}}{N} \text{ .}\]
   
    In order to see that the avalanche size distributions for the critical
    coupling scale with the exponent \(-\frac{3}{2}\) and how the
    exponential cutoff for large avalanche sizes is related to the
    system size we check how whether the power law cutoff scales with the system size.
    
*** Finite size scaling \label{sec:finits-size-scaling}
    
    This section is based on [[cite:levina2008mathematical][Section
    3.4]] using the method described in cite:PhysRevA.39.6524.
    
    Assuming the finite size scaling ansatz, finite sized critical
    avalanches should follow a distribution given by \(P_c(L,N) =
    N^{-\sigma}g(\frac{L}{N^{\nu}}) \text{ ,}\) with \(\sigma,\nu > 0, g
    \in C(\mathbb{R}_+), g(0)\in R\). In particular \(\sigma \)
    controls the slope of the avalanche and \(\nu \) how the finite
    size cutoff relates to the system size. For the homogeneous EHE
    model the correct exponents are \(\sigma = \frac{3}{2},\nu = 1\).

    In order to verify this theoretical finite size scaling relation
    
    \[P_{c}(L,N) = N^{-\frac{3}{2}}g\left(\frac{L}{N}\right ) \] 
    
    we check if avalanche size distributions for different system
    sizes collapse to a single line \(g(x)\) when
    \(N^{\frac{3}{2}}P_c(L,N)\)is plotted in dependence of \(x =
    \frac{L}{N}\).

    This is done in figure [[ref:fig:finite-size-scaling]] for system
    sizes ranging from 100 to 10000. The left panel shows avalanche
    distributions sampled over \(10^{8}\) avalanches from the EHE
    model with coupling strength \(\alpha_{\text{crit}}(N)\). All these avalanche
    size distributions collapse to \(g(\frac{L}{N})\) after rescaling.
    One observes a system size cutoff around \(\frac{L}{N} \approx 0.85\).

    :implementation:
    #+begin_src ipython :session master :tangle no :eval no
      Ns = [100,300,500,1000,
            3000,5000,7000,10000]
      avalanches = {N:(print(N),np.load("../avalanches/finite_size_scaling/avs"+str(N)+".npy"))[1] for N in Ns}
      pdfs = {N:points2pdf(avalanches[N]) for N in Ns}
      pdfs_rescaled = {N:scale_avalanches(1.5,avalanches[N],N) for N in Ns}
      pickle.dump(pdfs,open("../avalanches/finite_size_scaling/pdfs.pickle","wb"))
      pickle.dump(pdfs_rescaled,open("../avalanches/finite_size_scaling/pdfs_rescaled.pickle","wb"))
      pdfs_no_trans = {N:points2pdf(avalanches[N][10000:]) for N in Ns}
      pdfs_rescaled_no_trans = {N:scale_avalanches(1.5,avalanches[N][10000:],N) for N in Ns}
      pickle.dump(pdfs_no_trans,open("../avalanches/finite_size_scaling/pdfs_no_trans.pickle","wb"))
      pickle.dump(pdfs_rescaled_no_trans,open("../avalanches/finite_size_scaling/pdfs_rescaled_no_trans.pickle","wb"))
    #+end_src 

    #+begin_src ipython :session master 
      pdfs = pickle.load(open("../avalanches/finite_size_scaling/pdfs_no_trans.pickle","rb"))
      pdfs_rescaled = pickle.load(open("../avalanches/finite_size_scaling/pdfs_rescaled_no_trans.pickle","rb"))
      N_s = np.sort(list(pdfs.keys()))
      f,(ax1,ax2) = plt.subplots(1,2,figsize=(10,4))
      plt.rc('text', usetex=True)
      plt.rc('font', family='serif')
      for N in N_s:
          loglogplot(pdfs[N],ax=ax1,from_pdf=True,label=r'$N='+str(N)+'$')
          loglogplot(pdfs_rescaled[N],ax=ax2,from_pdf=True,label=r'$N='+str(N)+'$')
          ax1.set_title(r'Avalanche size distributions for different $N$')
          ax1.set_xlabel(r'$L$')
          ax1.set_ylabel(r'$P_c(L,N)$')
          ax2.set_title('Descaled avalanche size distributions') 
          ax2.set_xlabel(r'$L/N$')
          ax2.set_ylabel(r'$N^{1.5}P_c(L,N)$')

      handles, labels = ax1.get_legend_handles_labels()
      ax1.legend(handles, labels)
      handles, labels = ax2.get_legend_handles_labels()
      ax2.legend(handles, labels)
      f.savefig('../images/finite_size_scaling.pdf', dpi=300)

    #+end_src 

    #+RESULTS:
    : <matplotlib.figure.Figure at 0x7f3d418f16a0>

    :END:

    #+CAPTION: (Left) Critical avalanche distribution from \(10^{8}\) simulated avalanches. 
    #+CAPTION: (Right) When plotting \(N^{1.5}P_c(L,N)\) over \(\frac{L}{N}\) all distributions collapse to a single line
    #+CAPTION: corresponding to the \(g(\frac{L}{N})\) in the finite size scaling ansatz. \label{fig:finite-size-scaling}
    #+ATTR_LATEX: :width \linewidth
    [[file:images/finite_size_scaling.pdf]]

    
    
**** Quantifying distance to power law                       :implementation:

     In [[cite:levina2008mathematical][&& page 43]] a symmetric
     KL-divergence smaller than 0.005 is taken to be a very good fit to
     a power law. However, this fixed value is of course dependent on
     the system size and the number of samples obtained. The values of
     the symmetric KL distance to the ideal discrete power law is shown
     for the critical regime in figure for system sizes 100,1000,10000
     and different number of samples.

     # In this thesis we use the symmetric KL-divergence as a
     # nonparametric test statistic with confidence level computed by a
     # bootsrap sample of discrete power laws. This follows the
     # suggestions in cite:deluca2013fitting and cite:corral2012practical.

     # This method to evaluate the goodness of fit a reference
     # distribution to a target distribution is conceptually simple but
     # requires some computation time. It consists of sampling a
     # sufficient large number (for example \(N_{\text{samples}} = 1000
     # \) of i.i.d samples from the target distribution, each sample
     # having the same sample size and as the reference distribution. For
     # each of this samples the test statistic \(\operatorname{ts}(s)\)is
     # calculated and stored. To give a confidence level ('p_value') for
     # the Null-hypothesis that reference and target have the ssame
     # distribution, the value of the test statistic of the target is
     # compared to the values obtained on the bootstrap samples \(p =
     # \frac{\#\{s \in \text{samples}|\operatorname{ts}(s) \leq
     # \operatorname{ts}(\operatorname{target})\}}{N_{\text{samples}}}
     # \). \(p \) is thus a numerical approximation of the probability
     # that samples drawn from the target distribution have a value of the
     # test statistic smaller or equal as the reference distribution.

     # Like proposed in cite:corral2012practical, we generate samples of
     # discrete power laws with a generalized rejection sampling method
     # that does not need to compute the normalization constants. Figure
     # blablabup shows the convergence of the symm. KL Distance of
     # samples to the target power law in dependence of the sample size.
    
     # :implementation:
     # #+begin_src ipython :session master :file ./images/20171128_125726_2932431w.png :tangle no
     # num_samples = [10**x for x in [3,4,5,6]]
     # plt.loglog(num_samples,[sym_kl(points2pdf(dpl[:ns]),-3/2,90,1) for ns in num_samples])
     # #+end_src 

     # #+RESULTS:
     # [[file:./images/20171128_125726_2932431w.png]]
     # :END:

     # #+ATTR_LATEX: :width 400px
     # [[file:./images/20171128_125726_2932431w.png]]


     # We note that this is not a full procedure to check whether a given
     # dataset is power law distributed. For this case, one would have to
     # estimate the parameters of the power law (exponent and optionally
     # the cut-offs) at the same time. There are several methods
     # published, the most well known being cite:clauset2009power and
     # more recently (and handling both upper and lower cutoffs)
     # cite:deluca2013fitting. We don't use the full methods here since
     # the exponential of the limiting power law distribution is known
     # theoretically as well as the dependence of the cutoff from the
     # system size (section [[ref:sec:finits-size-scaling]]). The usage of
     # the symmetric KL-Divergence as test statistic is motivated by it's
     # use in previous publications investigating EHE type models
     # cite:eurich2002finite,levina2008mathematical.

     Figure bla shown values of the KL divergence and calculated
     \(p-\)values for the critical EHE model of sizes 100,1000,10000
     with sample size chosen to be 10 times the system size to a
     discrete power law with upper cutoff at 90% of the system size.
    
     :implementation:
     #+begin_src ipython :session master :file ./images/20170604_235118_1882Jr13423423.png :tangle no :cache yes
       recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
                               pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
       f,axs = plt.subplots(1,3,figsize=(16,6))
       for k,ax in zip(['crit','subcrit','supcrit'],np.reshape(axs,[-1])):
           avalanches = np.array(recreate_ehe_plot_res[k][0])
           dist = sym_kl(points2pdf(avalanches),-3/2,10000,5)
           loglogplot(avalanches,ax)
           ax.set_title('symmetric KL-Divergence: '+str(dist))
     #+end_src 

     #+RESULTS[abd399ba5a1d45707fb5fee2dcaf90be9b8199b0]:
     [[file:./images/20170604_235118_1882Jr13423423.png]]

    
     #+begin_src ipython :session master :file ./images/20170604_235118_1882Jölkjr13423423.png :tangle no :cache yes
       recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
                               pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
       f,axs = plt.subplots(1,3,figsize=(16,6))
       for k,ax in zip(['crit','subcrit','supcrit'],np.reshape(axs,[-1])):
           avalanches = np.array(recreate_ehe_plot_res[k][0])
           dist = sym_kl_old(points2pdf(avalanches),-3/2,10000)
           loglogplot(avalanches,ax)
           ax.set_title('symmetric KL-Divergence: '+str(dist))
     #+end_src 

     #+RESULTS[522c3dc43d1eb6bfa27bebdf5e682eda7ba11032]:
     [[file:./images/20170604_235118_1882Jölkjr13423423.png]]

     #+begin_src ipython :session master :tangle no :cache yes
       avs100   = np.load("../avalanches/finite_size_scaling/avs100.npy")
       avs1000  = np.load("../avalanches/finite_size_scaling/avs1000.npy")
       avs10000 = np.load("../avalanches/finite_size_scaling/avs10000.npy")
       pdf_avs100   = points2pdf(avs100[10000:])
       pdf_avs1000  = points2pdf(avs1000[10000:])
       pdf_avs10000 = points2pdf(avs10000[10000:])
     #+end_src


     #+RESULTS[6945f5878017bca294bba4e87feafa89352a2479]:

     #+begin_src ipython :session master :tangle no 
    transiente = 10000
    pdf_10N_avs100   = points2pdf(avs100[transiente:transiente + 10*100])
    pdf_10N_avs1000  = points2pdf(avs1000[5*transiente:5*transiente + 10*1000])
    pdf_10N_avs10000 = points2pdf(avs10000[transiente:transiente + 10*10000])
     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master :tangle no
    pdf_100N_avs100   = points2pdf(avs100[transiente:transiente + 100*100])
    pdf_100N_avs1000  = points2pdf(avs1000[5*transiente:5*transiente + 100*1000])
    pdf_100N_avs10000 = points2pdf(avs10000[transiente:transiente + 100*10000])

     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master :tangle no :results output
    dpl_100_N10up9   = pickle.load(open("../avalanches/discrete_pl_samples/dpl_100_N10.pickle","rb"))
    dpl100090  = pickle.load(open("../avalanches/discrete_pl_samples/dpl100090.pickle","rb"))
    dpl_1000_N10up9  = np.split(dpl100090,1000)
    dpl1000090 = pickle.load(open("../avalanches/discrete_pl_samples/dpl1000090.pickle","rb"))
    dpl_10000_N10up9 = np.split(dpl1000090,1000)
    print(len(dpl_100_N10up9[-1]),len(dpl_1000_N10up9[-1]),len(dpl_10000_N10up9[-1]))
    distances_100_N10up9   = np.array([sym_kl(points2pdf(dpls),-3/2,90,1) for dpls in dpl_100_N10up9])
    distances_1000_N10up9  = np.array([sym_kl(points2pdf(dpls),-3/2,900,1) for dpls in dpl_1000_N10up9])
    distances_10000_N10up9 = np.array([sym_kl(points2pdf(dpls),-3/2,9000,1) for dpls in dpl_1000_N10up9])
    print(np.min(distances_100_N10up9),np.min(distances_1000_N10up9),np.min(distances_10000_N10up9))
     #+end_src 

     #+RESULTS:
     : 1000 10000 100000
     : 0.0244577430454 0.0302832312759 0.0310828715776



     #+begin_src ipython :session master :results output :tangle no
    dpl10090 = np.concatenate(dpl_100_N10up9)
    dpl_100_N100up9   = np.split(dpl10090,100)
    dpl_1000_N100up9  = np.split(dpl100090,100)
    dpl_10000_N100up9 = np.split(dpl1000090,100)
    distances_100_N100up9   = np.array([sym_kl(points2pdf(dpls),-3/2,90,1) for dpls in dpl_100_N100up9])
    distances_1000_N100up9  = np.array([sym_kl(points2pdf(dpls),-3/2,900,1) for dpls in dpl_1000_N100up9])
    distances_10000_N100up9 = np.array([sym_kl(points2pdf(dpls),-3/2,9000,1) for dpls in dpl_1000_N100up9])
   
    distances_100_N100up9   = np.array([dist_KS(-3/2,1,90,dpls,null_ccdf=discrete_S_pl) for dpls in dpl_100_N100up9])
    dks,S,S_e,unique_s = dist_KS(3/2,5,10000,avs100[transiente:transiente + 100*100],null_ccdf=discrete_S_pl,return_ccdf=True)

     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master :file ./images/20170604_235118_1882Jr13423adf.png :tangle no :cache yes
    f,axs = plt.subplots(1,3,figsize=(16,6))
    for k,pdf,ax in zip([100,1000,10000],[pdf_avs100,pdf_avs1000,pdf_avs10000],np.reshape(axs,[-1])):
           dist = sym_kl(pdf,-3/2,int(0.9*k),1) 
           loglogplot(pdf,ax=ax,from_pdf=True)
           ax.set_title('sym kl distance 10^8 samples: '+str(dist))
     #+end_src 

     #+RESULTS[041e8bc16fd60ea63b295502c6665b6c64488208]:
     [[file:./images/20170604_235118_1882Jr13423adf.png]]


     #+begin_src ipython :session master :file ./images/20170604_235118_1882Jr1342olkjolkj3adf.png :tangle no :cache yes
    f,axs = plt.subplots(1,3,figsize=(16,6))
    for k,pdf,dists,dpls,ax in zip([100,1000,10000],
                               [pdf_100N_avs100,pdf_100N_avs1000,pdf_100N_avs10000],
                               [distances_100_N100up9,distances_1000_N100up9,distances_10000_N100up9],
                               [dpl_100_N100up9,dpl_1000_N100up9,dpl_10000_N100up9], 
                               np.reshape(axs,[-1])):
           dist = sym_kl(pdf,-3/2,int(0.9*k),1)    
           loglogplot(pdf,ax=ax,from_pdf=True)
           loglogplot(points2pdf(dpls[0]),ax=ax,from_pdf=True)
           p = p_value(dist,dists)
           ax.set_title('sym kl distance 10*N samples: '+str(dist)+"\n p-value: "+str(p)) 
     #+end_src 

     #+RESULTS[d74223517d4597a2577e7d296fe56d1cabfa7de1]:
     [[file:./images/20170604_235118_1882Jr1342olkjolkj3adf.png]]


     Now with discrete avalanches
     #+begin_src ipython :session master :file ./images/20171128_110050_29324U8U.png :tangle no
       unique,counts = np.unique(dpl_10000_N10up9[0],return_counts=True)
       norm_counts = counts/np.sum(counts)
       plt.loglog(unique,norm_counts)
       pdf_pl,unq_pl= discrete_power_law_dist(-alpha,l,9000)
       plt.loglog(unq_pl,pdf_pl)
     #+end_src 

     #+RESULTS:
     [[file:./images/20171128_110050_29324U8U.png]]

     :END:
 
     #+ATTR_LATEX: :width 400px 
     [[file:./images/20170604_235118_1882Jr1342olkjolkj3adf.png]]



     # There are severy methods available for statistically testing the
     # validity of the assumption with rejection tests (tests having
     # power law distribution as null-hypothesis). The most well known is
     # the method by cite:clauset2009power, which allows for a lower
     # cutoff. However, there were several critiques of the bootstrap
     # method advocated there and a more mathematical founded method was
     # published by cite:deluca2013fitting. This method can also deal
     # with system size cutoffs, and can be made to work for discrete
     # power laws (see cite:corral2012practical).

** Embedding critical subnetworks \label{sec:embedding}
   
   In this section we describe an approach to solve the task of
   embedding \(N_e \) subnetworks of \(N_s \) units each in the weight
   matrix \(W \) for a network of size \(N_u\), such that these
   subnetworks display critical dynamics while the activity of
   randomly chosen units stays subcritical.

   As shown in section [[ref:sec:pls]] the easiest way to embed a critical
   subnetwork of size \(N_s\) is to take the constant weight
   \alpha_{\text{crit}}(N_s) for all connections between elements of
   the subnetwork. However, when there are  multiple overlapping
   networks, this strategy alone leads to a supercritical regime with
   infinite avalanches, if the coupling matrix contains no inhibitory weights. 

*** Embedding critical subnetworks without inhibition \label{sec:embedding-excitation}

    To illustrate this point, we look at a particulary simple scenario
    of two overlapping networks (2-olv network): We embed two
    subnetworks (of size \(N_s=100\) with a given overlap \(N_o\),
    such that units \(1,\ldots,N_s \) belong to subnetwork 1 and units
    \(N_s - N_o + 1,\ldots,2N_s-N_o\) belong to subnetwork 2. Both
    subnetworks are recurrently coupled with weight
    \(\alpha_{\text{crit}}(N_s) \) and connections coupling units of
    different subnetworks are set to zero. See the left panel of figure
    [[ref:fig:excitatorx-wm-illus]] for an illustration.

    We give external input to all neurons setting \(M =\{1,\ldots,N\}\). However, the transition to supercriticality is
    not dependent on the support for the external stimulation in this
    model. Also note that the system size scales with the amount of
    overlap \(N = 2N_s-N_o\).


    #+CAPTION: Coupling matrieSimple 2-ovl network with and without inhibition \label{fig:excitatorx-wm-illus}.
    #+ATTR_LATEX: :width 300px
    [[/home/kima/Dropbox/uni/master/images/coral_draws/2ovl-network.pdf]]
    
    :implementation:
    #+begin_src ipython :session master
      def simple2ovl(N_s,N_o,beta=0):
          W = -beta * np.ones((2*N_s-N_o,2*N_s-N_o))
          inds_1 = range(N_s)
          inds_2 = range(N_s - N_o,2*N_s-N_o)
          W[np.ix_(inds_1,inds_1)] = ehe_critical_weights(N_s)
          W[np.ix_(inds_2,inds_2)] = ehe_critical_weights(N_s)
          return W
      N_s = 100
      N_o = 20
      W = simple2ovl(N_s,N_o) 
    #+end_src 

    #+RESULTS:

    #+RESULT:

    #+begin_src ipython :session master :file ./images/20171129_104905_29324f4O.png :tangle no
    plt.imshow(W)
    #+end_src 

    #+RESULTS:
    [[file:./images/20171129_104905_29324f4O.png]]

    #+begin_src ipython :session master :file ./images/20171129_110005_29324vFq.png :tangle no
    l,u = 10,26
    lens = [(ehe.simulate_model_mat(np.ones(2*100-N_o),1000,simple2ovl(100,N_o),deltaU),len(ehe.avs_sizes))[1] for N_o in range(l,u)]
    plt.plot(list(range(l,u)),lens,label='N_s = 100')
    lens_Ns200 = [(ehe.simulate_model_mat(np.ones(2*200-N_o),1000,simple2ovl(200,N_o),deltaU),len(ehe.avs_sizes))[1] for N_o in range(l,u)]
    plt.plot(list(range(l,u)),lens_Ns200,label='N_s = 200')
    lens_Ns300 = [(ehe.simulate_model_mat(np.ones(2*300-N_o),1000,simple2ovl(300,N_o),deltaU),len(ehe.avs_sizes))[1] for N_o in range(l,u)]
    plt.plot(list(range(l,u)),lens_Ns300,label='N_s = 300')
    plt.title('Number of finite avalanches 100 starting from initial condition u = 1' )
    plt.legend()
    #+end_src 

    #+RESULTS:
    [[file:./images/20171129_110005_29324vFq.png]]

    #+begin_src ipython :session master :file ./images/20171129_110005_29324vFadaalkfjq.png :tangle no
    l,u = 10,26
    lens = [(ehe.simulate_model_mat(np.random.random(2*100-N_o),1000,simple2ovl(100,N_o),deltaU),len(ehe.avs_sizes))[1] for N_o in range(l,u)]
    plt.plot(list(range(l,u)),lens,label='N_s = 100')
    lens_Ns200 = [(ehe.simulate_model_mat(np.random.random(2*200-N_o),1000,simple2ovl(200,N_o),deltaU),len(ehe.avs_sizes))[1] for N_o in range(l,u)]
    plt.plot(list(range(l,u)),lens_Ns200,label='N_s = 200')
    lens_Ns300 = [(ehe.simulate_model_mat(np.random.random(2*300-N_o),1000,simple2ovl(300,N_o),deltaU),len(ehe.avs_sizes))[1] for N_o in range(l,u)]
    plt.plot(list(range(l,u)),lens_Ns300,label='N_s = 300')
    plt.title('Number of finite avalanches from 100 simulated ones' )
    plt.legend()
    #+end_src 

    #+RESULTS:
    [[file:./images/20171129_110005_29324vFadaalkfjq.png]]

    #+begin_src ipython :session master :file ./images/20171129_124120_29324-Yy.png :tangle no
    lens_all = [(ehe.simulate_model_mat(np.ones(2*300-N_o),10,simple2ovl(300,N_o),deltaU),len(ehe.avs_sizes))[1] for N_o in range(1,301)]
    plt.plot(list(range(1,301)),lens_all,label='without inhibition')
    lens_all_i = [(ehe.simulate_model_mat(np.ones(2*300-N_o),10,simple2ovl(300,N_o,1),deltaU),len(ehe.avs_sizes))[1] for N_o in range(1,301)]
    plt.plot(list(range(1,301)),lens_all_i,label='with inhibition ')
    plt.xlabel('N_o')
    plt.ylabel('num. avs')
    plt.title('region of finite size avalanches for N_s=300')
    plt.legend()
    #+end_src 

    #+RESULTS:
    [[file:./images/20171129_124120_29324-Yy.png]]


    #+begin_src ipython :session master
      def ehe_one_step(u,W):
          act = (u >= 1).astype(int)
          u = u - act + W@act
          return u,act
    #+end_src 

    #+RESULTS:

    trace of activity
    #+begin_src ipython :session master :results output :tangle no
    N_s,N_o = 100,15
    start = np.ones(2*N_s-N_o) - 10**-8
    start[0] += deltaU
    W = simple2ovl(N_s,N_o)
    # start avalanche
    for i in range(1,16):
        print("-----------------------------------------------------")
        print("step "+str(i))
        start,act = ehe_one_step(start,W);print(act);print(start);
    #+end_src 

    #+RESULTS:
    #+begin_example
    -----------------------------------------------------
    step 1
    [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.03099999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999  0.99999999  0.99999999]
    -----------------------------------------------------
    step 2
    [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999  1.13499999
      1.13499999  1.13499999  1.13499999  1.13499999  1.13499999]
    -----------------------------------------------------
    step 3
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  1.66499999  1.66499999  1.66499999  1.66499999  1.66499999
      1.66499999  1.66499999  1.66499999  1.66499999  1.66499999  1.66499999
      1.66499999  1.66499999  1.66499999  1.66499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 4
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 1.05699999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999  1.03499999
      1.03499999  1.03499999  1.03499999  1.03499999  1.03499999]
    -----------------------------------------------------
    step 5
    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    [ 0.82199999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  2.32999999  2.32999999  2.32999999  2.32999999  2.32999999
      2.32999999  2.32999999  2.32999999  2.32999999  2.32999999  2.32999999
      2.32999999  2.32999999  2.32999999  2.32999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999  0.79999999
      0.79999999  0.79999999  0.79999999  0.79999999  0.79999999]
    -----------------------------------------------------
    step 6
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.95699999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  1.46499999  1.46499999  1.46499999  1.46499999  1.46499999
      1.46499999  1.46499999  1.46499999  1.46499999  1.46499999  1.46499999
      1.46499999  1.46499999  1.46499999  1.46499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999  0.93499999
      0.93499999  0.93499999  0.93499999  0.93499999  0.93499999]
    -----------------------------------------------------
    step 7
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 1.09199999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  0.59999999  0.59999999  0.59999999  0.59999999  0.59999999
      0.59999999  0.59999999  0.59999999  0.59999999  0.59999999  0.59999999
      0.59999999  0.59999999  0.59999999  0.59999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999  1.06999999
      1.06999999  1.06999999  1.06999999  1.06999999  1.06999999]
    -----------------------------------------------------
    step 8
    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    [ 0.85699999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  2.12999999  2.12999999  2.12999999  2.12999999  2.12999999
      2.12999999  2.12999999  2.12999999  2.12999999  2.12999999  2.12999999
      2.12999999  2.12999999  2.12999999  2.12999999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999  0.83499999
      0.83499999  0.83499999  0.83499999  0.83499999  0.83499999]
    -----------------------------------------------------
    step 9
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.99199999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  1.26499999  1.26499999  1.26499999  1.26499999  1.26499999
      1.26499999  1.26499999  1.26499999  1.26499999  1.26499999  1.26499999
      1.26499999  1.26499999  1.26499999  1.26499999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999  0.96999999
      0.96999999  0.96999999  0.96999999  0.96999999  0.96999999]
    -----------------------------------------------------
    step 10
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 1.12699999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  0.39999999  0.39999999  0.39999999  0.39999999  0.39999999
      0.39999999  0.39999999  0.39999999  0.39999999  0.39999999  0.39999999
      0.39999999  0.39999999  0.39999999  0.39999999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999  1.10499999
      1.10499999  1.10499999  1.10499999  1.10499999  1.10499999]
    -----------------------------------------------------
    step 11
    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    [ 0.89199999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  1.92999999  1.92999999  1.92999999  1.92999999  1.92999999
      1.92999999  1.92999999  1.92999999  1.92999999  1.92999999  1.92999999
      1.92999999  1.92999999  1.92999999  1.92999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999  0.86999999
      0.86999999  0.86999999  0.86999999  0.86999999  0.86999999]
    -----------------------------------------------------
    step 12
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 1.02699999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.06499999  1.06499999  1.06499999  1.06499999  1.06499999
      1.06499999  1.06499999  1.06499999  1.06499999  1.06499999  1.06499999
      1.06499999  1.06499999  1.06499999  1.06499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999  1.00499999
      1.00499999  1.00499999  1.00499999  1.00499999  1.00499999]
    -----------------------------------------------------
    step 13
    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    [ 0.92699999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  1.72999999  1.72999999  1.72999999  1.72999999  1.72999999
      1.72999999  1.72999999  1.72999999  1.72999999  1.72999999  1.72999999
      1.72999999  1.72999999  1.72999999  1.72999999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999  0.90499999
      0.90499999  0.90499999  0.90499999  0.90499999  0.90499999]
    -----------------------------------------------------
    step 14
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 1.06199999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  0.86499999  0.86499999  0.86499999  0.86499999  0.86499999
      0.86499999  0.86499999  0.86499999  0.86499999  0.86499999  0.86499999
      0.86499999  0.86499999  0.86499999  0.86499999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999  1.03999999
      1.03999999  1.03999999  1.03999999  1.03999999  1.03999999]
    -----------------------------------------------------
    step 15
    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    [ 0.82699999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  2.39499999  2.39499999  2.39499999  2.39499999  2.39499999
      2.39499999  2.39499999  2.39499999  2.39499999  2.39499999  2.39499999
      2.39499999  2.39499999  2.39499999  2.39499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999  0.80499999
      0.80499999  0.80499999  0.80499999  0.80499999  0.80499999]
#+end_example


(2 - a/b)*(1-1/sqrt(b)) >= 1,(1+a/b)*(1-1/sqrt(b)) >= 1, b > 0, a < b

    #+begin_src ipython :session master :results output :tangle no
    N_s,N_o = 100,95
    start = np.ones(2*N_s-N_o) - 10**-8
    start[0] += deltaU
    W = simple2ovl(N_s,N_o)
    # start avalanche
    for i in range(1,11):
        print("-----------------------------------------------------")
        print("step "+str(i))
        start,act = ehe_one_step(start,W);print(act);print(start);
    #+end_src 

    #+RESULTS:
    #+begin_example
    -----------------------------------------------------
    step 1
    [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.03099999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  1.00899999  1.00899999
      1.00899999  1.00899999  1.00899999  1.00899999  0.99999999  0.99999999
      0.99999999  0.99999999  0.99999999]
    -----------------------------------------------------
    step 2
    [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999  0.89999999  1.85499999  1.85499999
      1.85499999  1.85499999  1.85499999]
    -----------------------------------------------------
    step 3
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 4
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 5
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 6
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 7
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 8
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 9
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
    -----------------------------------------------------
    step 10
    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    [ 0.92199999  0.89999999  0.89999999  0.89999999  0.89999999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.94499999  0.94499999
      0.94499999  0.94499999  0.94499999  0.94499999  0.89999999  0.89999999
      0.89999999  0.89999999  0.89999999]
#+end_example


    #+begin_src ipython :session master
      def ineqN_o(N_s,N_o):
          return (2*N_s - N_o)*(1-N_o/N_s)*(1-1/np.sqrt(N_s)) - N_o

      def ineqN_oc(N_s,N_o):
          return N_o*2*(1-N_o/N_s)*(1-1/np.sqrt(N_s)) + 2*(N_s - N_o)*(1-N_o/N_s)*(1-1/np.sqrt(N_s)) - 2*(N_s - N_o)
    #+end_src 

    #+RESULTS:

    #+begin_src ipython :session master
      def phase_space_NsNo(N_s_range,N_o_range):
          ehe = load_module('ehe_detailed').EHE()
          psm = np.zeros((len(N_s_range),len(N_o_range)))
          for i,N_s in enumerate(N_s_range):
              for j,N_o in enumerate(N_o_range):
                  if N_o < N_s:
                      ehe.simulate_model_mat(np.ones(2*N_s-N_o),1,simple2ovl(N_s,N_o),deltaU)
                      psm[i,j] = len(ehe.avs_sizes)
                  else:
                      psm[i,j] = -1
          return psm


      def ana_phase_space_NsNo(N_s_range,N_o_range):
          psm = np.zeros((len(N_s_range),len(N_o_range)))
          for i,N_s in enumerate(N_s_range):
              for j,N_o in enumerate(N_o_range):
                  if N_o < N_s:
                      if (2-N_o/N_s)*(1-1/np.sqrt(N_s)) < 1:
                          psm[i,j] = 1
                      elif (1 + N_o/N_s)*(1-1/np.sqrt(N_s)) < 1:
                          psm[i,j] = 1
                      else:
                          psm[i,j] = 0
                  else:
                      psm[i,j] = -1
          return psm
    #+end_src 




    #+RESULTS:

    #+begin_src ipython :session master :tangle no 
    N_s_range = np.arange(100,901,40)
    N_o_range = np.arange(10,901,20)
    psm_big = pickle.load(open('/home/kima/Dropbox/uni/master/src/psm_big.pickle','rb'))#phase_space_NsNo(N_s_range,N_o_range)
    psm_ana = ana_phase_space_NsNo(N_s_range,N_o_range)
    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master :file ./images/20171129_224154_293248fC.png :tangle no
    f,axs = plt.subplots(1,2,figsize=(14,6))
    N_s_range = np.arange
    X,Y = np.meshgrid(N_s_range,N_o_range)
    axs[0].pcolor(Y,X,psm_ana.T)
    axs[1].pcolor(Y,X,psm_big.T)
    plt.suptitle('analytical and empirical phase space for the 2ovl network') 
    #+end_src 

    #+RESULTS:
    [[file:./images/20171129_224154_293248fC.png]]


    #+begin_src ipython :session master :tangle no
      plt.rc('text', usetex=True)
      plt.rc('font', family='serif')
      f,axs = plt.subplots(1,2,figsize=(9,3))
      X,Y = np.meshgrid(N_s_range,N_o_range)
      psm_ana = ana_phase_space_NsNo( np.arange(100,890),np.arange(10,890))
      s = axs[0].pcolor(Y,X,psm_big.T)
      f.colorbar(s, ax=axs[0],)
      #axs[0].colorbar()
      psm_inhibition = psm_big.copy()
      psm_inhibition[psm_inhibition != -1] = 0
      axs[1].pcolor(Y,X,psm_inhibition.T)
      f.colorbar(s, ax=axs[1],)

      axs[0].contour(np.arange(10,890),np.arange(100,890),psm_ana,[0])
      axs[0].set_title(r'2-ovl subcriticality witout inhibition') 
      axs[1].set_title(r'2-ovl subcriticality with inhibition') 
      axs[0].set_xlabel(r'$N_o$')
      axs[1].set_xlabel(r'$N_o$')
      axs[0].set_ylabel(r'$N_s$')
      axs[1].set_ylabel(r'$N_s$')
      f.savefig('2ovl-supercriticality.pdf',dpi=300)

    #+end_src 

    #+RESULTS:

    #+begin_src ipython :session master :file ./images/20171129_230427_29324xQW.png :tangle no
    X,Y = np.meshgrid(N_s_range,N_o_range)
    plt.pcolor(Y,X,psm_big.T)
    plt.colorbar()
    #+end_src 

    #+RESULTS:
    [[file:./images/20171129_230427_29324xQW.png]]


    :END:

    In this setting, there is a sharp transition to supercriticality
    at low values for \(N_o\). Only for very high values of \(N_o \),
    where the two subnetworks almost completely overlaps the system as
    a whole becomes subcritical again.

    For this simple model this transition can be explained using the
    framework derived in chapter [[ref:chap:gen-ehe]]. Here we will give
    sufficient conditions for the avalanches to remain finite
    regardless of the initial condition and the support of the
    external activation. \\

    #+BEGIN_prop 
    \label{prop:2ovl-trans}
    Avalanche sizes in the 2-ovl network stay finite if 
    \begin{align*}
    \left(2-\frac{N_o}{N_s}\right) \left(1-\frac{1}{\sqrt{N_s}} \right ) &< 1 \text{ , or} \\
    \left(1 + \frac{N_o}{N_s}\right )\left(1-\frac{1}{\sqrt{N_s}}\right ) &<1 \text{ .}
    \end{align*}
    #+END_prop

    #+BEGIN_proof 
    We will perform a constructive proof, illustrating how the
    avalanche grows in size during the first few steps while calculating sufficient
    conditions for the avalanche to stop at each step.

    First note that an infinite avalanche has to start at some point
    in \([0,1]^{2N_s-N_o}\). If \(u_1 \geq u_2\) componentwise for
    \(u_1,u_2 \in [0,1]^{2N_s-N_o}\), the size of the avalanche
    starting at \(u_1\) will not be smaller than the avalanche size
    resulting from the starting point \(u_2\) (this can be seen from
    lemma [[ref:lem:av_action]]).
    
    This enables us to consider without loss of generality the
    starting point \(u^{(0)} = (1-\epsilon)_{i=1,\ldots,2N_s-N_o} \)
    (with \(0 < \epsilon < \min(\Delta U,\alpha_{\text{crit}}(N))\)).

    We now look at the fast timescale to see how the avalanche proceeds.
    The first few steps of the avalanche are clear: One unit receives
    external input which makes it fire in the first step of the
    avalanche. If this unit belongs to the overlap units, then in the
    next step all other neurons will receive input of
    \(\frac{\alpha_c(N_s)}{N_s}\), causing them to fire at the second
    step. If a unit not belonging to the overlap units receives
    external input, it will take two additional steps until all units
    have fired. In the first step only the units belonging to its
    subnetwork receive external input and fire and in the second step
    the remaining units that received input from the overlap units
    firing in the second step. In both cases all units have fired
    exacly once and received input once from every connected unit thus
    providing a componentwise lower point the state vector
    
    \[u^{(1)}_i \geq \begin{cases}1 - \frac{1}{\sqrt N_s}  -\epsilon &\mbox{ if } i \in \operatorname{Ovl} \\
                                  (2 - \frac{N_o}{N_s})(1-\frac{1}{\sqrt N_s}) - \epsilon &\mbox{ otherwise ,}  \end{cases}\]
     where \(\operatorname{Ovl} \) denotes the set of units in the overlap region of the two subnetworks.
     
     A sufficient condition for the avalanche to end at this point is      \((2 - \frac{N_o}{N_s})(1-\frac{1}{\sqrt N_s}) < 1 \).
     
     If this condition is not fulfilled, then in the next step of the
     avalanche all overlap units will fire leading to the new point

    \[u^{(2)}_i \geq \begin{cases}(1 + \frac{N_o}{N_s})(1 - \frac{1}{\sqrt N_s})  -\epsilon &\mbox{ if } i \in \operatorname{Ovl} \\
                                  2(1 - \frac{N_o}{N_s})(1-\frac{1}{\sqrt N_s}) - \epsilon &\mbox{ otherwise ,}  \end{cases}\]
                                  
     In order for the avalanche to stop at this point, \((1 +
     \frac{N_o}{N_s})(1 - \frac{1}{\sqrt N_s})\) has to be smaller
     than 1, leading to the second condition and completing the proof.
    #+END_proof

    Figure [[ref:fig:2ovl-phase-space]] shows the phase space of this 2ovl network and
    the calculated sufficient conditions for the nonexistence of
    infinite avalanches.

    #+ATTR_LATEX: :width \textwidth
    #+CAPTION: Subcriticality index for the 2-ovl network with and without inhibition. The subcriticality index is simulated for \(N_s = 100,140,\ldots,900\) and \(N_o=10,30,\ldots,900)\). Left panel: Without inhibition, only a small fraction of the available phase space \(N_o \leq N_s\) is subcritical. The contour lines indicate the analytical transition boundary according to proposition~\ref{prop:2ovl-trans}. Right panel: With inhibitory connections of \(-\alpha_{\text{crit}}(N_s)\) between units not belonging to the overlap there is no supercritical region in the available phase space. \label{fig:2ovl-phase-space}
    [[/home/kima/Dropbox/uni/master/images/2ovl-supercriticality.pdf]]

    This leaves only a tiny portion of the phase space for a
    functional, not supercritical, network. The intuitive explanation
    for the transition to supercriticality in this model is the
    crosstalk excitation given to the subnetworks by the units in the
    overlap. These overlap units receive input from all neurons, thus
    they react like being in a subnetwork of size \(2*N_s-N_{o}\) but
    with coupling strength suitable for a network of size \(N_s\).
    
    If \(N_0\) is not very large, the coupling strength
    \(\alpha_{\text{crit}}(N_s)\) is far in the supercritical regime
    for a network of size \(2*N_s-N_o \), which allows the overlap
    units to fire multiple times. In order to cause the system as a
    whole to have infinite avalanches, the supercritical overlap units
    have to spread enough activation to the other units of the
    networks to make them fire multiple times. This happens only for
    sufficiently many units in the overlap.

    A possibility to enlarge the functional portion of the phase space
    one could choose a smaller coupling parameter than
    \(\alpha_{\text{crit}}\) for the subnetworks. However, this would
    lead to subcritical dynamics if only one network receives external
    input. Instead we consider the possibility of containing the
    excitatory crosstalk between networks by inhibitory weights
    between units belonging to no common subnetwork.       

    We now add inhibitory weights of \(- \alpha_{\text{crit}}\), or
    more generally with weight \(-\beta \alpha_{\text{crit}} \)
    between nodes not sharing a common subnetwork, as is illustrated
    in the right panel of figure [[ref:fig:excitatorx-wm-illus]]. The
    resulting inhibition manages to completly contain the excitatory
    crosstalk, even for \(\beta = 1 \), and the whole phase space
    stays functional (right panel of figure [[ref:fig:2ovl-phase-space]]).
    
*** Embedding critical subnetworks with inhibition    

   We now describe an algorithm to construct suitable weight matrices
   which produce critical avalanche dynamics when a subnetwork is
   activated but stay subcritical if a random subset of units is activated.
   
   
   # activated but stay subcritical otherwise. As seen in section
   # [[ref:sec:pls]], a critical network of size \(N_s \) can be
   # constructed by a constant connectivity matrix 
   # \(W_{sub} = \left(\alpha_{\text{crit}}(N_s) \right )_{i,j \in N_s} \).

   To embed \(N_e \) such subnetworks, one iteratively selects \(N_s
   \) distinct units from the set \(\{1,\ldots,N_u \}\) and setting
   the coupling strength for connections between the selected units to
   \(\alpha_{\text{crit}}(N_s)\).

   As seen in section [[ref:sec:embedding-excitation]], it is important to
   suppress the crosstalk excitation by sending inhibition between
   units having no common subnetwork. For simplicity we also choose a
   constant value for the inhibition given by \(-\beta
   \alpha_{\text{crit}}(N_s)\). 

   The algorithm consists of the following steps:
   - Initialize the weight matrix \(W\) of size \(N_u\times N_u\) with values \(-\beta \alpha_{\text{crit}}(N_s)\)
   - Repeat \(N_e\) times:
     - Uniformely choose \(N_s\) elements from the set \(\{1,\ldots,N_u \}\) without replacement and assign it to \(I\) 
     - Set the submatrix formed by the rows and columns in \(I\) to \(W_{I} = \alpha_{\text{crit}}(N_s)\).
     
   Weight matrices created by this algorithm are shown in figure
   [[ref:fig:weight-matrix-illustration]]. By construction every such
   weight matrix is symmetric, contains only the values
   \(\alpha_{\text{crit}}(N_s),-\beta\alpha_{\text{crit}}(N_s) \) and every
   unit is positively coupled to either at least \(N_s \) units
   (including itself) or to no unit at all if it does not belong to any subnetwork. 

   Up to reordering of the units and removing not selected units, the
   case of embedding two subnetworks reduce to the 2-ovl network analyzed in section [[ref:sec:embedding-excitation]].
   
   #+ATTR_LATEX: :width \textwidth
   #+CAPTION: Illustration of weight matrices from small network size to larger number of units and number of subnetworks. \label{fig:weight-matrix-illustration}. We will perform our simulate on networks with similar parameters as the large network displayed in the right panel.  
   [[/home/kima/Dropbox/uni/master/images/coral_draws/weight-matrices-illustration.pdf]]

*** Capacity of the embedding algorithm
   :PROPERTIES:
   :header-args: :tangle src/ehe_subnetworks.py
   :END:

   
   Similar to the purely excitatory 2-ovl network analyzed previously,
   we investigate the 'capacity' of this embedding scheme: How many
   subnetworks of a given size can be embedded before the system
   becomes supercritical?

    We introduce the subcriticality index \(\operatorname{I_{sub}} \)
   to be the probability that avalanche sizes stay finite (regardless
   of the external input support) for a realization of the weight
   matrix sampled with parameters \(N_u,N_{s},N_e,\beta\)\).

   Since in this case the system size always stays constant (\(N_u \))
   and the weight matrix contains more excitatory and less inhibitory
   connections when for fized \(N_s\) the number of subnetworks
   \(N_e\) is increased, we have for each \(N_s \) at most one
   transition point into the supercritical regime. See the first three subplots of figure
   [[ref:fig:phase-space]] for the subcritical index plotted over the
   phase space spanned by \(N_e,N_s\) for three values of
   \(\beta=1,2,3\) representing low, medium and high levels of inhibition.

   This shows that we have found a way to embed subnetworks into a
   larger inhibitory network that has a very high capacity to form a
   functional network with finite avalanche sizes. We use the numer of
   subnetworks one unit belongs to on average as a measure for the
   capacity. Figure [[ref:fig:capacity]] shows the maximal capacity (capacity
   evaluated at the median of the transition boundary) in dependence of 
   \(N_{s}\) for \(\beta=1,2,3 \).

   Numerically, the transition boundary on the phase space was
   calculated as follows: \(I_{\text{sub}} \) is approximated by the
   fraction of sampled weight matrices producing finite avalanches for
   20 realisations of weight matrices generated by the embedding
   algorithm. We use the following procedure to (approximately) check
   whether the network is able to generate infinite avalanches:

   Like in the purely excitatory case considered above, we set the
   initial values close to 1 for the simulations (\(u_i = 1-\epsilon\)
   for all \(i \leq N_u \)). We now give external input to one random
   unit and observe if the resulting avalanche stops after 100 fast
   timescale steps. The validity of the resulting transition boundary
   calculated with this simplifying assumptions was checked by
   comparing it to the transition boundary for the results of section
   [[ref:sec:coincidence-detector-performance]] (see figures
   [[ref:fig:heatplots]],[[ref:fig:2afc-avalanche-statistics]]). In these
   simulations not just one but 10000 avalanches were simulated using
   uniformely chosen random initial values. The criterion to check if
   an avalanche doesn't stop was to wait 10000 fast timescale steps
   and for each parameter set \(N_s,N_e\) simulations were performed
   with different realizations and supports for the external activation.

   In addition we used the property that there can be only one
   transition to supercriticality for a given \(N_s\) with increasing
   \(N_e\) to perform an efficient numerical search for the transition
   boundary in the parameter space: Given a pair of \(N_s,N_e\) first
   search in the direction of decreasing \(N_e\) until the
   subcriticality index is 1 and then search in the direction of
   increasing \(N_e \) until the subcriticality index is 0. Now
   proceed to neighboring \(N_s\) with the median point of the
   observed transition region as new starting value of \(N_e\).

   #+ATTR_LATEX: :width 400px
   #+CAPTION: Capacity (average number of subnetworks that a single unit belongs to) at the transition to supercriticality \(N_{e_{\text{crit}}}(N_s) \) in dependence of \(N_s\) for different levels of inhibition \(\beta=1,2,3\) \label{fig:capacity}.
   [[/home/kima/Dropbox/uni/master/images/max_capacity.pdf]]
   

   We now take a closer look at the transition boundary to the
   supercritical regime. Similar to the behavior of the purely
   excitatory network when started at in the state \(u= (1 -
   \epsilon)_{0 < i\leq N_u }\) the unit starting the avalanche pushes
   all the units that share a common subnetwork while inhibiting the
   rest. In the second fast timescale step of the avalanche all these
   connected units fire and give excitatory input to all units sharing
   a common subnetwork with them and sending inhibition to the rest.
   This /oscillatory/ spiking activity subsides after the first few
   fast timescale steps in the subcritical regime while it sustains
   itself in the supercritical regime. Since the number of units that
   share an excitatory connection with a given unit controls the
   amount of excitation given to the network the percentage of
   excitatory vs inhibitory connections of a units a prime candidate
   for a control parameter describing this phase boundary.

   We can analytically compute the probability that two units do not
   share at least one common subnetwork and therefore inhibit each
   other: Let \(u_i,u_j\) with \( i,j \leq N_u\) be two arbitrary
   units in a realization of the embedding scheme for parameters
   \(N_u,N_s,N_e\). We want to calculate the probability that \(u_1\)
   and \(u_2 \) are disconnected, i.e not belonging to a common
   subnetwork. If \(i=j\) this probability is trivially zero. If \(i
   \neq j\) we use the fact that all subnetworks are drawn
   independently from each other and fix one subnetwork \(s_1\)
   containing \(u_i\). The \(N_s-1\) other units belonging to this
   subnetwork are independently drawn without replacement
   (hypergeometric distribution) from the remainig \(N_u-1\) units.
   The probability that none of these units is \(u_i \) is given by
   \(\binom{N_u-2}{N_s-1}/\binom{N_u - 1}{N_s - 1} = 1 -
   \frac{N_s-1}{N_u-1}\). The expected number of subnetworks a unit is
   connected to is given by \(\frac{N_eN_s}{N_u}\). Thus the
   probability for two units to be not excitatorily connected is
   \begin{align}
   P[\text{two units are disconnected}]  = \frac{N_u-1}{N_u} \left (1-\frac{N_s-1}{N_u-1} \right )^{\frac{N_sN_e}{N_u}}\text{ .}
   \end{align}
   
   The bottom right panel of figure [[ref:fig:phase-space]] shows this
   probability in dependence on \(N_s,N_e\) including contour lines of
   levels \(0.4,0.5,0.68\) which most closely follow the fransition
   boundary to supercriticality for \(\beta = 3,2,1 \).
   
    #+ATTR_LATEX: :width 450px
    #+CAPTION: Phase space of the probabilistic embedding algorithm with inhibition in dependence on \(N_s,N_e\) for \(N_u=1000,\beta=1,2,3 \). The top row and the lower left panel show numerically evaluated subcriticality indices indicating the probability that avalanche sizes stay finite. This region becomes larger for increasing \(\beta\). The lower right panel shows a heat plot of the probabilty that two units don't share a common subnetwork. The contour lines (blue=0.68,red=0.5,green=0.4) indicate levels closely describing the phase transition boundary observed for \(\beta=3,2,1\). \label{fig:phase-space}
    [[/home/kima/Dropbox/uni/master/images/phase_spaces_big.png]]
    :implementation:
    #+begin_src ipython :session master 
      def test_supercriticality(N_u,N_s,N_e,beta=2,num_tests=20,max_duration_factor=0.1):
          ehe = load_module('ehe_detailed').EHE()
          ehe.max_duration_factor = max_duration_factor
          ret = 0;
          for i in range(num_tests):
              ensemble = embed_ensembles(N_u,N_s,N_e)
              W = simple_submatrix(N_u,ensemble,beta=beta)
              ext_support = np.zeros(N_u)
              ext_support[np.unique(np.concatenate(ensemble))] = 1
              ehe.simulate_model_mat(np.ones(N_u)-10**-6,1,W,deltaU,ext_support)
              ret += len(ehe.avs_sizes)
          del ehe
          return ret/num_tests
    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master :tangle no
    sup_idx_beta2 = [test_supercriticality(1000,100,N_e,2) for N_e in range(50,71)]
    sup_idx_beta1 = [test_supercriticality(1000,100,N_e,2) for N_e in range(50,71)]
    #+end_src 

    #+begin_src ipython :session master :file ./images/20171130_100105_29324sjk.png :tangle no
    plt.plot(list(range(50,71)),sup_idx_beta2)
    #+end_src 

    #+RESULTS:
    [[file:./images/20171130_100105_29324sjk.png]]


    #+begin_src ipython :session master :tangle no
    psm_beta1 = pickle.load(open("../avalanches/psm_beta1.pickle","rb"))
    psm_beta3 = pickle.load(open("../avalanches/psm_beta3.pickle","rb"))
    #+end_src 

    #+RESULTS:
    
    psm_beta1 did not work somehow above 80 but matches with expect. from test_supercriticality, as does beta3

    #+begin_src ipython :session master :file ./images/20171130_101230_29324fgS.png :tangle no
    N_es = list(range(40,73,2))
    N_ss = list(range(50,124,4))
    plt.pcolormesh(N_es,N_ss,psm_beta1.T)
    plt.colorbar()
    #+end_src 

    #+RESULTS:
    [[file:./images/20171130_101230_29324fgS.png]]

    #+begin_src ipython :session master :file ./images/20171130_102527_2932450e.png :tangle no
    N_es = list(range(40,73,2))
    N_ss = list(range(80,124,4))
    plt.pcolormesh(N_es,N_ss,psm_beta3.T)
    plt.colorbar()
    #+end_src 

    #+RESULTS:
    [[file:./images/20171130_102527_2932450e.png]]



    Now sample biig phase spaces cleverly 

    #+begin_src ipython :session master
      def sample_row(N_s,start_N_e,beta=2,N_u=1000,num_tests=20):
          res_start = test_supercriticality(N_u,N_s,start_N_e,beta=beta,num_tests=num_tests )
          lower_Ne,higher_Ne = start_N_e,start_N_e
          lower_res,higher_res = [],[]
          # move left until always subcritical
          while True:
              lower_Ne -= 1
              if lower_Ne < 1:
                  break;
              r = test_supercriticality(N_u,N_s,lower_Ne,beta=beta,num_tests=num_tests)
              if r == 1:
                  break
              lower_res.append(r)
          lower_res.reverse()
          # move right until always supercritical
          while True:
              higher_Ne += 1
              r = test_supercriticality(N_u,N_s,higher_Ne,beta=beta,num_tests=num_tests)
              if r == 0:
                  break
              higher_res.append(r)
          print((lower_res,[res_start],higher_res))    
          return (lower_Ne,higher_Ne),np.concatenate((lower_res,[res_start],higher_res))

      def sample_on_and_on(start_N_s,start_N_e,beta=2,N_u=1000,num_tests=20,rows_store=5,direction=1):
          N_s = start_N_s
          N_e = start_N_e
          results = []
          while 0 < N_s <= N_u:
              for i in range(rows_store):
                  print("explore row "+str(N_s)+' '+str(N_e))
                  (l,u),trans = sample_row(N_s,N_e,beta=beta,N_u=N_u,num_tests=num_tests)
                  results.append((N_s,(l,u),trans))
                  N_s += direction;
                  N_e = l; 
              pickle.dump(results,open('../avalanches/phase_spaces/psm_up_'+str(N_s)+'_beta='+str(beta),'wb'))


    #+end_src

    #+RESULTS:


     #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       parameteres = [(beta,dir) for beta in [1,2,3] for dir in [-1,1]]
       beta,dir = parameters[task_id]
       sample_on_and_on(100,50,beta=beta,N_u=1000,num_tests=20)
       """,name='phase_spaces',servername='server',task_ids='1-6',queue='maximus')
     #+end_src 


     #+begin_src ipython :session master 
       def construct_psm_from_lu_trans(lu_trans):
           min_N = np.min([row[0] for row in lu_trans])
           max_N = np.max([row[0] for row in lu_trans])
           min_N_e = np.min([row[1][0] for row in lu_trans])
           max_N_e = np.max([row[1][1] for row in lu_trans])
           N_s_range = list(range(min_N,max_N+1))
           N_e_range = list(range(min_N_e,max_N_e+1))
           psm = np.empty((max_N-min_N + 1,max_N_e-min_N_e + 1))
           for N,(l,u),trans in lu_trans:
               psm[N-min_N,:] = np.concatenate(([1.0]*(l - min_N_e + 1),trans,[0.0]*(max_N_e - u + 1)))
           return psm,N_s_range,N_e_range


     #+end_src 



     #+RESULTS:


     #+begin_src ipython :session master
     beta1down = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/phase_spaces/psm_-1_100_beta=1','rb'))
     beta2down = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/phase_spaces/psm_-1_100_beta=2','rb'))
     beta3down = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/phase_spaces/psm_-1_100_beta=3','rb'))
     beta1up = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/phase_spaces/psm_1_100_beta=1','rb'))
     beta2up = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/phase_spaces/psm_1_100_beta=2','rb'))
     beta3up = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/phase_spaces/psm_1_100_beta=3','rb'))
     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master :file ./images/20171130_231652_13566xFc.png :tangle no
     psm,N_es,N_ss = construct_psm_from_lu_trans(beta1down[:60]+beta1up[:200])
     plt.pcolormesh(N_ss,N_es,psm)
     plt.colorbar()
     #+end_src 

     #+RESULTS:
     [[file:./images/20171130_231652_13566xFc.png]]


     #+begin_src ipython :session master :file ./images/20171130_232135_13566-Pi.png :tangle no
       psm,N_es,N_ss = construct_psm_from_lu_trans(beta2down[:60]+beta2up[:300])
       plt.pcolormesh(N_ss,N_es,psm)
       plt.colorbar()
     #+end_src 

     #+RESULTS:
     [[file:./images/20171130_232135_13566-Pi.png]]

   #+begin_src ipython :session master :file ./images/20171130_232135_13566-Padfi.png :tangle no
       f = plt.figure(figsize=(8,10))
       psm,N_es,N_ss = construct_psm_from_lu_trans(beta3down[:55]+beta3up[:400])
       plt.pcolormesh(N_ss,N_es,psm)
       plt.colorbar()
     #+end_src 

     #+RESULTS:
     [[file:./images/20171130_232135_13566-Padfi.png]]


     #+begin_src ipython :session master :file ./images/20171130_232853_13566Lao.png :tangle no
       f,((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(10,8))
       psm1,N_ss1,N_es1 = construct_psm_from_lu_trans(beta1down+beta1up)
       psm2,N_ss2,N_es2 = construct_psm_from_lu_trans(beta2down+beta2up)
       psm3,N_ss3,N_es3 = construct_psm_from_lu_trans(beta3down+beta3up)
       subrange_N_s = np.arange(20,301)
       subrange_N_e = np.arange(1,1051)
       psm1sub = psm1[np.ix_(subrange_N_s - N_ss1[0],subrange_N_e - N_es1[0])]
       psm2sub = psm2[np.ix_(subrange_N_s - N_ss2[0],subrange_N_e - N_es2[0])]
       psm3sub = psm3[np.ix_(subrange_N_s - N_ss3[0],subrange_N_e - N_es3[0])]
       nes,nss = np.meshgrid(subrange_N_e,subrange_N_s)
       N_u = 1000
       prob_not_conn = ((N_u-1)/N_u) * (1-(nss-1)/(N_u-1))**(nss*nes/N_u)
       im1 = ax1.pcolormesh(subrange_N_e,subrange_N_s,psm1sub,cmap=plt.get_cmap('Greys'))
       ax1.contour(subrange_N_e,subrange_N_s,prob_not_conn,[0.68],colors=['blue'],linewidth=5)
       im2 = ax2.pcolormesh(subrange_N_e,subrange_N_s,psm2sub,cmap=plt.get_cmap('Greys'))
       ax2.contour(subrange_N_e,subrange_N_s,prob_not_conn,[0.5],colors=['red'],linewidth=5)
       im3 = ax3.pcolormesh(subrange_N_e,subrange_N_s,psm3sub,cmap=plt.get_cmap('Greys'))
       ax3.contour(subrange_N_e,subrange_N_s,prob_not_conn,[0.4],colors=['green'],linewidth=5)
       im4 = ax4.pcolormesh(subrange_N_e,subrange_N_s,prob_not_conn,cmap=plt.get_cmap('Greys'))
       ax4.contour(subrange_N_e,subrange_N_s,prob_not_conn,[0.4,0.5,0.68],colors=['green','red','blue'],linewidth=5)
       ax1.set_xlabel(r'$N_e$')
       ax1.set_ylabel(r'$N_s$')
       ax1.set_title(r'supercriticality index ($\beta=1$)')
       ax2.set_xlabel(r'$N_e$')
       ax2.set_ylabel(r'$N_s$')
       ax2.set_title(r'supercriticality index ($\beta=2$)')
       ax3.set_xlabel(r'$N_e$')
       ax3.set_ylabel(r'$N_s$')
       ax3.set_title(r'supercriticality index ($\beta=3$)')
       ax4.set_xlabel(r'$N_e$')
       ax4.set_ylabel(r'$N_s$')
       ax4.set_title(r'Probability that two units are not connected')
       f.colorbar(im4,ax=ax4)
       f.savefig('phase_spaces_big.png',dpi=300)
       # f.colorbar(im1,ax=ax1)
       # f.colorbar(im2,ax=ax2)
       # f.colorbar(im3,ax=ax3)

     #+end_src 

     #+RESULTS:
     [[file:./images/20171130_232853_13566Lao.png]]


     #+end_src 


     Now boldy explore the phase transition boundary!

     #+begin_src ipython :session master
       def test_stat(stat,N_u,N_s,N_e,beta=2,num_tests=20):
           ret = 0;
           for i in range(num_tests):
               ensemble = embed_ensembles(N_u,N_s,N_e)
               W = simple_submatrix(N_u,ensemble,beta=beta)
               ret += stat(W)
           return ret/num_tests

     def num_connected_stat(W):
         return np.mean(np.sum(W >= 0,axis=1))


     def balance_stat(W):
         return np.mean(np.sum(W,axis=1))

     def     

     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master
       def evaluate_on_trans_region(stat,lu_trans,num_repetitions=10,num_tests=3,beta=2,outpath='../avalanches/phase_spaces',outname="eotr",N_u=1000):
           lutidx = 0;
           rows = []
           while lutidx < len(lu_trans):
               for i in range(num_repetitions):
                   N,(l,u),trans = lu_trans[lutidx]
                   res = []
                   for N_e in range(l,u+1):
                       res.append(test_stat(stat,N_u,N,N_e,beta=beta,num_tests=num_tests))
                   rows.append((N,(l,u),res))
                   lutidx+=1
                   if lutidx == len(lu_trans):
                       break
               pickle.dump(rows,open(outpath+'/'+outname+'.pickle','wb'))
           return rows


       def construct_statpsm_from_lu_trans(lu_trans):
           min_N = np.min([row[0] for row in lu_trans])
           max_N = np.max([row[0] for row in lu_trans])
           min_N_e = np.min([row[1][0] for row in lu_trans])
           max_N_e = np.max([row[1][1] for row in lu_trans])
           N_s_range = list(range(min_N,max_N+1))
           N_e_range = list(range(min_N_e,max_N_e+1))
           psm = np.empty((max_N-min_N + 1,max_N_e-min_N_e + 1))
           for N,(l,u),trans in lu_trans:
               psm[N-min_N,:] = np.concatenate(([np.median(trans)]*(l - min_N_e),trans,[np.median(trans)]*(max_N_e - u)))
           return psm,N_s_range,N_e_range
     #+end_src 

     #+RESULTS:



     #+begin_src ipython :session master :file ./images/20171201_112554_13566hQy.png :tangle no
     psm1,N_ss1,N_es1 = construct_psm_from_lu_trans(beta1down+beta1up)
     psm2,N_ss2,N_es2 = construct_psm_from_lu_trans(beta2down+beta2up)
     psm3,N_ss3,N_es3 = construct_psm_from_lu_trans(beta3down+beta3up)
     subrange_N_s = np.arange(20,301)
     subrange_N_e = np.arange(1,1051)
     psm1sub = psm1[np.ix_(subrange_N_s - N_ss1[0],subrange_N_e - N_es1[0])]
     psm2sub = psm2[np.ix_(subrange_N_s - N_ss2[0],subrange_N_e - N_es2[0])]
     psm3sub = psm3[np.ix_(subrange_N_s - N_ss3[0],subrange_N_e - N_es3[0])]   
     N_u = 1000
     max_cap_1 = [N_s*(np.median(np.where((psm1sub[N_s-20,:] < 1) & (psm1sub[N_s-20,:] > 0))[0])+1)/N_u for N_s in subrange_N_s] 
     max_cap_2 = [N_s*(np.median(np.where((psm2sub[N_s-20,:] < 1) & (psm2sub[N_s-20,:] > 0))[0])+1)/N_u for N_s in subrange_N_s] 
     max_cap_3 = [N_s*(np.median(np.where((psm3sub[N_s-20,:] < 1) & (psm3sub[N_s-20,:] > 0))[0])+1)/N_u for N_s in subrange_N_s] 
     f = plt.figure(figsize=(6,4))
     plt.plot(subrange_N_s,max_cap_1,label=r'$\beta = 1$')
     plt.plot(subrange_N_s,max_cap_2,label=r'$\beta = 2$')
     plt.plot(subrange_N_s,max_cap_3,label=r'$\beta = 3$')
     plt.xlabel(r'N_s')
     plt.ylabel(r'$\frac{N_sN_{e_{\text{crit}}}}{N_u}$')
     plt.legend()
     plt.title(r'Capacity at the transition to supercriticality')
     f.savefig('max_capacity.pdf',dpi=300)
     #+end_src 

     #+RESULTS:
     [[file:./images/20171201_112554_13566hQy.png]]

    :END:

    In this section we have shown that it is possible to embed a high
    number of subnetworks in a bigger network while the system as a
    whole remains in a functional state producing finite avalanches
    without runaway activity. We made no special assumptions about the
    structure of subnetworks and testet the embedding algorithm with
    independently drawn subnetworks that were chosen by uniformly
    selecting distinct units.

    The transition region to the supercritical region was numerically
    calculated for large parts of the phase space and shown to be
    closely connected to the ratio of excitatory and inhibitory
    connections of a unit on average.

    Up to now functionality was equated with producing finite
    avalanche sizes, but in order to use these networks for feature
    integration we have to show that the dynamics in activated
    feature-subnetworks is different than the dynamics in activated
    subnetworks chosen randomly.

     
** Separability of network dynamics for figure and background activation \label{sec:separability}
   
   After we have succesfully developed an algorithm which can embed a
   high number of subnetworks we are now going to test whether these
   embedded figure subnetwork show clearly separate dynamics from
   randomly chosen activated networks. 

   We test this by simulating a 2-afternate forced choice thask, which
   is a standard paradigm in behavioral contour integration
   experiments. Contour integration is a special case of perceptual
   grouping in which oriented line elements are perceived as a contour
   when they are aligned according to Gestalt laws like the law of
   good continuation. In particular this process is thought to occur
   in the early stages of visual perception in the brain in the
   striate cortex, and there is evidence for an association field
   citep:vanrullen2001feed,yen1998extraction,li2002global,FIELD1993173.
   Similar the coupling scheme considered here, the association field
   puts excitatory connections between stimuli (or neurons receptive
   to stimuli) that are well aligned while proposing inhibitory
   connections between misaligned stimuli. In this respect it shares
   some similarities with the embedding matrices considered in this
   thesis and can serve as an illustration for the general principle
   of feature integration. See figure [[ref:fig:illus-ci]] for an
   illustration of our proposed avalanche dynamics for the example of contour integration.  

  #+ATTR_LATEX: :width \linewidth
  #+CAPTION: Proposal for critical avalanche statistics in subnetworks for contour integration. The left panel shows a stimulus used in psychophysical contour integration experiments. The task of the proband is to identify whether there is a countor shown at some position of the stimulus or whether it only consists of randomly aligned bar elements. The blue region marks a region of only randomly oriented background elements while there is a contour of coaligned orientation elements in the red region. Sampling avlanches from the blue region will result in a subcritical avalanche size distribution while the activity of the elements belonging to the aligned countor becomes critical (right panel) \label{fig:illus-ci}.       
  [[/home/kima/Dropbox/uni/master/images/coral_draws/2afc-stimulus.pdf]]
  

  In this section, we will introduce the 2-AFC task and evaluate two
  key properties our networks should have:
  - Well separated avalanche statistics, with activated figure
    networks displaying critical behavior
  - Well separated temporal dynamics which can be used to
    differentiate in short time between the dynamics of an activated
    figure network and a randomly chosen one.

  
*** 2-AFC task description \label{sec:2afc-description}

    To evaluate the computational capabilities of the proposed
    approach we model a 2-alternative forced choice (2-AFC) task which
    is a standard paradigm in behavioral experiments.

    In a 2-AFC experiment, the proband is presented two stimuli, a
    /target/ stimulus containing a figure and a /distractor/ stimulus
    containing only randomly oriented background elements. The task of
    the proband is to detect the target stimulus. The contour
    integration 2-AFC task is made harder for the participant if the
    contrasts of the stimuli are varied. Human participants
    nevertheless can easily distinguish the figure in the target
    stimulus with less background contrast from the random elements
    presented in the distractor stimulus.

    We simulate the 2-AFC task in the following way: For given
    parameter values \(N_s,N_e,K,\beta\) we construct a weight matrix
    with \(N_e \) subnetworks of size \(N_s\) using the embedding
    algorithm with inhibition level \(\beta \). We distinguish between
    the target and distractor case by activating (giving external
    input to) different sets of units. For the target stimulus, we
    activate one whole feature subnetwork and \(K \) additional
    randomly selected units. The same number \(N_s + K\) units are
    given external input in the distractor stimulus, but here these
    units are randomly chosen. The target and distractor stimuli
    consists of the spiking activity recorded from the weight matrix
    with this different activation schemes. For each parameter set
    \(N_s,N_e,K,\beta\) 10000 avalanches were recorded for 10 different
    realization of the embedding matrix and 10 different choices of
    activated subnetwork and random background elements. 

    The correspondence between the contour integration task and the
    performed simulations are illustrated in figure [[ref:fig:illus-2afc-stimuli]].
    #+ATTR_LATEX: :option :width \textwidth
    #+CAPTION: Illustration of stimuli used for the 2-AFC-Task \label{fig:illus-2afc-stimuli}. Left: Examples of visual stimuli used in contour integration 2-AFC tasks. Middle: Support for the external activation given to the EHE model is visualized by red bars on top of the elements receiving external input. Right: Spike trains arising from this model constitute the Target and Distractor stimuli. The visual stimulus that the proband gets presented are represented by spike trains arising from an EHE model with \(N_e\) embedded subnetworks of size \(N_s\). When this visual stimulus contains a contour, i.e. multiple patches oriented similarly and located colinearly/cocircularly, this is modeled by giving activation to a whole embedded subnetwork whereas the units receiving external input are chosen randomly for the distractor stimulus. In this example, the visual Target stimulus has a lower background contrast than the Distractor stimulus. This is incorporated into the model by scaling overall firing rates. \label{fig:illus-2afc-stimuli}
    [[/home/kima/Dropbox/uni/master/images/coral_draws/2AFC-illustration.pdf]]


    In order to simulate changes in the background contrast we scale
    the global firing rates. We show that the coincidence detector
    described in section [[ref:sec:coincidence-detector-performance]] can
    distinguish between target and distractor even for the case of low
    background contrast in the target stimulus, where the global
    firing rates are equal in both stimuli. 

    Adjusting the global firing rates could be done in two different
    ways in the simulation. The first way is to adjust the strength of
    the external input \(\Delta U\). Since the dynamical system just
    performs a random walk along the positive directions of the axes,
    the time between two spiking events is inversely related to
    \(\Delta U\). However, this approach bears some difficulties.
    First,the qualitative behavior of the dynamical system may change
    for too large values of \(\Delta U\), for the generalized
    EHE model without inhibition the condition \(\sum_{j=1}^Nw_{ij} +
    \Delta U < 1\) is a fundamental assumption for all results stated
    in chapter [[ref:chap:gen-ehe]]. Changing the values of \(\Delta U \)
    is also not physiologically plausible since \(\Delta U\)
    represents input from other (sensory) neurons.

    Instead we use a simpler procedure that allows
    to change the background contrast after the simulation, but
    discards some samples, by scaling not \(\Delta U \) but the time
    \(T\). For the target stimulus time is equal to the number of
    external stimulation steps, while for the background contrast
    adjusted distractor stimulus it is given by the number of external
    stimulation steps divided by the background contrast.
   
*** Separability of avalanche size distributions \label{sec:2afc-avalanches}

    In this section we evaluate the separability of the avalanche size
    statistics arising in the target vs the distractor stimuli. We find
    that over a large part of parameter space avalanche statistics are
    well separated between Target and Distractor stimuli, with the
    avalanche statistics in the Target being close to critical vs the
    subcritical avalanche distribution arising in the Target stimulus.

    For the parameters \(\beta=2,K=100\) and \(40 \leq N_s \leq 70, 80
    \leq N_e \leq 116\) Figure [[ref:fig:2afc-avalanche-statistics]] shows
    the discriminability of Target and Distractor avalanche dynamics
    over the phase space. The figure in the top row depicts the
    simulated phase space and shows the transition to the
    supercritical phase, which coincides with the transition boundary
    shown in figure [[ref:fig:phase-space]]. Before the transition region,
    all 100 simulations produced 10000 avalanches each for both the
    target and distractor stimuli, leading to \(2*10^6\) avalanches.
    On the transition boundary some of the 100 simulations produced
    infinite avalanches while inside the supercritical region the
    networks produced no avalanche of finite size. Numerically the
    simulation was stopped if an avalanche continued after \(10000\)
    steps on the fast time scale.

    For points being far away from the transition region, avalanche
    size distributions are very well separated, see bottom left panel
    of figure [[ref:fig:2afc-avalanche-statistics]]. The avalanche size
    distribution in the Target for this realizaton is close to a
    critical distribution, as this distribution closely coincides with
    a reference avalanche size distribution which was sampled from a
    homogeneous EHE model of size \(N_s\) using the same number of
    avalanches as produced by the 2-AFC Target stimuli. When
    approaching the transition boundary, the subcritical avalanche
    size distribution in the Target stimuli becomes closer to the
    still critical Target avalanche size distribution, see the bottom
    middle panel of figure [[ref:fig:2afc-avalanche-statistics]]. The
    bottom right panel shows the avalanche size distributions for a
    point on the transition boundary. Here, target and distractor
    distribution can become very similar, before - when further
    increasing \(N_e\) - they become supercritical and infinite.
   

    \begin{landscape}
    \centering
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{/home/kima/Dropbox/uni/master//images/20171202_093414_2175Y3a.png}
    \caption{Avalanche statistics in the 2-AFC task \label{fig:2afc-avalanche-statistics}. The top panel shows the phase space of the 2-AFC simulations. The yellow area marks the subcritical region
    where all realisations produced finite avalanches. The transition region to superciticality coincides with the one observed in figure~ref{fig:phase-space} (b). The bottom row shows the observed avalanche statistics for different points in phase space indicated by the red arrows. The Target avalanche size distribution stays similar to a reference critical distribution while the Distractor distribution is subcritical, but becomes more and more critical with decreasing distance to the supercritical region.}
    \end{figure}
    \end{landscape}
   
     :implementation:
    #+begin_src ipython :session master
      def KL_target_distractor(sim_res):
          e = load_module('ehe_detailed').EHE()
          all_avs_sizes_a = []
          all_avs_sizes_b = []
          for ens in range(10):
              for act in range(10):
                  avs_a,avs_inds_a,act_inds_a = sim_res['sim_res'][ens]['sim_res_a'][act]
                  avs_sizes_a,avs_durations_a = e.get_avs_size_and_duration(avs_a,avs_inds_a)
                  avs_b,avs_inds_b,act_inds_b = sim_res['sim_res'][ens]['sim_res_b'][act]
                  avs_sizes_b,avs_durations_b = e.get_avs_size_and_duration(avs_b,avs_inds_b)
                  all_avs_sizes_a.extend(avs_sizes_a)
                  all_avs_sizes_b.extend(avs_sizes_b)
          dist = sym_kl2(points2pdf(all_avs_sizes_a),points2pdf(all_avs_sizes_b))
          return dist


      def spike_rasters(spiking_patterns,N_u):
          raster = np.zeros((N_u,len(spiking_patterns)))
          for i,pattern in enumerate(spiking_patterns):
              raster[np.concatenate(pattern),i] = 1
          return raster


      def plot_spike_raster(raster,act_inds,ax):
          for i,col in enumerate(raster):
              ax.plot(act_inds,col+i)

      def raster_figure_noise(raster,fig_u,noise_u,num_other):
          fig_cols = raster[fig_u,:]
          noise_cols = raster[noise_u,:]
          other_units = np.random.choice(list(set(np.arange(raster.shape[0])) - set(fig_u) - set(noise_u)),
                                        size=num_other,replace=False)
          other_cols = raster[other_units,:]
          return np.vstack((fig_cols,noise_cols,other_cols))





    #+end_src

    #+RESULTS:


    #+begin_src ipython :session master :file ./images/20171204_125953_8888euH.png :tangle no
    f,(ax1,ax2) = plt.subplots(1,2,figsize=(14,8))
    sim_res = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/sim_res100,50,100.pickle','rb'))
    (avs,avs_inds,act_inds) = sim_res['sim_res'][0]['sim_res_a'][1]
    ehe = load_module('ehe_detailed').EHE()
    patterns = ehe.get_spiking_patterns(avs,avs_inds)
    raster = spike_rasters(patterns,N_u=1000)
    fig_u = sim_res['sim_res'][0]['figure_units'][1]
    noise_u =sim_res['sim_res'][0]['noise_units'][1]
    num_other = 10
    raster_fn = raster_figure_noise(raster,fig_u,noise_u,num_other=0)
    most_active_noise = np.argsort(np.sum(raster_fn[len(fig_u):len(fig_u)+len(noise_u),:],axis=1))[-100:] + 100
    f = plt.figure(figsize=(12,12))
    plot_spike_raster(raster_fn[np.concatenate((most_active_noise,np.arange(100))),:100],act_inds[1:101],ax=ax1)
    ens,act = (0,0)
    (avs,avs_inds,act_inds) = sim_res['sim_res'][ens]['sim_res_b'][act]
    ehe = load_module('ehe_detailed').EHE()
    patterns = ehe.get_spiking_patterns(avs,avs_inds)
    raster = spike_rasters(patterns,N_u=1000)
    raster_fn = raster
    most_spiking = np.argsort(np.sum(raster_fn,axis=1))[800:1000]
    f = plt.figure(figsize=(12,12))
    plot_spike_raster(raster_fn[most_spiking,:500],act_inds[1:501],ax=ax2)
    ax1.set_xlabel('T')
    ax2.set_xlabel('T')
    ax1.set_ylabel('neuron id shuffled')
    ax2.set_ylabel('neuron id shuffled')
    ax1.set_title('spike raster plot Target')
    ax2.set_title('spike raster plot Background')
    #+end_src 

    #+RESULTS:
    [[file:./images/20171204_125953_8888euH.png]]

      #+begin_src ipython :session master :file ./images/2jkj0171204_125953_8888euH.png :tangle no
     
        sim_res = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/sim_res100,50,100.pickle','rb'))
        ens,act = (0,0)
        (avs,avs_inds,act_inds) = sim_res['sim_res'][ens]['sim_res_b'][act]
        ehe = load_module('ehe_detailed').EHE()
        patterns = ehe.get_spiking_patterns(avs,avs_inds)
        raster = spike_rasters(patterns,N_u=1000)
        raster_fn = raster
        most_spiking = np.argsort(np.sum(raster_fn,axis=1))[700:900]
        f = plt.figure(figsize=(12,12))
        plot_spike_raster(raster_fn[most_spiking,:],act_inds[1:-1],ax=plt)
       
    #+end_src 

    #+RESULTS:
    [[file:./images/2jkj0171204_125953_8888euH.png]]

     :END:
 

     The separability of the avalanche size distribution can be
     understood by inspecting spike raster plots resulting from the
     2-AFC simulation, which are shown in figure [[ref:fig:spike-raster]].
     After the first avalanches, which have a chance to occur both in
     the figure and activated background units depending on the initial
     conditions and sequence of external inputs, the figure units
     quickly win the competition and due to the all-to-all inhibition
     all nonfigure units are silenced during the rest of the
     simulation, and the model behaves just as the critical ehe-model
     in the subnetwork. The activation pattern in the Distractor
     stimulus is unlikely to favor a particular embeded subnetwork
     significantly more than others and activation stays spread over
     more than one subnetwork. 
    
     #+ATTR_LATEX: :width 400px
     #+CAPTION: spike raster plot for 2-AFC simulation with parameters \(N_s = 100,N_e=500,K=100,\beta=2 \) for Target (left) and Distractor (right) stimuli. (a) neurons 100-200 are figure units and 0-100 background units receiving external input ordered by their total spike count. (b) 200 most active units, sorted by their total spike count\label{fig:spike-raster}
     [[file:./images/20171204_125953_8888euH.png]]
    

     To check whether this separability is robust with respect to the
     other parameters, simulations were done with higher number of
     random background units receiving external input \(K=200\), and
     also with the embedding parameter \(\beta \) set to one in which
     case the inhibitory connections are only halve as strong.

     The top row of figure [[ref:fig:avalanches-K200]] shows that this qualitative
     behavior stays the same even for \(K=200\) while the bottom row shows the same for \(\beta=1 \)    

     \begin{landscape}
     \centering
     \begin{figure}[H]
    \begin{tabular}{ccc}
    \includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master/images/all_avs100,44,200,2.pdf} & \includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master/images/all_avs100,50,200,2.pdf}  & 
\includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master/images/all_avs100,54,200,2.pdf} \\ 
 (a)  & (b) & (c)\\
\includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master/images/all_avs80,40,100,1.pdf} & \includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master/images/all_avs80,44,100,1.pdf}  & 
\includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master/images/all_avs80,46,100,1.pdf} \\  
 (d)  & (e) & (f)\\
    \end{tabular}
    \caption{\label{fig:avalanches-K200} Avalanche plots for \(\beta=2,K=200\) (top row) and \(\beta=1,K=100\) (bottom row). TODO thicker lines etc. etc. maybe draw one smooth reference distribution instead of reference distributions sampled with the same number of avalanches }
    \end{figure}
    \end{landscape}

*** Performance of coincidence detector \label{sec:coincidence-detector-performance}
   
    In order to make practical use of criticality in subnetworks for
    feature interation, the brain has to distinguish between feature
    and random subnetworks in a short amount of time. In particular,
    it cannot wait a long time to sample sufficient avalanches until
    it is clear that one avalanche size distribution resembles a power law.

    Here we investigate the performance of a simple coincidence
    detector, which can easily be realized by neural mechanisms
    (discussed below), to show that the different dynamics between
    target and distractor can be differentiated in a short time. Here
    we measure time \(T\) on the slow timescale of the external input.

    The coincidence detector records large (synchronous) events and uses
    the number of observed synchronous events to distinguish between
    target and distractor. A possible neural realization would be a
    single readout neuron, for example a linear intergrate and fire
    neuron, receiving input from afferent neurons with
    firing threshold and synaptic gain tuned to a level such that only
    multiple synchronous afferent spikes can elicit a postsynaptic
    spike.

    The coincidence detector is parameterized by the avalanche size
    threshold for synchronous events \(s_0\) and is given an allowed
    observation time \(T\). It receives as input the spike trains from
    the target \(\operatorname{avs}_1 \) and distractor
    \(\operatorname{avs}_2 \) stimuli sampled during this time horizon \(T \).
    
    It outputs 1 to indicate \(\operatorname{avs}_1\) comes from the
    target and 2 to for \(\operatorname{avs}_2 \). The coincidence
    detector bases its decision on the sizes of avalanches occuring at
    each time step. It records whether the size crosses the threshold
    \(s_0\). The number of these synchronous events for the input
    avalanches is compared and the time series with the higher number
    is given the target label. In case where in both time series the
    same number of synchronous events is recorded, it picks randomly.
   
    This is formalized by the following classifier:
   
    \begin{align*}
    \operatorname{coince}_{s_{0}}(\operatorname{avs}_1^T,\operatorname{avs}_2^T) = \begin{cases}
    1 &\mbox{ if } \sum_{\operatorname{av_1} \in \operatorname{avs}_1^T} \delta[\operatorname{size}(\operatorname{av}_1)] > \sum_{\operatorname{av_2} \in \operatorname{avs}_2^T} \delta[\operatorname{size}(\operatorname{av}_2)] \\
    2 &\mbox{ if } \sum_{\operatorname{av_1} \in \operatorname{avs}_1^T} \delta[\operatorname{size}(\operatorname{av}_1)] < \sum_{\operatorname{av_2} \in \operatorname{avs}_2^T} \delta[\operatorname{size}(\operatorname{av}_2)] \\
    1 + U   &\mbox{ otherwise} \text{, } U \sim \operatorname{Ber}(0.5) \text{ .} \end{cases} 
    \end{align*}

    Denote by \(\operatorname{syn}_1^T, (\operatorname{syn}_2^T) \) the
    random variables representing the number of synchronous events
    during \(T \) external stimulation timesteps in the target
    (distractor) stimulus with support \(0,\ldots,n_1^T (0,\ldots,n_2^T)\). The accuracy of \(\operatorname{coince}_{s_0}\) with time
    horizon \(T \) is then calculated by 

    \begin{align*}
    P[\operatorname{coince}_{s_{0}}(\operatorname{avs}_1^T,\operatorname{avs}_2^T) = 1] &= P[\operatorname{syn}_1^T > \operatorname{syn}_2^T] + \frac{1}{2}P[\operatorname{syn}_1^T = \operatorname{syn}_2^T] \\
    &= \sum_{i=1}^{n_1^T}P[\operatorname{syn}_1^T = i]P[\operatorname{syn}_2^T \leq i-1] + \frac{1}{2}P[\operatorname{syn}_1^T = \operatorname{syn}_2^T] \\ 
    &=  \sum_{i=1}^{n_1^T}P[\operatorname{syn}_1^T = i]\sum_{j=0}^{i-1}P[\operatorname{syn}_2^T = i] + \frac{1}{2}P[\operatorname{syn}_1^T = \operatorname{syn}_2^T] \\
    &=  \sum_{i=1}^{n_1^T}P[\operatorname{syn}_1^T = i]\sum_{j=0}^{i}\frac{1}{1+\delta_j^i}P[\operatorname{syn}_2^T = i]
    \end{align*}

    and thus directly available given empirical histograms of
    \(\operatorname{syn}_1^T,\operatorname{syn}_2^T\). When performing
    an Receiver-Operator Characteristic with this two distributions
    \(\operatorname{syn}_1^T,\operatorname{syn}_2^T\), the accuracy of
    the coincidence detector is given by the area under the curve.
    

    The coincidence detector is evaluated for each 2-AFC simulation
    with parameters \(s_0,T\) logarithmically scaled between 20 and
    1000, and linearly scaled between \((0,20)\). 

    We are now going to present the performance results of the
    coincidence detector over the 2-AFC phase space starting from a
    close look at a specific realization
    \(N_s=100,N_e=50,K=100,T=1000,s_0=30 \) and we describe how the
    coincidence distributions
    \(\operatorname{syn}_1^T,\operatorname{syn}_2^T\) arise from a
    population plot for this parameters.

    We the study the dependence of the 2-AFC accuracy on the
    coincidence detector parameters \(s_0,T\) for fixed 2-AFC
    parameters \(N_s,N_e,K,\beta\) and show that for fixed \(T\) there
    exists an optimal \(s_0\) maximising the accuracy. Finally we look
    at the maximal performance and optimal threshold over the 2-AFC
    phase space \(N_s,N_e \)and check robustness of these results with
    respect to increased background activation \(K \) and lower levels
    of inhibition \(\beta\).

    Figure [[ref:fig:pop_act]] shows a section of the population activity
    in the target and distractor stimuli with parameters \(N_s = 100,
    N_e = 50, K= 100, \beta = 2 \),adjusted background contrast of \(\operatorname{bc} =
    4.56\), and with coincidence detector parameters parameters \(s_0 = 30, T = 1000 \).
   
    #+CAPTION: Population activity in background contrast adjusted target and distractor stimulus for parameters \(N_{s}=100,N_e = 50,K=100,\beta=2 \) \label{fig:pop_act}, \(s_0 = 30,T=1000\). 
    #+ATTR_LATEX: :width \linewidth
    [[/home/kima/Dropbox/uni/master/images/coral_draws/threshold_illustration.pdf]]
   

    This gives rise to the threshold distribution sown in panel (c) of
    figure [[ref:fig:threshold_dist]]. Figure [[ref:fig:threshold_dist]] shows
    the distributions \(\operatorname{syn}_1^T,\operatorname{syn}_2^T\)
    for parameters \(N_s=100,N_e=50,K=100,T=1000,\Delta U = 0.022\) and
    different thresholds \(s_0=1,24,30,45\). The maximal accuracy of
    the coincidence detector is reached for paramter \(s_0=300 \) shown
    in panel (c). 

    Panel (a) shows the distributions for avalanche size \(s_0 = 1\),
    which reduces to the number of avalanches occuring in the allowed
    Time horizon. Here the distributions from target and distractor
    are most clearly separated which is easy to understand. The
    average avalanche sizes in the target stimulus is much higher but
    the overall rates are kept equal, which means that the number of
    occuring avalanches in the distractor stimulus is much bigger than
    the number of avalanches occuring in the avalanche input. However
    detecting just the number of occuring avalanches would mean having
    a neuron in the neurophysiological realization tuned such that
    every single arriving spike causes it to fire, which is
    physiologically implausible.

    Looking through panel (b)-(d) we see that up to \(s_{0} = 30\),
    the separability of Target and Distractor distributions increases,
    but for larger \(s_0\) it decreases again. The existence of an
    optimal threshold of intermediate size can be explained like
    follows: 

    When \(s_0\) is too low, not just avalanche sizes resulting from
    the target stimulus cross the threshold but also many avalanches
    from the distractor stimulus, decreasing the separability of the
    distributions \(\operatorname{syn}_1^T\) and
    \(\operatorname{syn}_2^T\). If on the other hand \(s_0\) is too
    large, then even for the target stimulus it is very rare to have
    one or two avalanches with sizes bigger than \(s_0\) during the
    time horizon \(T \), which narrowing the support of the
    distribution and reduces separability from the distractor
    threshold distribution, which is almost fully supported on zero.

   
    \begin{figure}[H]
    \begin{tabular}{cc}
    \includegraphics[width=0.45 \textwidth,height=0.45 \textwidth]{/home/kima/Dropbox/uni/master/images/hist1000,1.pdf} &
  \includegraphics[width=0.45 \textwidth,height=0.45 \textwidth]{/home/kima/Dropbox/uni/master/images/hist1000,24.pdf} \\
    (a)  & (b)\\
    \includegraphics[width=0.45 \textwidth,height=0.45 \textwidth]{/home/kima/Dropbox/uni/master/images/hist1000,30.pdf} &
  \includegraphics[width=0.45 \textwidth,height=0.45 \textwidth]{/home/kima/Dropbox/uni/master/images/hist1000,45.pdf}  \\
   (c) & (d)
    \end{tabular}
    \caption{\label{fig:threshold_dist} \(\operatorname{syn}_1^T,\operatorname{syn}_2^T\)
    for parameters \(N_s=100,N_e=50,K=100,T=1000,\Delta U = 0.022\) and
    different thresholds \(s_0=1,24,30,45\).} 
    \end{figure}


    Going up a level we can draw 2-AFC accuracy heat plots over all
    parameters \(s_0,T \) and fixed \(N_s,N_e,K,\beta\), see figure
    [[ref:fig:heatplots]]. In each heat plot the line connecting the \(s_0\)
    values maximising the accuracy for fixed \(T \) is shown in red.
    Comparing the heatplots shown for \(N_e=42,50,54\) one notices a
    decrease in the maximal accuracy obtainable and for a large
    parameter range an increase of the maximal \(s_0 \) with increasing \(N_e\).
   
    Going even another level up we plot a heat map of the best accuracy
    for a specific \(T=10000\) over the 2-AFC simulation phase space
    shown in panel (c) of figure [[ref:fig:heatplots]].
   
    One notices that approaching the transition region to
    supercriticality performance decreases while on the transition
    region also high maximal accuracys can be obtained. This is
    explained by some realizations producing zero avalanches while
    others are just short the supercritical regime but having very well
    separated avalanches.

    Overall, figure [[ref:fig:heatplots]] shows that the coincidence
    detector achieves very high accuracy in the 2-AFC task over a wide
    range of parameters, thus showing that detecting the activation of
    a critical subnetwork can easily be done with a
    neurophysiologically realizable coincidence detector. Note that
    the best accuracies increase inside the transitioning region to
    criticality. This is explained by the observation that in this
    region many realizations of the weight matrix display
    supercritical behavior, but for the realizations which produce
    finite avalanches, the dynamics between target and distractor can
    be easily discriminated.

    
    #+CAPTION: Heat plots of 2-AFC coincidence detector performance \label{fig:heatplots}. The lower left figure shows the maximal accuracy obtainable in for the coincidence detectors with time horizon \(T=10000\). The dependency of the coincidence detector performance of the parameters \(T,s_0\) is shown in the panels 1-3. For each parameter value \(T\) there is an optimal \(s_{0}\) maximizing the accuracy in the 2-AFC task. These maximizers are connected by the red lines.
    #+ATTR_LATEX: width=\linewidth
    [[/home/kima/Dropbox/uni/master/images/coral_draws/2afc-performance.pdf]]


    Finally, we show that this robustness against changes in K and
    \(\beta \) was shown by simulating the same 2-AFC task with
    parameters \(\beta=2,K=100\) and \(\beta=1,K=200\). As expected,
    increasing the numer of activated elements in the background
    decreases the maximal accuracy achievable by the coincidence
    detector. Lowering the levels of inhibition we have a smaller
    region of phase space available where the networks are
    subcritical. Nevertheless the coincidence detector achieves also
    for this high performance scores in the 2-AFC task.

    

    \begin{figure}[H]
    \begin{tabular}{c}
    \includegraphics[width=0.9 \textwidth]{/home/kima/Dropbox/uni/master/images/best_accuracies_k200_T1000.png} \\
    \includegraphics[width=0.9 \textwidth]{/home/kima/Dropbox/uni/master/images/best_accuracies_k100_T1000_beta1.png} \\
    \end{tabular}
    \caption{maximal accuracy of the coincidence detector for higher background noise (\(K=200\)) and lower levels of inhibition (\(\beta=1\)).}. 
    \end{figure}

   
* Generalized EHE model \label{chap:gen-ehe}
  :PROPERTIES:
  :header-args: :tangle src/ehe.py
  :END:
  

  In this chapter we extend the analytical treatment of the
  homogeneous EHE model given in
  cite:eurich2002finite,levina2008mathematical to the generalized
  EHE model allowing for a large class of nonnegative coupling
  matrices \(W\). We adopt the skew product formulation of the
  dynamical system using a bernoulli shift on the base to model the
  external input paired with a function modeling one time step of the
  slow time scale on the fibers (phase space) \([0,1)^N\).

  We derive the existence of a non-inhabited region where the density
  of states eventually vanishes, and proof that the system acts
  bijectively on the its complement. This  that the system has
  an equilibrium distribution given by the uniform density supported
  on the complement of this non-inhabited region when paired with an
  uniform bernoulli measure on the base.

  With the detailed understanding of the invariant phase space we show
  that for general nonnegative matrices the EHE model is homeomorphic
  to a skewed random walk on a torus.

  We will carefully prove how the almost sure topological transitivity
  follows from the key lemma 7.3.7 in cite:levina2008mathematical. The
  topological transitivity paired with the uniform bernoulli measure
  on the base guarantees ergodicity for the homogeneous model. We
  conjecture that this also holds for a general class of non-constant weight
  matrices.

  Using the (assumption) of ergodicity we construct the probability
  space of occuring avalanche patterns by computing the volumes of the
  regions leading to specific avalanches upon external input.
  Finally, we use this framework to rigorously derive the avalanche
  size statistic reported in cite:eurich2002finite.

  To help the reader in following the steps, we provide a table of
  often used symbols with descriptions and references to the definition.

  | Symbol                                        | Description                                                                             |
  |-----------------------------------------------+-----------------------------------------------------------------------------------------|
  | \(N\)                                         | System size                                                                             |
  | \(\mathcal{N} \)                              | \(\{1,\ldots,N\} \)                                                                     |
  | \(e_1,\ldots,e_N \)                           | Standart basis of \(\mathbb{R}^N\)                                                      |
  | \(\mathds{1}_N \)                             | \(\sum_{i=1}^Ne_i\)                                                                     |
  | \([a_i,b_i)_{i\in I}\)                        | Cylinder set [[ref:eq:def-cyl]]                                                             |
  | \(W_I \)                                      | Submatrix formed of columns and rows of \(I \)                                          |
  | \(\Delta U\)                                  | External input strength                                                                 |
  | \(u\)                                         | point in state space \(u\in [0,1]^N \)                                                  |
  | \(\delta[\text{cond}] \)                      | 1 if cond 0 otherwise [[ref:eq:def-delta]]                                                  |
  | \(J,I,H\)                                     | Index sets with \(\emptyset \neq J\subseteq I \subseteq H \subseteq \mathcal{N}  \)     |
  | \(T\)                                         | EHE model as skew product dynamical system  [[eqref:eq:def-t]]                            |
  | \(E_k\)                                       | Function giving external input to unit \(k\) [[eqref:eq:ext-input]]                         |
  | \(A(x)\)                                      | Indicator vector for superthreshold units [[eqref:eq:Ax]]                                   |
  | \(F(x)\)                                      | Function modeling one step of the avalanche [[eqref:eq:Fx]]                                 |
  | \(k(a,u)\)                                    | number of times F(x) has to be invoked until avalanche stops [[eqref:eq:kau]]               |
  | \(\operatorname{av}(a_1,u) = \)               | Avalanche starting at position u upon external input to \(u_{a_1}\).                    |
  | \((G_i)_{i\in \{1,\ldots,D\}} \)              | G_i denotes the set of units firing at step i [[eqref:eq:def-av]]                           |
  | \(\Omega \)                                   | Set of all possible avalanche patterns \(\operatorname{av}\) [[eqref:eq:def-omega]]         |
  | \(\operatorname{act}(\operatorname{av},i,j)\) | Internal input given to unit \(i\) up to step \(j\) of avalanche \(\operatorname{av} \) |
  | \(\Gamma(W,U,I) \)                            | Part of non-inhabited volume generated by  \(I \) [[eqref:eq:def-gamma]]                    |
  | \(\Lambda(W,U,H) \)                           | Noninhabited region along dimensions \(H\) [[eqref:eq:def-Lambda]]                          |
  | \(\mathcal{V} \)                              | N-dimensional Volume                                                                    |
  | \(D \)                                        | Complement of the Non-inhabited region in the phase space [[eqref:eq:def-D]]                |
  | \(R_{\operatorname{av}} \)                    | Region leading to avalanche pattern \(\operatorname{av}\) [[ref:eq:def-R-av]]               |



** EHE model Implementation                                  :implementation:
  :PROPERTIES:
  :header-args: :tangle src/ehe.py
  :END:
   In this section a reference simple python implementation of the ehe
   model is provided

   #+begin_src ipython :session master
   from utils import *
   import pickle
   import numpy as np
   import matplotlib.pyplot as plt
   from wurlitzer import sys_pipes
   import sys
   from multiprocessing.pool import ThreadPool

   #+end_src

   #+RESULTS:

   #+begin_src ipython :session master :tangle no
    pool = ThreadPool(4)
   #+end_src 

   #+RESULTS:

   As getting accurate avalanche distributions of large systems is
   computationally intensive, it is a good idea to let efficient c++
   code do the computations and use an interface to python so that
   quick prototyping and analyzing of results can by done in python.
   Pybind11 provides such a bridge and provides bindings also for the
   highly optimized matrix library arma. Cppimport automatically
   compiles the cpp sources.

    The simulation of the model is simplified by the infite separation
    of timescales. The time evolution of the model can be splitted
    into an outer loop that takes the role of the external input and
    an inner loop that handles the resulting avalanches.

    The outer loop initializes the network to a random state and applies
    for the desired number of iterations the external input while
    collecting the resulting avalanche statistics.
    #+begin_src ipython :session master
      import numpy as np
      U = 1 # upper limit
      N = 10000
      epsilon = 1 # reset parameter
      deltaU = 0.022
      alpha = 1 - 1/np.sqrt(N)

      def ehe_critical_weights(N,U=1):
          return np.ones([N,N])*(1 - 1/np.sqrt(N)) * U / N

      def simulate_model(units=np.random.random(N)*U,numAvalanches=1000,
                         W=None,deltaU = deltaU):
          if W is None:
              W = ehe_critical_weights(len(units))
          avalanche_sizes = []
          avalanche_durations = []
          sumavd = 0
          while sumavd < numAvalanches:
              r = np.random.randint(len(units))
              units[r] += deltaU
              if units[r] >= U:
                  avs,avd = handle_avalanche(r,units,W,epsilon=1,U=U)
                  avalanche_sizes.append(avs)
                  avalanche_durations.append(avd)
                  sumavd += avd
          return avalanche_sizes,avalanche_durations
    #+end_src 

    #+RESULTS:


    The inner loop handles the avalanche by calculating \(A(t)\),
    resetting the energy for the spiking neurons and adding the
    resulting internal input to the neurons iteratively until the
    internal input doesn't cause threshold crossings anymore.


    #+begin_src ipython :session master
          def handle_avalanche(start_unit,units,W,epsilon=1,U=U):
              avalanche_size = 0
              avalanche_duration = 0
              # handle starting single unit avalanche
              A = np.zeros_like(units)
              A[start_unit] = 1
              units[start_unit] = epsilon * (units[start_unit] - U)
              s = 1
              while s > 0:
                  avalanche_size += s
                  avalanche_duration += 1
                  units += W * A #interior input to all units
                  A = units >= U
                  units[A] = epsilon * (units[A] - U) # resetting threshold crossed units
                  s = np.sum(A)
              return avalanche_size,avalanche_duration

    #+end_src 

    #+RESULTS:



   For a detailed analysis of the dynamics of the EHE model, not only
   the avalanche sizes and durations should be analyzed but the whole
   spiking behavior of the system, that means when did which neurons spike.

   In order to investigate whether the dynamics of a subnetwork is
   closer to criticality than other subnetworks or the system as a
   whole, one has to record not only the avalanche sizes and durations
   but for each internal activation step the indices of the units that
   are firing. The return type is then a num-avalanches vector of
   avalanche-results which each consist of a avalanche_duration length
   vector containing the indices of each firing unit during one avalanche step.

  #+BEGIN_SRC c++ :noweb no-export :tangle src/cpp/ehe_detailed.cpp :eval no
    /* <%

       cfg['linker_args'] = ['-lopenblas']
       setup_pybind11(cfg)
       cfg['compiler_args'] = ['-O3','-std=c++11','-march=native']
       %>
    ,*/
    #define ARMA_DONT_USE_WRAPPER
    #define ARMA_DONT_USE_LAPACK
    #define ARMA_USE_BLAS

    #include <pybind11/pybind11.h>
    #include <pybind11/stl.h>

    #include <iostream>
    #include <armadillo>
    #include <pybind11/pybind11.h>
    #include <pybind11/stl.h>
    #include "pybind11_armadillo.h"
    #include "arma_supportlib.h"

    #include <cstdlib>
    #include <cmath>
    #include <tuple>
    #include <list>
    #include <random>
    #include <iostream>
    #include <limits>

    namespace py = pybind11;
    using namespace std;
    using namespace arma;
    using inds_t = unsigned short;
    using avalanche_t = vector<inds_t>;
    const auto U = 1;
    //const auto N = 1000;
    const auto epsilon = 1;
    const auto delta_u = 0.022;
    //const auto alpha = 1 - 1/std::sqrt(N);
    const inds_t STEP_SEP = numeric_limits<inds_t>::max();

    struct SupcritException {};

    struct EHE__module_suffix__ {

      vector<inds_t> activated_units;
      vector<unsigned int> act_inds;
      vector<inds_t> avalanches;
      vector<unsigned int> avs_inds;
      Col<inds_t> avs;
      Col<unsigned int> inds;
      bool record_units = false;
      mat unit_hist;
      bool clamp_neg = false;
      vector<unsigned int> avs_sizes;
      vector<unsigned int> avs_durations;
      float max_duration_factor = 10;

      template <typename Func>
      void handle_avalanche(unsigned int start_unit,vec  &units,
                            Func &int_func, double epsilon=1, int U=U) {
        units(start_unit) = epsilon*(units(start_unit) - U);
        uvec A = {start_unit};
        vector<inds_t> Avec;
        unsigned int avalanche_duration = 0;
        unsigned int avalanche_size = 0;
        while(!A.is_empty()) {
          avalanche_size += A.n_elem;
          avalanche_duration++;
          if(avalanche_duration > max_duration_factor*units.n_elem) {
            cout << "TOO BIG I QUIT!" << endl;
            throw SupcritException();
          }
          int_func(units,A);
          if(clamp_neg) units.elem(find(units < 0)).zeros(); // reset negative values to zero
          Avec = conv_to<vector<inds_t>>::from(move(A));
          avalanches.insert(avalanches.end(),make_move_iterator(Avec.begin()),make_move_iterator(Avec.end()));
          A = find(units > U);
          if(!A.is_empty()) avalanches.push_back(STEP_SEP);
          units.elem(A) = epsilon*(units.elem(A) - U);
        }
        avs_sizes.push_back(avalanche_size);
        avs_durations.push_back(avalanche_duration);
      }


      template <typename Func>
      void simulate_model(vec &units,unsigned int numAvalanches,double deltaU,
                          const Col<inds_t> &external_weights,Func int_func) {
        py::gil_scoped_release release;
        avalanches = vector<inds_t>();
        avs_inds = vector<unsigned int>();
        avs_inds.reserve(numAvalanches);
        avalanches.reserve(10*numAvalanches); //reserve least amount of space needed
        avs_sizes = vector<unsigned int>();
        avs_sizes.reserve(numAvalanches);
        avs_durations = vector<unsigned int>();
        avs_durations.reserve(numAvalanches);
        activated_units = vector<inds_t>();
        activated_units.reserve(numAvalanches);
        act_inds = vector<unsigned int>();
        act_inds.reserve(numAvalanches);
        vector<vec> uh;
        std::random_device rd;  
        std::mt19937 gen(rd());
        std::discrete_distribution<> dis(external_weights.begin(),external_weights.end());
        unsigned int avalanche_counter=0;
        unsigned int r;
        act_inds.push_back(0);
        while(avalanche_counter < numAvalanches){
          r = dis(gen);
          activated_units.push_back(r);
          units(r) += deltaU;  
          if(units(r) >= U) {
            act_inds.push_back(activated_units.size());
            avs_inds.push_back(avalanches.size());
            try {
              handle_avalanche(r,units,int_func,1,U);
            } catch(SupcritException & ex) {
              cout << "catch exception end now" << endl;
              avalanches.push_back(STEP_SEP); //two following STEP_SEPs at the end signal error
              avalanches.push_back(STEP_SEP);
              break;
            }
            if(avalanche_counter % 1000 == 0){ 
              cout << "avalanche counter " << avalanche_counter << endl;
            }
            avalanche_counter++;
          }
          if(record_units) {
              uh.push_back(units);
          }
        }
        if(record_units) {
          unit_hist = mat(uh.size(),units.n_elem);
          for(unsigned int i = 0; i < uh.size();i++) {
            unit_hist.row(i) = uh[i].t();
          }
        }
        avs  = arma::Mat<inds_t>(avalanches.data(),avalanches.size(),1,false,true);
        inds = arma::Mat<unsigned int>(avs_inds.data(),avs_inds.size(),1,false,true);
      }

      void simulate_model_mat(vec &units, int numAvalanches,const mat &W,double deltaU,Col<inds_t> &external_weights) {
        simulate_model(units,numAvalanches,deltaU, external_weights,
                       [&](vec &units,uvec &inds) {
                         vec A(size(units),fill::zeros);
                         A.elem(inds) += 1;
                         units += W*A;});
      }

      void simulate_model_mat(vec &units, int numAvalanches,const mat &W,double deltaU) {
        auto external_weights = Col<inds_t>(units.n_elem,fill::ones);
        simulate_model_mat(units,numAvalanches,W,deltaU, external_weights);
      }

      void simulate_model_const(vec &units, int numAvalanches,double w,double deltaU) {
        auto external_weights = Col<inds_t>(units.n_elem,fill::ones);
        simulate_model(units,numAvalanches,deltaU, external_weights,
                       [=](vec &units,uvec &inds) { units += w*inds.n_elem;});
      }

      tuple<vector<int>,vector<int>>
      get_avs_size_and_duration(Col<inds_t> &avs,vector<unsigned int> &avs_inds) {
        py::gil_scoped_release release;
        vector<int> avs_sizes;
        vector<int> avs_durations;
        Col<inds_t> avalanches;
        int step_separators;
        for(unsigned int i=0; i < avs_inds.size()-1;i++) {
          avalanches = avs.subvec(avs_inds[i],avs_inds[i+1]-1);
          step_separators = static_cast<uvec>(find(avalanches == STEP_SEP)).n_elem;
          avs_durations.push_back(step_separators+1);
          avs_sizes.push_back(avs_inds[i+1] - avs_inds[i] - step_separators);
        }
        return make_tuple(avs_sizes,avs_durations);
      }

      tuple<vector<int>,vector<int>> get_avs_size_and_duration() {
        return get_avs_size_and_duration(avs,avs_inds);
      }

      vector<vector<vector<inds_t>>>
      get_spiking_patterns(Col<inds_t> & avs, vector<unsigned int> &avs_inds) {
        py::gil_scoped_release release;
        vector<vector<vector<inds_t>>> spiking_patterns;
        spiking_patterns.reserve(avs_inds.size());
        for(unsigned int i=0; i < avs_inds.size()-1;i++) {
          vector<vector<inds_t>> sp;
          vector<inds_t> cur_step;
          int ub = avs_inds.size();
          if(i < avs_inds.size()-1) {
            ub = avs_inds[i+1];
           }
          for(unsigned int j=avs_inds[i];j < ub;j++) {
            if(avs[j] != STEP_SEP) {
              cur_step.push_back(avs[j]);
            } else {
              sp.push_back(move(cur_step));
              cur_step.clear();
            }
          }
          sp.push_back(move(cur_step));
          cur_step.clear();
          spiking_patterns.push_back(move(sp));
          sp.clear();
        }
        return spiking_patterns;
      }

      vector<vector<vector<inds_t>>> get_spiking_patterns() {
        return get_spiking_patterns(avs,avs_inds);
      }

      tuple<Col<inds_t>,vector<unsigned int>>
      subnetwork_avalanches(Col<inds_t> &avs,vector<unsigned int> &avs_inds,
                            set<inds_t> &subnet_inds) {
        py::gil_scoped_release release;
        vector<unsigned int> subavs_inds = {0};
        vector<inds_t> subavs;
        unsigned int subavs_idx = 0;
        inds_t cur;
        unsigned int avs_inds_idx = 0;
        for(unsigned int avs_idx=0; avs_idx < avs.n_elem; avs_idx++) {
          if(avs_idx == avs_inds[avs_inds_idx]) { // reached a new avalanche, add it to subavalanche indices
            avs_inds_idx++;
            if (subavs_inds.back() != subavs_idx) { // but only if at least one element in subnetwork spiked
              subavs_inds.push_back(subavs_idx);
            }
          }
          cur = avs[avs_idx];
          if(cur == STEP_SEP) {
            if(subavs_idx > 0 && subavs.back() != cur) { // add step sep but not if no subnetwork element spiked since last step sep
              subavs.push_back(cur);subavs_idx++;
            } else if(subavs_idx > subavs_inds.back()) {
              //current avalanche contained subnetwork elements but not in this step,
              //-> next subnetwork index starts new avalanche
              subavs_inds.push_back(subavs_idx);
            }
          } else if(subnet_inds.find(cur) != subnet_inds.end()) {
            subavs.push_back(cur);subavs_idx++;
          }
          //otherwise a unit spiked that is not in the subnet - will be ignored
        }
        if(subavs_inds.back() >= subavs.size()) {
          subavs_inds.pop_back(); //remove trailing avs of size 0
        }
        return make_tuple(conv_to<Col<inds_t>>::from(subavs),subavs_inds);
      }

      tuple<Col<inds_t>,vector<unsigned int>>
      subnetwork_avalanches(set<inds_t> &subnet_inds) {
        return subnetwork_avalanches(avs,avs_inds,subnet_inds);
      }

    };

    void test(mat & test) {
      std::cout << "Hi there from c++" << std::endl;
    }

    using EHE = EHE__module_suffix__;

    PYBIND11_PLUGIN(ehe_detailed__module_suffix__) {

      py::module m("ehe", "Simulate ehe model");
      py::class_<EHE>(m, "EHE")
        .def(py::init<>())
        .def("simulate_model_mat",(void (EHE::*)(vec&,int,const mat&,double)) &EHE::simulate_model_mat)
        .def("simulate_model_mat",(void (EHE::*)(vec&,int,const mat&,double,Col<inds_t>&)) &EHE::simulate_model_mat)
        .def("simulate_model_const",&EHE::simulate_model_const)
        .def("get_avs_size_and_duration",(tuple<vector<int>,vector<int>> (EHE::*)(void)) &EHE::get_avs_size_and_duration)
        .def("get_avs_size_and_duration",
             (tuple<vector<int>,vector<int>> (EHE::*)(Col<inds_t>&,vector<unsigned int>&)) &EHE::get_avs_size_and_duration)
        .def("subnetwork_avalanches",
             (tuple<Col<inds_t>,vector<unsigned int>> (EHE::*)(Col<inds_t>&,vector<unsigned int>&,set<inds_t>&))
             &EHE::subnetwork_avalanches)
        .def("subnetwork_avalanches",
             (tuple<Col<inds_t>,vector<unsigned int>> (EHE::*)(set<inds_t>&)) &EHE::subnetwork_avalanches)
        .def("get_spiking_patterns", (vector<vector<vector<inds_t>>> (EHE::*)(void)) &EHE::get_spiking_patterns)
        .def("get_spiking_patterns", (vector<vector<vector<inds_t>>> (EHE::*)(Col<inds_t>&,vector<unsigned int>&))
             &EHE::get_spiking_patterns)
        .def_readwrite("avalanches",&EHE::avalanches)
        .def_readwrite("avs",&EHE::avs)
        .def_readwrite("unit_hist",&EHE::unit_hist)
        .def_readwrite("clamp_neg",&EHE::clamp_neg)
        .def_readwrite("record_units",&EHE::record_units)
        .def_readwrite("activated_units",&EHE::activated_units)
        .def_readwrite("act_inds",&EHE::act_inds)
        .def_readwrite("avs_inds",&EHE::avs_inds)
        .def_readwrite("avs_durations",&EHE::avs_durations)
        .def_readwrite("max_duration_factor",&EHE::max_duration_factor)
        .def_readwrite("avs_sizes",&EHE::avs_sizes);

      m.def("test",&test,"test");

      return m.ptr();

    }
   #+END_SRC 

   
** The generalized EHE-Model as a skew-product
   :PROPERTIES:
   :header-args: :tangle src/ehe_ana.py
   :END:
   
   This section outlines an approach to model the dynamics of the EHE
   model in the framework of dynamical systems theory, and extends the
   approach taken in cite:levina2008mathematical.
   
   To do so, we have to take away the stochasticity of the external
   input. We can do this by going from the one dimensional stochastic
   drive to a shift operation on the infininte dimensional sequence
   space. Abstracting the fast timescale avalanche
   dynamics in a function \(H\) (defined in [[ref:eq:def-H]]), the generalized EHE model can be
   modeled by a skew-product dynamical system of the form
   \begin{align}
   T&: \Sigma_{\mathcal{N}}^+ \times [0,1)^{N} \rightarrow \Sigma^+_{\mathcal{M}} \times [0,1)^N \nonumber \\
   T&(a,u) = (\sigma(a),H(a,u)) \text{ .}
   \label{eq:def-t} 
   \end{align}
   
   \(u \in [0,1)^N \),\(\Sigma_N^+\) is the space of right infinite
   sequences \(a = a_1,a_2,\ldots \in \Sigma_N^+\) over the alphabet
   \(a_i \in \{1,\ldots,N\} \) and let the measurable space of these
   sequences \(A \) be equipped with a probability measure \(\nu\)
   which is the uniform bernoulli measure on \(\mathcal{N} =
   \{1,\ldots,N\}\). Each application of \(T \) represents one time
   step on the slow external activation timescale, with unit \(a_1\)
   receiving external input and spreading of activation induced by the
   potentially resuling avalanche.

   \(H \) can be written as composition of first handling the external
   input by \(E_{a_1} \) [[eqref:eq:ext-input]] followed by fixpoint
   application of \(F \) [[eqref:eq:Fx]], which handles the internal
   input.

   Let \(e_1,\ldots,e_N\) denote the standard basis of \(R^{N} \). To
   handle the external input, the coordinate of the unit receiving
   external input is increased by \(\Delta U \).
   \begin{align}\label{eq:ext-input}E_k(u) := u + e_k\Delta U \end{align}

   We define \(A(x)\) to be vector indicating which units are above
   the threshold
   \begin{align}A(x) := (\delta[x_i \geq 1])_{i=1,\ldots,N}\label{eq:Ax}\text{ , }\end{align}
   where 
   \begin{align}
   \label{eq:def-delta}
   \delta[\text{cond}] = \begin{cases} 1 &\mbox{ if cond}  \\
                                          0 &\mbox{ otherwise .} 
                           \end{cases}
   \end{align} 

   Using this indicator vector \(A\), the procedure to reset the
   spiking units and to distribute internal activation in the network
   is described by 
   \begin{align}F(x) := x - A(x) + WA(x) \text{ .}\label{eq:Fx}\end{align}

   In this chapter we analyze the dynamical system in the regime where
   avalanche sizes are bounded by \(N \) and each unit can fire at
   most once in an avalanche. As shown in proposition
   [[ref:thm:at-most-once]], this is ensured by the assumptions \(W \geq
   0\) componentwise and \(W\sum_{i=1}^Ne_i+ \Delta U < 1 \).
   Additionally we require \(1 - 2W\sum_{i=1}^Ne_i \geq 0\).

   *Assumptions:* Throughout this chapter we restrict the analysis to the class of
   weight matrices \(W\) satisfying     
   \begin{align}
   \label{eq:assumptions}
   W \geq 0,W\sum_{i=1}^Ne_i+ \Delta U < 1,1 - 2W\sum_{i=1}^Ne_i \geq 0 \text{ componentwise .}
   \end{align}
   
   We now define \(H(a,u)\) modeling one step on the slow timescale by
   first giving external input to the system with the function
   \(E_{a_1}\) followed by distributing internal activation with \(F\)
   until no unit is above threshold.
   \begin{align}H(a,u) := F^{k(a_1,u)} \circ E_{a_1}(u) \text{ ,} \label{eq:def-H}\end{align}
   where 
   \begin{align}k(a_1,u) := \min_{k\in \mathbb{N}_{\geq 0}}F^k \circ E_{a_1}(u) \in [0,1)^N \label{eq:kau}\text{ .}\end{align} 

   When \(k(a_1,u) > 0\) we say that an avalanche occurs in the
   system. The function \(\operatorname{av}(a_1,u)\) returns the
   avalanche resulting by applicatin of \(H(a_1,u)\) and is defined
   by
   \begin{align}
   \operatorname{av}(a_1,u) &:= (G_i)_{i=1,\ldots,k(a,x)} \nonumber \\
   G_i &= \{j \in \mathcal{N} | A_j(F^{i-1}\circ E_{a_1}(u)) = 1\}.
   \label{eq:def-av}
   \end{align}

   \(G_i\) denotes the set of indices that fired during step \(i\).
   When no avalanche occurs \(\operatorname{av}(a_1,u)\) is given by
   the empty sequence \(\operatorname{av}(a_1,u) = ()\). Since an
   avalanche is started always by the external input pushing exactly
   one unit above the threshold we have \(\operatorname{av}(a_1,u)_{1} =
   \{a_1\} \).
   
   #+BEGIN_prop
   \label{thm:at-most-once} If \(\sum_{j=1}^N w_{ij} + \Delta U < 1\)
   for all \(i=1,\ldots,N\), \(k(a,u) \leq N\) and each unit can fire
   at most once during an avalanche and we have for
   \(\operatorname{av}(a_1,u) = (G_i)_{i=1,\ldots,k(a,x)} \) that
   \[\biguplus_{i=1}^{k(a,u)}G_i \subseteq N \text{ .}\]
   #+END_prop

   #+BEGIN_proof
   We will give a proof by contradiction. Let \(u\in [0,1)^N,a_1 \in
   \mathcal{N}\) be arbitrary and let \(j\) be a unit that spiked
   twice during an avalanche at steps \(s_1\) and \(s_2\) and no other
   unit spiked twice before step \(s_2\). 
   It follows that
   \(A_j(F^{l-1}\circ E_{a_1}(u)) = 1\) for \(l\in \{s_1,s_2\}\). But
   since no other unit spiked twice before \(s_2 \),
   it also follows that
   \begin{align*}
   F^{s_2-1}(E_{a_1}(u))_j &= E_{a_1}(u)_j + (W\sum_{l=1}^{s_2}A(F^{l-1}\circ E_{a_1}(u)))_j - 1 \\
   &\leq  E_{a_1}(u)_j - 1 + \sum_{l=1}^N w_{jl} \leq \Delta U + \sum_{l=1}^N w_{jl} < 1 \text{,}
   \end{align*} 
   which contradicts \(A_j(F^{s_2-1}\circ E_{a_1}(u)) = 1\) This shows
   that every unit can spike at most once during an avalanche and
   using [[eqref:eq:def-av]] we have that the \(G_i\) are pairwise disjoint.
   \(k(a_1,u) \leq N\) follows immediately.
   #+END_proof


   This Proposition ensures that the set of avalanches in a network of
   \(N\) unitsis is finite. We denote this set by \(\Omega \):
      \begin{align}
   \label{eq:def-omega}
   \Omega := \left \{(G_i)_{i=1\ldots D} \bigg | \emptyset \neq G_i \subseteq 
   \mathcal{N} \forall i \in \{1\ldots D\},\biguplus_{i=1}^DG_i
    \subseteq \mathcal{N},|G_1| \leq 1, D \geq 0 \right \}
   \end{align}

   We introduce two auxillary functions to shorten the following
   derivations in this chapter. \(\mathcal{U}\) represents the set of
   units firing in the avalanche.
   \begin{align}
   \mathcal{U}((G_i)_{i=1,\ldots,D},s,e) &:= \biguplus_{i=s}^{e}G_i \nonumber\\
   \mathcal{U}((G_i)_{i=1,\ldots,D}) &:= \mathcal{U}((G_i)_{i=1,\ldots,D},1,D) 
   \end{align}

   \(\operatorname{act}(\operatorname{av},i,j)\) is  the
   internal input (activation) given to unit \(i\) up to step \(j\) of the avalanche \(\operatorname{av}\).
   \begin{align}
   \operatorname{act}(\operatorname{av},i,j) := \sum_{k = 1}^{j-1}\sum_{l \in G_k}w_{i,l}
   \label{eq:def-act}
   \end{align}
    \\
    #+BEGIN_lem
    \label{lem:av_action}
    Let \(\operatorname{av} = \operatorname{av}(a_1,u) = (G_i)_{i=1,\ldots,D}\). Then 
    \begin{align*}
    (F^{j-1}\circ E_{a_1}(u))_i - E_{a_1}(u)_i = \operatorname{act}(\operatorname{av},i,j) - \delta[i\in \mathcal{U}(\operatorname{av},1,j-1)]\text{ .}
    \end{align*}
    #+END_lem

    #+BEGIN_proof
    If \(i\in \mathcal{U}(\operatorname{av},1,j-1) = \biguplus_{k=1}^{j-1}G_k \), then from [[eqref:eq:def-av]] we have 
    \begin{align*}
    (F^{j-1} \circ E_{a_1}(u))_i &= E_{a_1}(u) - 1 + \sum_{k=1}^{j-1}WA(F^{k-1}\circ E_{a_1}(u)_i \\
    &= E_{a_1}(u) - 1 + \sum_{k=1}^{j-1}\sum_{l\in G_k}w_{il} \text{ .}
    \end{align*}
    Similarly, if \(i \notin \biguplus_{k=1}^{j-1}G_k \), \((F^{j-1}
    \circ E_{a_1}(u))_i = E_{a_1}(u) + \sum_{k=1}^{j-1}\sum_{l\in
    G_k}w_{il} \), which proves the statement.
    #+END_proof




** Overview of the qualitative dynamics of the EHE-Model     :implementation:
   Almost all qualitative properties of the homogeneous EHE-Model
   still hold for the generalized EHE-Model. We will shortly describe
   this properties and how they change for no
   
   - density of states vanish in non-inhabited region
   - in complement system acts bijectively and has uniform equilibrium density
   - show images with different weight matrices
   - goal to really understand this behaviour mathematically
   - describe how the strategy here differs from the approaches taken previously
   
   See figure [[ref:fig:phase-space-2]] for an illustration.

    #+ATTR_LATEX: :width 200px :height 200px
    #+CAPTION: 2d histogram of phase space density for the 2-unit EHE Model with weight matrix \(W = \alpha_{crit} \begin{pmatrix}1.5 & 1 \\ 1 & 1.3 \end{pmatrix}\)
    [[file:./images/20170622_123227_1871Jvwe3r.png]]


   
** Non-inhabited region of phase space \label{sec:noninhabited-region}

   The following considerations show that an invariant measure can't
   have the whole phase space \([0,1]^N\) as support and that there is
   a /non-inhabited region/ which can not be entered by the dynamical
   system:

   Suppose that unit \(u_{i}\) starts an avalanche in which the units
   in \(I \subseteq \mathcal{N}\) fire, i.e
   \(\operatorname{av}=(G_i)_{i=1,\ldots,D},G_1=\{i\},\mathcal{U}(\operatorname{av})
   = I\). The coordinate of unit \(u_i\) after the avalanche stopped
   can not be smaller than the total input it received during the
   avalanche. Applying this reasoning for each possible starting unit
   \(u_j,j \in I\) for avalanches with
   \(\mathcal{U}(\operatorname{av}) = I \) identifies a hyperrectangle
   which can never by entered by the dynamical system, i.e every point
   in the hyperrectangle has no preimage under \(T\). This statement
   will be proven later in theorem [[ref:thm:invariant]].

   We start by describing this region of phase space as a union of
   all the noninhabited regions generated by the nonempty subsets
   \(I\). In the following, \(\emptyset \neq J \subseteq I \subseteq
   H \subseteq \mathcal{N}\) denote index sets. We introduce the
   following notation for cylinder sets:
   \begin{align}
    \left [a_i,b_i\right )_{i\in K} := \left \{x\in [0,1)^N | a_{i} \leq x_i < b_i \text{ for all } i \in I \right \}
    \label{eq:def-cyl}
    \end{align}

   The part of the non-inhabited Region generated by \(I\) is defined by
    \begin{equation}
    \Gamma(W,I) := \left[0, \sum_{j \in I}w_{i j} \right )_{i \in I} \text{ .}
    \label{eq:def-gamma}
    \end{equation}

    The total non-inhabited region along the dimensions \(H \) is the union of the non-inhabited
    regions generated by all the possible subsets \(\emptyset \neq I \subseteq H \)
    \begin{equation}
    \label{eq:def-Lambda}
    \Lambda(W,H) := \bigcup_{\emptyset \neq I \subseteq H} \Gamma(W, I) \text{ .}
    \end{equation} 

    Note that \(\Lambda(W,\emptyset) = \emptyset \). See figures
    [[ref:fig:ovl-gammas2d]] and [[ref:fig:ovl-gammas3d]] for Illustrations of
    the non-inhabited regions in 2 and 3 dimensions formed by the
    union of overlapping \(\Gamma\) sets.

#    \begin{landscape}
#     \centering
#     \begin{figure}[H]
#    \begin{tabular}{cc}
# \includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master//images/20171206_125529_8888mEo.png} & 
# \includegraphics[width=0.5\linewidth]{/home/kima/Dropbox/uni/master//images/20171207_013200_8888RKb.png} \\
# \\  (a)  & (b))\\
#    \end{tabular}
#    \caption{Non-inhabited regions formed by union of overlapping \(\Gamma(W,I)\) for \(\emptyset \neq I \subseteq \mathcal{N}\). (a) \(\Gamma(W,\{1\}) \) (blue region), \(\Gamma(W,\{2\})\) (red region) and   \(\Gamma(W,\{1,2\})\) (orange region) for \(N=2\). (b) TODO Annotate directly in the plot \label{fig:ovl-gammas}}
#    \end{figure}
#    \end{landscape}


   #+ATTR_LATEX: :width 300px 
   #+CAPTION: Non-inhabited region for the 2-dimensional EHE model formed by the union of non-inhabited regions generated by the subsets of units \(\emptyset \neq I \subseteq \{1,2\}\). If the coordinate \(u_1\) is set above the threshold by the external input \(E_1\), action of \(F\) resets the activation value by substracting \(1\), but also adding the internal activation given by \(w_{1,1}\). It follows that the new state vector of the system has \(u_1 \geq w_{1,1}\). \(u_1\) not only gives internal activation to itself but also to \(u_2\) of size \(w_{21}\). Should this in turn cause \(u_2\) to fire, \(u_1 \) receives the additional activation \(w_{12}\) so that the new system state can't be inside of \(\Gamma(W,\{1,2\})\) \label{fig:ovl-gammas2d}.  
   [[/home/kima/Dropbox/uni/master/images/gammas2.pdf]]

   #+ATTR_LATEX: :width 450px
   #+CAPTION: Illustration Non-inhabited region for the 3-dimensional EHE model formed by the union of non-inhabited regions generated by \(\emptyset \neq I \subseteq \mathcal{N}\) \label{fig:ovl-gammas3d}.
   [[/home/kima/Dropbox/uni/master/images/Gammas.pdf]]

    Given in this form, the volume of \(\Lambda(W,H) \), for \(H\subseteq \mathcal{N}\), is not easy
    to calculate by inclusion-exclusion. Fortunately, there are
    self-similarities in this region that can be used to derive a
    recursive decomposition into non-overlapping regions. Based on
    this decomposition it is possible to derive a closed form for the
    volume of the non-inhabited phase space.
    
    The strategy to arrive at the recursive decomposition consists of three steps:
    1. Perform a suitable decomposition of the state space by disjoint hyperrectangles.
    2. Observe that the union of \(\Gamma\) intersecting such a hyperrectangle form a lower dimensional non-inhabited volume.
    3. Write \(\Lambda(W,H) \) as a disjoint union of these lower dimensional non-inhabited volumes.

*** Decomposition of the Phase space by disjoint hyperrectangles
    # The maximum coordinate of the excluded region for the unit \(i\)
    # occuring in the avalanche pattern is \(\sum_{j\in
    # \mathcal{N}}w_{i,j} \) for  \(I =
    # \mathcal{N} \). Splitting each axis at this coordinate leads to
    # the decomposition of the phase space

    In figure [[ref:fig:ovl-gammas3d]] one can see the self-similar
    structure of the noninhabited volume. The faces of the cube with
    \(u_i=1,i\in \{1,2,3\}\) each display a two dimensional
    non-inhabited volume while there is additional structure found at
    the origin (\(\Gamma(W,\{1,2,3\})\). We make this observed
    self-similarity concreate by decomposing the phase space into
    non-overlapping hyperrectangles and showing the self-similarities
    in the intersections.
    
    We introduce the decomposition 
     \begin{equation}
    [0, U_i]_{i\in H} =  \biguplus_{ I \subseteq H} R(W,H,I) \text{, }
    \end{equation}
     with
    \begin{equation}
    \label{def:R}
    R(W,H,I) = \left [0, \sum_{j \in H}w_{i j}\right )_{i \in I} \cap \left [\sum_{l\in H} w_{k l},1 \right )_{k \in H\setminus I} \text{ .}
    \end{equation}
    
    This decomposition corresponds to the following expansion for the
    volume of the phase space:
    \begin{equation}
    \mathcal{V}([0, U_i]_{i\in H}) = \prod_{i\in H} U_i = \prod_{i\in H} (\sum_{j \in H} w_{i j} + (U_i - \sum_{j\in H} w_{i j})) \text{ .}
    \end{equation}

    See figure [[ref:fig:decomp]] for visualizations of these hyperrectangles for \(\mathcal{N} = 3\).

    #+ATTR_LATEX: :width 450px
    #+CAPTION: Decomposition of \([0,1)^3\) into the non-overlapping rectangles. \(R(W,\mathcal{N},I)\) contains the non-inhabited region generated by \(I\) \label{fig:decomp}.
    [[/home/kima/Dropbox/uni/master/images/Rs.pdf]]
    
#    \begin{landscape}
#     \centering
#     \begin{figure}[H]
#    \begin{tabular}{cc}
# \includegraphics[width=0.3\linewidth]{/home/kima/Dropbox/uni/master//images/20171207_010536_88883ua.png} & 
# \includegraphics[width=0.55\linewidth]{/home/kima/Dropbox/uni/master//images/20171207_010355_8888qkU.png} \\
# \\  (a)  & (b))\\
#    \end{tabular}
#    \caption{Decomposition of \(\Lambda(W,\mathcal{N})\) for \(N=2\) (a) and \(N=3\) (b) with non-overlapping hyperrectangles
#    \(R(W,\mathcal{N},I)\) for \(\emptyset \neq I\subseteq \mathcal{N}\). This illustrates \(R(W,U,\mathcal{N},\mathcal{N} = \Gamma(W,\mathds{1}_N,\mathcal{N}\) and gives the geometric intuition for Lemma~\ref{lem:one} ,see intersection of \(\Lambda(W,\{1,2,3\})\) with \(R(W,\mathcal{N},\{2,3\})\) (light blue area) \label{fig:decomp}}.
#    \end{figure}
#    \end{landscape}

*** Self similarity of the non-inhabited volume
    
    Figure [[ref:fig:decomp]] graphically suggest that
    \label{sec:self-similarity} The overlap between a $R(W,H,I)$ and
    \(\Lambda(W,H) \) is given by the overlap between \(R(W,H,I)\) and
    the non-inhabitated region along the dimensions \(I\). \\

    #+BEGIN_lem
    \label{lem:one} For \(\emptyset \subseteq I \subseteq H \subseteq \mathcal{N}\) we have
    \begin{equation}
      R(W,H,I) \cap \Lambda(W,H) = R(W,H,I) \cap \Lambda(W,I) \text{ .}
    \end{equation}
    #+END_lem
    
#+BEGIN_proof
    Note that by definition \(\Lambda(W,H) = \bigcup_{\emptyset \neq I \subseteq H}\Gamma(W,I)\).
    The result follows if \( R(W,H,I) \cap \Gamma(W,J) = \emptyset\) for all \(\emptyset \neq J \subseteq H\setminus I\).
    \begin{align*}
    R(W,H,I) \cap \Gamma(W,J) =\left [0, \sum_{j \in H}w_{i j}\right )_{i \in I} \cap \left [\sum_{l\in H} w_{k l}, 1 \right )_{k \in H\setminus I}
     \cap \left[0, \sum_{j \in J}w_{i j} \right )_{i \in J} = \emptyset \text{, }
    \end{align*}
    because the intersection along \(j \in J \cap (H\setminus I) \) is empty.
#+END_proof

   

*** Recursive Decomposition of the non-inhabited region
    Using the results from the previous steps, the non-inhabited
    region can be recursively decomposed into disjoint subsets.\\
    
    #+BEGIN_thm 
    \label{thm:rec-dec}For \(W \geq 0, \sum_{j\in \mathcal{N}}w_{ij} + \Delta U \leq 1
  \text{ for all } i\in \mathcal{N}\),\(\emptyset \neq H \subseteq \mathcal{N}\) and
 \(U = (U_i)_{i\in H},\sum_{j\in H}w_{ij} \leq U_i \leq 1\) for all \(i\in H \) we have
    \begin{align}
    \Lambda(W,H)\cap [0,U_i)_{i\in H} = \biguplus_{\emptyset \neq I \subseteq H} \Lambda\left (W,I\right) \cap \left [0,\sum_{j\in H}w_{ij}\right)_{i\in H} \cap \left [\sum_{l\in H} w_{k l}, U_k \right)_{k \in H\setminus I} \text{. }     \label{eq:recursive-decomposition}
    \end{align}
    #+END_thm

    #+BEGIN_proof
    \begin{align*}
    \Lambda(W,H) \cap [0,U_i)_{i\in H} &= \biguplus_{\emptyset \neq I \subseteq H}R(W,H,I) \cap \Lambda(W,H) \cap [0,U_i)_{i\in H} \\
    &= \biguplus_{\emptyset \neq I \subseteq H} R(W,H,I) \cap \Lambda(W,I)\cap [0,U_i)_{i\in H} \\
    % &= \biguplus_{\emptyset \neq I \subseteq H} \left [0, \sum_{j \in H}w_{i j}\right )_{i \in I} \cap \Lambda(W,U,I) \cap [0,U_i)_{i\in H} \cap\left [\sum_{l\in H} w_{k l},1 \right)_{k \in H\setminus I}   \\
    &= \biguplus_{\emptyset \neq I \subseteq H} \Lambda\left (W,I\right)\cap \left [0, \sum_{j \in H}w_{i j}\right )_{i \in I} \cap \left [\sum_{l\in H} w_{k l}, U_k \right)_{k \in H\setminus I} \text{ ,}
    \end{align*}
    where we used Lemma [[ref:lem:one]] for the second step and
    [[eqref:def:R]] as well as \(\sum_{j\in H}w_{ij} \leq U_i \leq 1\) in
    the last step.
    #+END_proof
    

    Theorem [[ref:thm:rec-dec]] provides the direct generalization of
    [[cite:eurich2002finite][Equation B5]] for non-negative weight
    matrices.

*** Volume of the non-inhabited region

    With equation [[ref:eq:recursive-decomposition]] one can directly
    compute the volume of the non-inhabited region generated by \(H\)
    by recursively computing all volumes of non-inhabited generated by
    all nonempty subsets of \(H\). However with this formula it is not
    clear in what way the volume of the non-inhabited region depends
    on properties of \(W\). However, it turns out that this volume
    does depend in a very structured way on the weight matrix: It is
    an alternating sum of determinants of submatrices:\\

    #+begin_thm
    \label{thm:volume} Under the same assumptions as in [[ref:thm:rec-dec]] we have 
    \begin{align}\mathcal{V}(\Lambda(W,H) \cap [0,U_i)_{i\in H}) = \sum_{\emptyset
    \neq I \subseteq H}(-1)^{|I|+1}|W_{I}|\prod_{j \in
    H\setminus I}U_j \label{eq:determinant-formula} \text{ .}\end{align}
    #+end_thm

    #+begin_proof
    We give a proof by induction on the cardinality \(|H| \).\\

    *Induction Start*:\\
    For \(H = \{i\} \) we have \[\Lambda(W,\{i\}) \cap [0,U_i)_{\{i\}}
    = \Gamma(W,\{i\}) \cap [0,U_i)_{\{i\}} = [0,w_{ii})_{\{i\}} \cap
    [0,U_i)_{\{i\}} = [0,w_{ii})_{\{i\}} \text{, }\] and thus
    \(\operatorname{Vol}(\Lambda(W,\{i\}) \cap [0,U_i)_{\{i\}} ) =
    w_{ii}\), which is equal to the right hand side of
    [[eqref:eq:determinant-formula]].\\

    *Inductive step*:\\
    Let [[eqref:eq:determinant-formula]] hold for all \(\emptyset \neq I
    \subseteq \mathcal{N}\) with cardinality \(|I| \leq n\). We show
    that it also holds for all \(H \subseteq \mathcal{N}\) of
    cardinality \(|H| = n+1\).\\
    
    We have \(R(W,H,H) = \Gamma(W,H)\) and thus \(R(W,H,H) \cap
    \Lambda(W,H) = R(W,H,H)\). Using this and inserting the induction
    hypothesis in each lower dimensional \(\Lambda(W,I) \cap
    [0,U_i)_{i\in I}\) with \(I \subsetneq H \) occuring in
    \eqref{eq:recursive-decomposition} leads to
    \begin{align}
\mathcal{V}{\left(\Lambda(W,H)\right)} = &\prod_{k\in H}\sum_{l\in H}w_{k,l} \nonumber \\ 
    &+ \underbrace{\sum_{\emptyset \neq I \subsetneq H} \left (\sum_{\emptyset
    \neq J \subseteq I}(-1)^{|J|+1}|W_{J}|\prod_{k \in
    \mathcal{I}\setminus J}\sum_{l\in H}w_{k,l} \right ) \prod_{k \in H\setminus I}\left( U_k - \sum_{l\in H} w_{k l}  \right )}_{\text{Ind}} \text{ .}
    \label{eq:ind}
   \end{align}

    We expand the term \(\text{Ind}\) into a linear combination of terms 
    \[F_{I' ,J',K'} = (-1)^{|J'|-1}|W_{J'}|\prod_{i\in I' }\sum_{j\in H}w_{i,j}\prod_{k\in K'}U_k\]
    for \(I'  \uplus J' \uplus K'  = H,J'\subsetneq H\):

    \[\text{Ind} = \sum_{I'  \uplus J' \uplus K'  = H,J'\subsetneq H}\operatorname{C_F}(I',J',K')(-1)^{|J'|-1}|W_{J' ,J' }|\prod_{i\in I' }\sum_{j\in H}w_{i,j}\prod_{k\in K'}U_k\]
    
    In order to calculate \(\operatorname{C_F}(I',J' ,K')\) we use
    that \(F_{I',J',K'}\) only occurs in [[eqref:eq:ind]] for the
    assigments \(J = J'\) and \(I' = J' \uplus I\) and perform a
    case analysis:
    - \(I' = \emptyset, K' = H\setminus J' \):
      \(\operatorname{C_F}(\emptyset,J' ,H\setminus J') = 1\), since
      with \(I' = \emptyset\) there is only one assigment \(I=J=J'\)
      in [[eqref:eq:ind]] resulting in \(F_{\emptyset,J' ,H\setminus J'} \).
    - \(K' = \emptyset, I' = H\setminus J'\):
      \(\operatorname{C_F}(N\setminus J,J' ,\emptyset) = -1\). The
      \(\prod_{i\in I'}\sum_{j\in H}w_{i,j}\) terms can originate
      from any \(J=J',J' \subseteq I \subsetneq H\) combined
      with the \(\prod_{k\in H\setminus I}-\sum_{l\in
      H}w_{k,l} \) term. The sign of this term is given by
      \((-1)^{|H\setminus I|} \). Counting the possible ways
      to chose \(i' \) elements from \(I'\) and the signs of the
      resulting combinations we arrive at 
    \[\operatorname{C_F}(N\setminus J,J' ,\emptyset) = \sum_{i'=1}^{|I'|} (-1)^{|I'| - i'}{|I'| \choose i'} = -1 \text{ .}\]
    - \(I' \neq \emptyset , K' \neq \emptyset\):
      \(\operatorname{C_F}(I',J',K') = 0\). As in the previous case,
      the \(\prod_{i\in I'}\sum_{j\in H}w_{i,j}\) terms can
      be formed from any completion \(J' \subseteq I \subseteq J'
      \uplus K'\), with the difference that now also \(I = I'\) can be chosen. 
      This results in 
      \[\operatorname{C_F}(I',J' ,K') = \sum_{i'=0}^{|I'|} (-1)^{|I'| - i'}{|I'| \choose i'} = 0 \text{ .}\]
      
    Setting this into [[eqref:eq:ind]] gives
      \begin{align*}
      \mathcal{V}{\left(\Lambda(W,H)\right)} = &\sum_{\emptyset \neq I \subsetneq H}(-1)^{|I|-1} |W_{I,I}|\prod_{k\in H\setminus I}U_k \\&+ \underbrace{\sum_{\emptyset \neq J \subsetneq H}(-1)^{|J|}|W_{J,J}|\prod_{k\in H\setminus J}\sum_{l\in H}w_{k,l} + \prod_{m\in H}\sum_{n\in H}w_{m,n}}_{T} \text{ .}\\
      \end{align*}
      The left term alone contains all lower dimensional determinant
      terms needed in [[eqref:eq:determinant-formula]]. 
      We complete the proof by showing that 
      
      \begin{align}\label{eq:toshow}T = (-1)^{|H|+1}|W_{H}| \text{ .}\end{align}

      The expansion of the third term contains all \(|H|^{|H|} \)
      products of matrix elements \(\prod_{i\in H}w_{i,\sigma(i)}\)
      where \(\sigma(i)\) is a function from \(H\) to \(H \), while \((-1)^{|H|+1}|W_{H} \)
      can be expanded using the Leibniz formula
      
      \[(-1)^{|H|+1}|W_{H}| = \sum_{\varphi}(-1)^{|H|-1}\operatorname{sign}(\varphi) \prod_{i\in H}w_{i,\varphi(i)} \text{, }\]
      
      where the sum is performed over all permutations of \(H \).

      To show the equality in [[eqref:eq:toshow]] we again expand \(T\)
      into a linear combination of the terms \(T_{\sigma} =
      \prod_{i\in H}w_{i,\sigma(i)}\) for the \(|H|^{|H|}\) functions
      \(\sigma:H \longrightarrow H\) with coefficients \(C_{\sigma}\)

      \[T = \sum_{\sigma}C_{\sigma}\prod_{i\in H}w_{i,\sigma(i)}\text{ .} \]

      The term 

      \[(-1)^{|J|}|W_{J}|\prod_{k\in H\setminus J}\sum_{l\in H}w_{k,l} = \sum_{\sigma}C_{\sigma}^J\prod_{i\in H}w_{i,\sigma(i)}\]

      contains just the terms corresponding to functions \(\sigma \) for
      which \(\sigma_{|_{J}}\) is a permutation, so we have 

      \[C_{\sigma}^J = \delta[\sigma_{|_J} \text{ is permutation of } J](-1)^{|J|+\operatorname{sign}(\sigma_{|_J})} \]

      and \(C_{\sigma} = \sum_{\emptyset \neq J \subsetneq H}C_{\sigma}^J + 1\).

      In order to calculate \(C_{\sigma}\) for a given function
      \(\sigma\), we introduce the set 
      
      \begin{align*}
      Z^{\sigma} = \{Z \subset H |&\sigma_{|_Z}\text{ is a permutation of } Z \\
      &\text{and there is no } Z' \subset Z \text{ such that}\\
      &\sigma_{|_{Z'}} \text{ is a permutation of } Z'\} \text{ .}
      \end{align*}

      For a permutation, this is just the set of cycles and more
      generally when viewing a functin \(\sigma \) as markov chain,
      \(Z^{\sigma}\) are its transitive components.

      For a subset \(\mathcal{Z} \) of \(Z^{\sigma}\) we introduce
      \(\mathcal{U}(\mathcal{Z}) = \biguplus_{Z_i\in \mathcal{Z}}Z_i\).
      We now have that \(\sigma_{|_J}\) is a permutation of \(J\) if
      and only if \(J = U(\mathcal{Z}) \) for a \(\emptyset \neq
      \mathcal{Z} \subseteq Z^{\sigma}\).
      We have \(\mathcal{U}(Z^{\sigma}) = H\) if and only if \(\sigma\) is
      a permutation of \(H\).

      We now have for \(\emptyset \neq \mathcal{U}(\mathcal{Z}) \subsetneq H\)
      \begin{align*}
      C_{\sigma}^{\mathcal{U}(\mathcal{Z})} &= (-1)^{|\mathcal{U}(\mathcal{Z})|}\operatorname{sign}(\sigma_{|_{\mathcal{U}(\mathcal{Z})}})\\
       &= (-1)^{|\mathcal{U}(\mathcal{Z})|+\sum_{Z\in \mathcal{Z}}(|Z|-1)} \\
       &= (-1)^{2|\mathcal{U}(\mathcal{Z}) -|\mathcal{Z}|} = (-1)^{|\mathcal{Z}|} \text{ ,}
       \end{align*}

       and

       \begin{align*}
       C_{\sigma} &= 1 + \sum_{\emptyset \neq J \subsetneq H}C_{\sigma}^J  = 1 + \sum_{\emptyset \neq \mathcal{Z} \subseteq Z^{\sigma},\mathcal{U}(\mathcal{Z}) \subsetneq H}C_{\sigma}^{\mathcal{U}(\mathcal{Z})} \\
                 &= 1 + \sum_{\emptyset \neq \mathcal{Z} \subseteq Z^{\sigma},\mathcal{U}(\mathcal{Z}) \subsetneq H}(-1)^{|\mathcal{Z}|} 
                 = \sum_{z=0}^{|Z^{\sigma}|-1} {|Z^{\sigma}| \choose z}(-1)^{z} + \delta[\mathcal{U}(Z^{\sigma}) \subsetneq H](-1)^{|Z^{\sigma}|} \\
                 &= \begin{cases} (-1)^{|Z^{\sigma}|-1} &\mbox{ if } \sigma \text{ is permutation of } H \\
                                  0 &\mbox{ otherwise}\text{ .}\end{cases} 
       \end{align*}

       This completes the proof since for a permutation \(\varphi \)
       of \(H\) \((-1)^{|H|-1}\operatorname{sign}(\phi) = (-1)^{|H| -
       1 + |H| - |Z^{\varphi}|} = (-1)^{|Z^{\varphi}| - 1}\) and by comparing the coefficients we have
       \[T = \sum_{\sigma}C_{\sigma}\prod_{i\in H}w_{i,\sigma(i)} = (-1)^{|H|+1}|W_{H}| \text{ .}\]
    #+end_proof


    
*** Recursive calculation of noninhabitated region           :implementation:
    
    
  The non-inhabited region is a union of certain hyperrectangles.
  Therefore it's volume can be directly calculated by calculating the
  volume of the union of these hyperrectangles.

    #+begin_src ipython :session master
      from functools import reduce
      import itertools

      def spiking_configurations(iterable):
          "powerset([1,2,3]) --> reverse of (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)"
          s = list(iterable)
          return chain.from_iterable(combinations(s, r) for r in range(len(s),0,-1))
        
      def intersection(hyperr1,hyperr2):
          """compute intersection of two hyperrectangles given by list of  end values along each axes.
             So the rectangles always have their lower values at 0"""
          if len(hyperr1) == 0 or len(hyperr2) == 0:
              return []
          intersection  = []
          for (u1l,u1u),(u2l,u2u) in zip(hyperr1,hyperr2):
              il,iu = (max(u1l,u2l),min(u1u,u2u))
              if iu <= il:
                  return []
              intersection.append((il,iu))
          return intersection

      def volume(hyperr):
          if len(hyperr) == 0:
              return 0
          return prod(hu - hl for hu,hl in hyperr)

      def volume_union(hyperrectangles):
          # uses approach from https://stackoverflow.com/questions/28146260
          # TODO use best known algorithm for Klee's measure problem
          # https://pdfs.semanticscholar.org/9528/ef345d013577fd6c9d7fde8a54a1b86d3bbe.pdf
          dim = len(hyperrectangles[0])
          gridlines = [np.unique([hyperr[i][0] for hyperr in hyperrectangles] +
                                 [hyperr[i][1] for hyperr in hyperrectangles]) for i in range(dim)]
          grid = np.zeros([len(gl)-1 for gl in gridlines],dtype=bool) #TODO Better to create grid on the fly to save space?
          #fill the grid
          for hyperr in hyperrectangles:
              grid_slices = tuple(slice(np.searchsorted(gridlines[i],hyperr[i][0]),np.searchsorted(gridlines[i],hyperr[i][1]))
                                  for i in range(dim))
              grid[grid_slices] = True #indicate grid cells covered by hyperr
          # calculate total volume from grid and gridlines
          it = np.nditer(grid, flags=['multi_index'])
          volume = 0
          while not it.finished:
              if(it[0]): # for each filled cell compute it's volume and add it
                  volume += prod(grid[i+1] - grid[i] for grid,i in zip(gridlines,it.multi_index))
              it.iternext()
          return volume

      def R_sp(W,spiking_configuration,num_dims):
          """returns excluded hyper rectangle induced by the spiking_configuration"""
          ind_sp = np.zeros(num_dims,dtype=int)
          for s in spiking_configuration:
              ind_sp[s] = 1
          limits = (W @ ind_sp) # Don't rescale here
          sp_comp = np.ones_like(ind_sp) - ind_sp # 1 at nodes that are not in sp and 0 otherwise
          return [(0,x) for x in np.maximum(limits,sp_comp)]

      def noninhabited_region(units,W,N=None):
          if N is None:
            N = len(units)
          return volume_union([R_sp(W,sp,N) for sp in spiking_configurations(units)])

    #+end_src 

    #+RESULTS:


        Instead of using computational geometry algorithms to compute
    the volume of an union of hyperrectangles, the non-inhabited region
    can be recursively decomposed into nonoverlapping regions, so that
    the volume can be directly calculated.
        #+begin_src ipython :session master
          from itertools import chain, combinations

          def prod(gen):
              res = 1;
              for i in gen:
                  res *= i
              return res

          def nonempty_subsets(iterable):
              "nonempty_subsets([1,2,3]) -->  (1,) (2,) (3,) (1,2) (1,3) (2,3)"
              s = list(iterable)
              return chain.from_iterable(combinations(s, r) for r in range(1,len(s)))

          class NoninhabitedRegion():
              def __init__(self,W):
                  self.results = {}
                  self.W = W;

              def noninhabited_region(self,N=None,U=None):
                  W = self.W
                  if N is None:
                      N = range(W.shape[0])
                  elif type(N) == int:
                      N = range(N)
                  N = tuple(N)
                  if U is None:
                      U = np.ones(len(N))
                  U = np.array(U)
                  res = self.results.get((N,tuple(U)))
                  if res is not None:
                      res
                  row_sums = np.array([sum(W[i,j] for j in N) for i in N]) # works with sympy
                  res = prod(row_sums)#Gamma with I=N
                  for I in nonempty_subsets(N):
                      I_inds = np.in1d(N,I)
                      res += self.noninhabited_region(I,U=row_sums[I_inds]) * prod(U[~I_inds]-row_sums[~I_inds])
                  self.results[(N,tuple(U))] = res
                  return res
        #+end_src 

        #+RESULTS:


     #+begin_src ipython :session master :results output :tangle no
       N = 5;W = np.ones((N,N))/N;NR = NoninhabitedRegion(W)
       print(NR.noninhabited_region())
       print(noninhabited_region(list(range(N)),W))
     #+end_src 

    #+RESULTS:
    : 1.0
    : 1.0

    Symbolic calculations can be done using sympy.

    First check that with a constant matrix \(W_{i,j} = a \), the volume of the N dimensional non-inhabited region is \(Na \).
    #+begin_src ipython :session master :tangle no
      import sympy as sym 
      N = 5
      W = np.empty((N,N),dtype=object)
      W[:,:] = sym.symbols('a')
      NR = NoninhabitedRegion(W)

      sym.simplify(NR.noninhabited_region())

    #+end_src 

    #+RESULTS:
    : 5.0*a
    
:exploration:
    Also the general algebraic formulas can be calculated with sympy
    #+begin_src ipython :session master
      import sympy as sym
      def symbolic_matrix(N):
           W = np.empty((N,N),dtype=object);
           for i in range(N):
               for j in range(N):
                   W[i,j] = sym.symbols('w'+str(i)+str(j))
           return W
    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master :eval no :tangle no
      N = 5
      W = symbolic_matrix(N)
      NR = NoninhabitedRegion(W)
      sym.simplify(NR.noninhabited_region())
    #+end_src 


    #+begin_src ipython :session master :eval no :tangle no
    1.0*w00*w11*w22*w33*w44 - 1.0*w00*w11*w22*w33 - 1.0*w00*w11*w22*w34*w43 - 1.0*w00*w11*w22*w44 + 1.0*w00*w11*w22 - 1.0*w00*w11*w23*w32*w44 + 1.0*w00*w11*w23*w32 + 1.0*w00*w11*w23*w34*w42 + 1.0*w00*w11*w24*w32*w43 - 1.0*w00*w11*w24*w33*w42 + 1.0*w00*w11*w24*w42 - 1.0*w00*w11*w33*w44 + 1.0*w00*w11*w33 + 1.0*w00*w11*w34*w43 + 1.0*w00*w11*w44 - 1.0*w00*w11 - 1.0*w00*w12*w21*w33*w44 + 1.0*w00*w12*w21*w33 + 1.0*w00*w12*w21*w34*w43 + 1.0*w00*w12*w21*w44 - 1.0*w00*w12*w21 + 1.0*w00*w12*w23*w31*w44 - 1.0*w00*w12*w23*w31 - 1.0*w00*w12*w23*w34*w41 - 1.0*w00*w12*w24*w31*w43 + 1.0*w00*w12*w24*w33*w41 - 1.0*w00*w12*w24*w41 + 1.0*w00*w13*w21*w32*w44 - 1.0*w00*w13*w21*w32 - 1.0*w00*w13*w21*w34*w42 - 1.0*w00*w13*w22*w31*w44 + 1.0*w00*w13*w22*w31 + 1.0*w00*w13*w22*w34*w41 + 1.0*w00*w13*w24*w31*w42 - 1.0*w00*w13*w24*w32*w41 + 1.0*w00*w13*w31*w44 - 1.0*w00*w13*w31 - 1.0*w00*w13*w34*w41 - 1.0*w00*w14*w21*w32*w43 + 1.0*w00*w14*w21*w33*w42 - 1.0*w00*w14*w21*w42 + 1.0*w00*w14*w22*w31*w43 - 1.0*w00*w14*w22*w33*w41 + 1.0*w00*w14*w22*w41 - 1.0*w00*w14*w23*w31*w42 + 1.0*w00*w14*w23*w32*w41 - 1.0*w00*w14*w31*w43 + 1.0*w00*w14*w33*w41 - 1.0*w00*w14*w41 - 1.0*w00*w22*w33*w44 + 1.0*w00*w22*w33 + 1.0*w00*w22*w34*w43 + 1.0*w00*w22*w44 - 1.0*w00*w22 + 1.0*w00*w23*w32*w44 - 1.0*w00*w23*w32 - 1.0*w00*w23*w34*w42 - 1.0*w00*w24*w32*w43 + 1.0*w00*w24*w33*w42 - 1.0*w00*w24*w42 + 1.0*w00*w33*w44 - 1.0*w00*w33 - 1.0*w00*w34*w43 - 1.0*w00*w44 + 1.0*w00 - 1.0*w01*w10*w22*w33*w44 + 1.0*w01*w10*w22*w33 + 1.0*w01*w10*w22*w34*w43 + 1.0*w01*w10*w22*w44 - 1.0*w01*w10*w22 + 1.0*w01*w10*w23*w32*w44 - 1.0*w01*w10*w23*w32 - 1.0*w01*w10*w23*w34*w42 - 1.0*w01*w10*w24*w32*w43 + 1.0*w01*w10*w24*w33*w42 - 1.0*w01*w10*w24*w42 + 1.0*w01*w10*w33*w44 - 1.0*w01*w10*w33 - 1.0*w01*w10*w34*w43 - 1.0*w01*w10*w44 + 1.0*w01*w10 + 1.0*w01*w12*w20*w33*w44 - 1.0*w01*w12*w20*w33 - 1.0*w01*w12*w20*w34*w43 - 1.0*w01*w12*w20*w44 + 1.0*w01*w12*w20 - 1.0*w01*w12*w23*w30*w44 + 1.0*w01*w12*w23*w30 + 1.0*w01*w12*w23*w34*w40 + 1.0*w01*w12*w24*w30*w43 - 1.0*w01*w12*w24*w33*w40 + 1.0*w01*w12*w24*w40 - 1.0*w01*w13*w20*w32*w44 + 1.0*w01*w13*w20*w32 + 1.0*w01*w13*w20*w34*w42 + 1.0*w01*w13*w22*w30*w44 - 1.0*w01*w13*w22*w30 - 1.0*w01*w13*w22*w34*w40 - 1.0*w01*w13*w24*w30*w42 + 1.0*w01*w13*w24*w32*w40 - 1.0*w01*w13*w30*w44 + 1.0*w01*w13*w30 + 1.0*w01*w13*w34*w40 + 1.0*w01*w14*w20*w32*w43 - 1.0*w01*w14*w20*w33*w42 + 1.0*w01*w14*w20*w42 - 1.0*w01*w14*w22*w30*w43 + 1.0*w01*w14*w22*w33*w40 - 1.0*w01*w14*w22*w40 + 1.0*w01*w14*w23*w30*w42 - 1.0*w01*w14*w23*w32*w40 + 1.0*w01*w14*w30*w43 - 1.0*w01*w14*w33*w40 + 1.0*w01*w14*w40 + 1.0*w02*w10*w21*w33*w44 - 1.0*w02*w10*w21*w33 - 1.0*w02*w10*w21*w34*w43 - 1.0*w02*w10*w21*w44 + 1.0*w02*w10*w21 - 1.0*w02*w10*w23*w31*w44 + 1.0*w02*w10*w23*w31 + 1.0*w02*w10*w23*w34*w41 + 1.0*w02*w10*w24*w31*w43 - 1.0*w02*w10*w24*w33*w41 + 1.0*w02*w10*w24*w41 - 1.0*w02*w11*w20*w33*w44 + 1.0*w02*w11*w20*w33 + 1.0*w02*w11*w20*w34*w43 + 1.0*w02*w11*w20*w44 - 1.0*w02*w11*w20 + 1.0*w02*w11*w23*w30*w44 - 1.0*w02*w11*w23*w30 - 1.0*w02*w11*w23*w34*w40 - 1.0*w02*w11*w24*w30*w43 + 1.0*w02*w11*w24*w33*w40 - 1.0*w02*w11*w24*w40 + 1.0*w02*w13*w20*w31*w44 - 1.0*w02*w13*w20*w31 - 1.0*w02*w13*w20*w34*w41 - 1.0*w02*w13*w21*w30*w44 + 1.0*w02*w13*w21*w30 + 1.0*w02*w13*w21*w34*w40 + 1.0*w02*w13*w24*w30*w41 - 1.0*w02*w13*w24*w31*w40 - 1.0*w02*w14*w20*w31*w43 + 1.0*w02*w14*w20*w33*w41 - 1.0*w02*w14*w20*w41 + 1.0*w02*w14*w21*w30*w43 - 1.0*w02*w14*w21*w33*w40 + 1.0*w02*w14*w21*w40 - 1.0*w02*w14*w23*w30*w41 + 1.0*w02*w14*w23*w31*w40 + 1.0*w02*w20*w33*w44 - 1.0*w02*w20*w33 - 1.0*w02*w20*w34*w43 - 1.0*w02*w20*w44 + 1.0*w02*w20 - 1.0*w02*w23*w30*w44 + 1.0*w02*w23*w30 + 1.0*w02*w23*w34*w40 + 1.0*w02*w24*w30*w43 - 1.0*w02*w24*w33*w40 + 1.0*w02*w24*w40 - 1.0*w03*w10*w21*w32*w44 + 1.0*w03*w10*w21*w32 + 1.0*w03*w10*w21*w34*w42 + 1.0*w03*w10*w22*w31*w44 - 1.0*w03*w10*w22*w31 - 1.0*w03*w10*w22*w34*w41 - 1.0*w03*w10*w24*w31*w42 + 1.0*w03*w10*w24*w32*w41 - 1.0*w03*w10*w31*w44 + 1.0*w03*w10*w31 + 1.0*w03*w10*w34*w41 + 1.0*w03*w11*w20*w32*w44 - 1.0*w03*w11*w20*w32 - 1.0*w03*w11*w20*w34*w42 - 1.0*w03*w11*w22*w30*w44 + 1.0*w03*w11*w22*w30 + 1.0*w03*w11*w22*w34*w40 + 1.0*w03*w11*w24*w30*w42 - 1.0*w03*w11*w24*w32*w40 + 1.0*w03*w11*w30*w44 - 1.0*w03*w11*w30 - 1.0*w03*w11*w34*w40 - 1.0*w03*w12*w20*w31*w44 + 1.0*w03*w12*w20*w31 + 1.0*w03*w12*w20*w34*w41 + 1.0*w03*w12*w21*w30*w44 - 1.0*w03*w12*w21*w30 - 1.0*w03*w12*w21*w34*w40 - 1.0*w03*w12*w24*w30*w41 + 1.0*w03*w12*w24*w31*w40 + 1.0*w03*w14*w20*w31*w42 - 1.0*w03*w14*w20*w32*w41 - 1.0*w03*w14*w21*w30*w42 + 1.0*w03*w14*w21*w32*w40 + 1.0*w03*w14*w22*w30*w41 - 1.0*w03*w14*w22*w31*w40 - 1.0*w03*w14*w30*w41 + 1.0*w03*w14*w31*w40 - 1.0*w03*w20*w32*w44 + 1.0*w03*w20*w32 + 1.0*w03*w20*w34*w42 + 1.0*w03*w22*w30*w44 - 1.0*w03*w22*w30 - 1.0*w03*w22*w34*w40 - 1.0*w03*w24*w30*w42 + 1.0*w03*w24*w32*w40 - 1.0*w03*w30*w44 + 1.0*w03*w30 + 1.0*w03*w34*w40 + 1.0*w04*w10*w21*w32*w43 - 1.0*w04*w10*w21*w33*w42 + 1.0*w04*w10*w21*w42 - 1.0*w04*w10*w22*w31*w43 + 1.0*w04*w10*w22*w33*w41 - 1.0*w04*w10*w22*w41 + 1.0*w04*w10*w23*w31*w42 - 1.0*w04*w10*w23*w32*w41 + 1.0*w04*w10*w31*w43 - 1.0*w04*w10*w33*w41 + 1.0*w04*w10*w41 - 1.0*w04*w11*w20*w32*w43 + 1.0*w04*w11*w20*w33*w42 - 1.0*w04*w11*w20*w42 + 1.0*w04*w11*w22*w30*w43 - 1.0*w04*w11*w22*w33*w40 + 1.0*w04*w11*w22*w40 - 1.0*w04*w11*w23*w30*w42 + 1.0*w04*w11*w23*w32*w40 - 1.0*w04*w11*w30*w43 + 1.0*w04*w11*w33*w40 - 1.0*w04*w11*w40 + 1.0*w04*w12*w20*w31*w43 - 1.0*w04*w12*w20*w33*w41 + 1.0*w04*w12*w20*w41 - 1.0*w04*w12*w21*w30*w43 + 1.0*w04*w12*w21*w33*w40 - 1.0*w04*w12*w21*w40 + 1.0*w04*w12*w23*w30*w41 - 1.0*w04*w12*w23*w31*w40 - 1.0*w04*w13*w20*w31*w42 + 1.0*w04*w13*w20*w32*w41 + 1.0*w04*w13*w21*w30*w42 - 1.0*w04*w13*w21*w32*w40 - 1.0*w04*w13*w22*w30*w41 + 1.0*w04*w13*w22*w31*w40 + 1.0*w04*w13*w30*w41 - 1.0*w04*w13*w31*w40 + 1.0*w04*w20*w32*w43 - 1.0*w04*w20*w33*w42 + 1.0*w04*w20*w42 - 1.0*w04*w22*w30*w43 + 1.0*w04*w22*w33*w40 - 1.0*w04*w22*w40 + 1.0*w04*w23*w30*w42 - 1.0*w04*w23*w32*w40 + 1.0*w04*w30*w43 - 1.0*w04*w33*w40 + 1.0*w04*w40 - 1.0*w11*w22*w33*w44 + 1.0*w11*w22*w33 + 1.0*w11*w22*w34*w43 + 1.0*w11*w22*w44 - 1.0*w11*w22 + 1.0*w11*w23*w32*w44 - 1.0*w11*w23*w32 - 1.0*w11*w23*w34*w42 - 1.0*w11*w24*w32*w43 + 1.0*w11*w24*w33*w42 - 1.0*w11*w24*w42 + 1.0*w11*w33*w44 - 1.0*w11*w33 - 1.0*w11*w34*w43 - 1.0*w11*w44 + 1.0*w11 + 1.0*w12*w21*w33*w44 - 1.0*w12*w21*w33 - 1.0*w12*w21*w34*w43 - 1.0*w12*w21*w44 + 1.0*w12*w21 - 1.0*w12*w23*w31*w44 + 1.0*w12*w23*w31 + 1.0*w12*w23*w34*w41 + 1.0*w12*w24*w31*w43 - 1.0*w12*w24*w33*w41 + 1.0*w12*w24*w41 - 1.0*w13*w21*w32*w44 + 1.0*w13*w21*w32 + 1.0*w13*w21*w34*w42 + 1.0*w13*w22*w31*w44 - 1.0*w13*w22*w31 - 1.0*w13*w22*w34*w41 - 1.0*w13*w24*w31*w42 + 1.0*w13*w24*w32*w41 - 1.0*w13*w31*w44 + 1.0*w13*w31 + 1.0*w13*w34*w41 + 1.0*w14*w21*w32*w43 - 1.0*w14*w21*w33*w42 + 1.0*w14*w21*w42 - 1.0*w14*w22*w31*w43 + 1.0*w14*w22*w33*w41 - 1.0*w14*w22*w41 + 1.0*w14*w23*w31*w42 - 1.0*w14*w23*w32*w41 + 1.0*w14*w31*w43 - 1.0*w14*w33*w41 + 1.0*w14*w41 + 1.0*w22*w33*w44 - 1.0*w22*w33 - 1.0*w22*w34*w43 - 1.0*w22*w44 + 1.0*w22 - 1.0*w23*w32*w44 + 1.0*w23*w32 + 1.0*w23*w34*w42 + 1.0*w24*w32*w43 - 1.0*w24*w33*w42 + 1.0*w24*w42 - 1.0*w33*w44 + 1.0*w33 + 1.0*w34*w43 + 1.0*w44
       #+end_src 



    #+begin_src ipython :session master :results output :tangle no
      for N in range(1,4):
          W = symbolic_matrix(N)
          NR = NoninhabitedRegion(W)
          U = [sym.symbols('U')]*N
          print(N,NR.noninhabited_region(U=U))
    #+end_src 

    #+RESULTS:
    : 1 w00
    : 2 w00*(U - w10 - w11) + w11*(U - w00 - w01) + (w00 + w01)*(w10 + w11)
    : 3 w00*(U - w10 - w11 - w12)*(U - w20 - w21 - w22) + w11*(U - w00 - w01 - w02)*(U - w20 - w21 - w22) + w22*(U - w00 - w01 - w02)*(U - w10 - w11 - w12) + (w00 + w01 + w02)*(w10 + w11 + w12)*(w20 + w21 + w22) + (w00*w12 + w02*w11 + (w00 + w01)*(w10 + w11))*(U - w20 - w21 - w22) + (w00*w21 + w01*w22 + (w00 + w02)*(w20 + w22))*(U - w10 - w11 - w12) + (w10*w22 + w11*w20 + (w11 + w12)*(w21 + w22))*(U - w00 - w01 - w02)

    #+begin_src ipython :session master :eval no :tangle no
     1 w00
     2 -1.0*w00*w11 + 1.0*w00 + 1.0*w01*w10 + 1.0*w11
     3 1.0*w00*w11*w22 - 1.0*w00*w11 - 1.0*w00*w12*w21 - 1.0*w00*w22 + 1.0*w00 - 1.0*w01*w10*w22 + 1.0*w01*w10 + 1.0*w01*w12*w20 + 1.0*w02*w10*w21 - 1.0*w02*w11*w20 + 1.0*w02*w20 - 1.0*w11*w22 + 1.0*w11 + 1.0*w12*w21 + 1.0*w22
     4 -1.0*w00*w11*w22*w33 + 1.0*w00*w11*w22 + 1.0*w00*w11*w23*w32 + 1.0*w00*w11*w33 - 1.0*w00*w11 + 1.0*w00*w12*w21*w33 - 1.0*w00*w12*w21 - 1.0*w00*w12*w23*w31 - 1.0*w00*w13*w21*w32 + 1.0*w00*w13*w22*w31 - 1.0*w00*w13*w31 + 1.0*w00*w22*w33 - 1.0*w00*w22 - 1.0*w00*w23*w32 - 1.0*w00*w33 + 1.0*w00 + 1.0*w01*w10*w22*w33 - 1.0*w01*w10*w22 - 1.0*w01*w10*w23*w32 - 1.0*w01*w10*w33 + 1.0*w01*w10 - 1.0*w01*w12*w20*w33 + 1.0*w01*w12*w20 + 1.0*w01*w12*w23*w30 + 1.0*w01*w13*w20*w32 - 1.0*w01*w13*w22*w30 + 1.0*w01*w13*w30 - 1.0*w02*w10*w21*w33 + 1.0*w02*w10*w21 + 1.0*w02*w10*w23*w31 + 1.0*w02*w11*w20*w33 - 1.0*w02*w11*w20 - 1.0*w02*w11*w23*w30 - 1.0*w02*w13*w20*w31 + 1.0*w02*w13*w21*w30 - 1.0*w02*w20*w33 + 1.0*w02*w20 + 1.0*w02*w23*w30 + 1.0*w03*w10*w21*w32 - 1.0*w03*w10*w22*w31 + 1.0*w03*w10*w31 - 1.0*w03*w11*w20*w32 + 1.0*w03*w11*w22*w30 - 1.0*w03*w11*w30 + 1.0*w03*w12*w20*w31 - 1.0*w03*w12*w21*w30 + 1.0*w03*w20*w32 - 1.0*w03*w22*w30 + 1.0*w03*w30 + 1.0*w11*w22*w33 - 1.0*w11*w22 - 1.0*w11*w23*w32 - 1.0*w11*w33 + 1.0*w11 - 1.0*w12*w21*w33 + 1.0*w12*w21 + 1.0*w12*w23*w31 + 1.0*w13*w21*w32 - 1.0*w13*w22*w31 + 1.0*w13*w31 - 1.0*w22*w33 + 1.0*w22 + 1.0*w23*w32 + 1.0*w33

    #+end_src 


    There seems to be a +spiking+ striking pattern in the noninhabited volume regions!!

    2D case: 
    1x1 Determinants
    \[w00 + w11\]
    - determinants 2x2
    \[ + w01*w10 -w00*w11\]

    3D case: 
    1x1 Det all combos
    \[w00 + w11 + w22\]

    minus Det 2x2 all combos: 
    minus 2x2 01
    \[+  w01*w10 - w00*w11\]
    minus Det 2x2 02
    \[+ w02*w20 - w00*w22\]
    minus Det 2x2 12
    \[- w11*w22 + w12*w21\]

    Det 3x3
    \[+ w00*w11*w22  + w01*w12*w20 + w02*w10*w21 - w00*w12*w21 - w01*w10*w22  - w02*w11*w20\]

    4D Case

    1x1 Det all combos
    \[+ w00 + w11 + w22 + w33\]

    minus Det 2x2 alle combinations
      \[- w00*w11  + w01*w10\]
      \[- w00*w22 + w02*w20\]
      \[- w00*w33   + w03*w30\]
      \[- w11*w22   + w12*w21\]
      \[- w11*w33 + w13*w31\]
      \[- w22*w33  + w23*w32 \]

    Det 3x3 all combos: 

    Det 3x3 012
    \[+ w00*w11*w22 - w00*w12*w21 - w01*w10*w22  + w01*w12*w20   + w02*w10*w21  - w02*w11*w20 \]
    Det 3x3 013
    \[+ w00*w11*w33 - w00*w13*w31  - w01*w10*w33 +  w01*w13*w30 + w03*w10*w31  - w03*w11*w30 \]
    Det 3x3 023
    \[+ w00*w22*w33  - w00*w23*w32  - w02*w20*w33 + w02*w23*w30  + w03*w20*w32 - w03*w22*w30\]
    Det 3x3 123
    \[+ w11*w22*w33 - w11*w23*w32   - w12*w21*w33 + w12*w23*w31 + w13*w21*w32 - w13*w22*w31\]

   (compare with http://www.wolframalpha.com/input/?i=det+%7B%7Bw_00,w_01,w_02,w_03%7D,%7Bw_10,w_11,w_12,w_13%7D,%7Bw_20,w_21,w_22,w_23%7D,%7Bw_30,w_31,w_32,w_33%7D%7D)
   
    minus Det 4x4
    \[+ w02*w11*w20*w33 - w01*w12*w20*w33 - w02*w10*w21*w33 + w00*w12*w21*w33 \]
    \[+ w01*w10*w22*w33 -w00*w11*w22*w33 + w03*w12*w20*w31 - w02*w13*w20*w31\]
    \[- w03*w11*w20*w32 + w01*w13*w20*w32 - w03*w12*w21*w30 + w02*w13*w21*w30\]
    \[+ w03*w11*w22*w30 - w01*w13*w22*w30 - w02*w11*w23*w30 + w01*w12*w23*w30\]
    \[- w03*w10*w22*w31 + w00*w13*w22*w31 + w02*w10*w23*w31 - w00*w12*w23*w31\]
    \[+ w03*w10*w21*w32 - w00*w13*w21*w32 - w01*w10*w23*w32 + w00*w11*w23*w32    \]


    Here is my conjecture - now a theorem -  about the volume of the noninhabited region: 

    \begin{align}
    \operatorname{Vol}(\Lambda^N(W,1)) = \sum_{\emptyset \neq I \subseteq N} (-1)^{|I|+1} |W_{I,I}| \text{ ,}
    \end{align}
    
    where \(W_{I,I} \) is the \(|I|\times |I| \) submatrix formed of
    the elements with indices in \(I \).

    For the case of a constant coupling matrix \(W_{i,j}= \alpha \), the volume of the original EHE Model is reproduced: 
    
    \begin{align}
    \operatorname{Vol}(\Lambda^N(W,1)) &= \sum_{\emptyset \neq I \subseteq N} (-1)^{|I|+1} |W_{I,I}| \\
                                       &= \sum_{I \in \{\{i\}, i\in N\}} W_{ii}  \\
                                       &= \operatorname{Trace}(W) = N\alpha
    \end{align}

    

    Test this hypothesis algebraically for small N 
    #+begin_src ipython :session master
      from sympy.combinatorics import Permutation
      def symbolic_det(W,I):
          ret = 0
          for perm_idx in itertools.permutations(range(len(I))):
              ret += Permutation(perm_idx).signature() * prod(W[i,I[j_idx]] for i,j_idx in zip(I,perm_idx))
          return ret

      def hypothesis_volume(W,N):
          if type(N) == int:
              N = range(N)
          return sum((-1 if len(I) % 2 == 0 else 1) * symbolic_det(W,I) for I in spiking_configurations(N))
    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master
      def test_conjecture(N=5):
          W = symbolic_matrix(N)
          NR = NoninhabitedRegion(W)
          Lambda = NR.noninhabited_region()
          print('Lambda',Lambda,flush=True)
          Lambda = sym.simplify(Lambda)
          Lambda_bar = hypothesis_volume(W,N)
          print('Lambda_bar',Lambda_bar,flush=True)
          Lambda_bar = sym.simplify(Lambda_bar)
          print(sym.simplify(Lambda - Lambda_bar))

    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master :eval no :tangle no
      qsub("test_conjecture(task_id)",name='test_conjecture',servername='server_inline',task_ids='7-8',queue='maximus')
    #+end_src 


    Test my proposed formula
    #+begin_src ipython :session master
      from sympy import Symbol, Rational, binomial, expand_func
      from sympy.utilities.iterables import partitions

      def gen_partitions(N):
          for p in partitions(N,m=N):
              yield reduce(lambda a,b:a+b,([k]*v for k,v in p.items()))


      def coef_i(p,i,N):
          return sum((-1)**(len(J) - 1)*binomial(N - sum(J),i - sum(J)) for J in (list(nonempty_subsets(p)) + [p]))

      def coef(p,N):
          return sum((-1)**(N-i)*coef_i(p,i,N) for i in range(1,N+1))


      def sign(p,N):
          if sum(p) != N:
              return 0;
          return (-1)**(sum(lz-1 for lz in p))

      def expected_sign(p,N): 
          return (-1)**(N-1)*sign(p,N)
    #+end_src 

    #+RESULTS:

    It works!

    #+begin_src ipython :session master :results output :tangle no
    N = 4
    for p in gen_partitions(N): 
        print('Partition ',p)
        print('Coef          ',coef(p,N))
        print('Expected sign ',expected_sign(p,N))
    #+end_src 

    #+RESULTS:
    #+begin_example
    Partition  [4]
    Coef           1
    Expected sign  1
    Partition  [1, 3]
    Coef           -1
    Expected sign  -1
    Partition  [2, 2]
    Coef           -1
    Expected sign  -1
    Partition  [1, 1, 2]
    Coef           1
    Expected sign  1
    Partition  [1, 1, 1, 1]
    Coef           -1
    Expected sign  -1
#+end_example


    And I can prove it!
:END:

   

** Identification of regions leading to avalanches \label{sec:regions-avalanches}
   
   We introduce the shorthand
   \begin{align}
   \label{eq:def-D}
   D := [0,1)^N \setminus \Lambda(W,\mathcal{N})
   \end{align}
   for the complement of the non-inhabited region.

    Before we proceed to show that that the complement of the
    non-inhabited region is invariant under T we identify the regions
    \(R(W,{(G_i)_{i=1,\ldots,D}}) \subseteq D\). For \(u \in
    R(W,{(G_i)_{i=1,\ldots,D}})\), with \(G_1 = \{a_1\}\) external
    input to unit \(u_{a_1}\) produces the avalanche
    \(\operatorname{av}(a_1,u) = (G_i)_{i=1,\ldots,D}\).

    The coordinate of unit \(u_{a_1}\), which receives the external
    input and starts the avalanche has to be in the interval
    \([1-\Delta U,1)\). The coordinates the units in \(G_j, 1 < j \leq
    D \) have to be such that the total internal input generated
    during the avalanche up to step \(j-1\) did not cause them to
    spike, but the additional internal input generated by step \(j-1\)
    is enough to push them above the threshold. For the coordinates
    along the axis not occuring in \(\operatorname{av}\), the
    condition is that the total input generated from the whole
    avalanche is not enough to make them spike and that they lie
    outside of the non-inhabited area.
    
    This reasoning specifies \(R(W,\operatorname{av})\),
    \(\operatorname{av} = (G_i)_{i=1,\ldots,D},G_1 = \{a_1 \}\) as

  \begin{align}
R(W,\operatorname{av}) &= [1 - \Delta U,1)_{\{a_1\}}\bigcap_{j=2}^D [1 - \operatorname{act}(\operatorname{av},k,j),1-\operatorname{act}(\operatorname{av},k,j-1))_{k \in G_j} \nonumber \\
    &\cap \left ([0,1-\operatorname{act}(\operatorname{av},l,D)]_{l\in \mathcal{N}\setminus \mathcal{U}(\operatorname{av})} \setminus \Lambda(W,\mathcal{N}\setminus \mathcal{U}(\operatorname{av})) \right)\label{eq:def-R-av} \text{ .}
    \end{align}



\\


    #+BEGIN_thm
    \label{thm:R_av}
    For \((G_i)_{1=1,\ldots,D} \in \Omega, 1 \leq D \leq N\) 
    \[R(W,(G_i)_{i=1,\ldots,D}) = \{u \in D|\operatorname{av}(u,a_1) = (G_i)_{i=1,\ldots,D},G_1=\{a_1\}\}\]
    #+END_thm

    #+BEGIN_proof
    We will list the conditions on \(u \in D\) so that it generates the nonempty avalanche
    \(\operatorname{av} = (G_i)_{i=1,\ldots,D}\) upon external activation of unit
    \(\{a_1\} = G_1\).

    Since \(G_1 = \{a_1\}\), it follows that
    \[A(E_{a_1}(u))_{a_1} = 1 \iff u_{a_1} + \Delta U \geq 1 \iff u_{a_1} \in [1-\Delta U,1) \text{ .}\]
    
    Using theorem [[ref:thm:at-most-once]], the condition that unit \(k\) spikes only in step \(j\) in
    the avalanche, \(k\in G_j\) reduces to  
    \(A(F^{j-1}\circ E_{a_1}(u)))_k = 1\), and we have
    \[(F^{j-1}\circ E_{a_1}(u))_k \geq 1 > (F^{j-2} \circ E_{a_1}(u))_k \text{ .}\]
    Performing the iterations of \(F\) using lemma [[ref:lem:av_action]] and observing
    that \(E_{a_1}(u)_k = u_k\), this condition reduces to
    \begin{align*}
    &u_k + \operatorname{act}(\operatorname{av},k,j)  \geq 1 > u_k + \operatorname{act}(\operatorname{av},k,j-1) \iff \\ &u_k \in [1 - \operatorname{act}(\operatorname{av},k,j),1-\operatorname{act}(\operatorname{av},k,j-1)) \text{ .}
    \end{align*}
    If unit \(l \in [0,1) \setminus \Lambda^{\mathcal{N}}(W,1)\) does
    not fire during \(\operatorname{av}\), \(l \notin
    \mathcal{U}(\operatorname{av})\), then \(A(F^{i-1} \circ
    E_{a_1}(u))_l = 0\) for all \(i=1,\ldots,D\). This restricts each
    \(u_l\) to the interval \(u_l \in [0,1 -
    \operatorname{act}(\operatorname{av},l,D))\).

    We now only have to satisfy the condition that \(u \in D = [0,1)^N
    \setminus \Lambda(W,\mathcal{N})\). We have with [[eqref:eq:assumptions]]
    \begin{align*}
    &[1 - \Delta U,1)_{\{a_1\}}\bigcap_{j=2}^D [1 - \operatorname{act}(\operatorname{av},k,j),1-\operatorname{act}(\operatorname{av},k,j-1))_{k \in G_j} \cap \Lambda(W,\mathcal{N}) =\\ 
&[1 - \Delta U,1)_{\{a_1\}}\bigcap_{j=2}^D [1 - \operatorname{act}(\operatorname{av},k,j),1-\operatorname{act}(\operatorname{av},k,j-1))_{k \in G_j} \cap \Lambda(W,\mathcal{N}\setminus \mathcal{U}) \text{ ,}
    \end{align*}
      since \(1 - \operatorname{act}(\operatorname{av},k,j) \geq \sum_{l\in \mathcal{N}}w_{kl}\).

    #+END_proof


** T acts bijectively on \(D\) \label{sec:bijectivity}
   Let \(\pi_D \) denote the projection to \(D\). We are now
    proceeding to show that \(\pi_D(T(a_1,\cdot))\) with \(a_1 \in
    \mathcal{N}\) has the complement of the non-inhabited region as
    invariant set and acts bijectively on it. For all points \(u\in D
    \setminus \bigcup_{\operatorname{av}\in \Omega,\operatorname{av}
    \neq ()}R(W,\operatorname{av})\) this is trivially true since on
    this region \(T(a_1,\cdot)\) is just a shift in the direction
    \(e_{a_1}\) of length \(\Delta U\) which does not leave \(D\),
    since it starts no avalanche.

    What remains to be shown is that the Images of the regions
    \(R_{\operatorname{av}}\) are in the complement of the
    non-inhabited region and that they are pairwise disjoint. We will
    show that in a constructive way by constructing all images of the
    regions \(R(W,\operatorname{av})\). We use \(S+v\) for a set \(S \) and
    a vector \(v \) as shorthand for the set \(\{s+v|s\in S\}\).\\

    #+BEGIN_prop
    \label{prop:images-r-av} For \(() \neq \operatorname{av}\in
    \Omega,a_1\in \mathcal{N} \) the image \(\pi_D(T_{a_1},R(W,\operatorname{av}))\) of
    \(R(W,\operatorname{av})\) under \(T(a_1,\cdot)\) is given by
    \begin{align}
    &\left [\sum_{j\in \mathcal{U}(\operatorname{av})}w_{ij},\sum_{j\in \mathcal{U}(\operatorname{av})}w_{ij} + \Delta U \right )_{G_1} \cap \nonumber \\
    & \bigcap_{j=2,\ldots,D}\left [\sum_{n\in \mathcal{U}(\operatorname{av},j,D)}w_{mn},\sum_{n\in \mathcal{U}(\operatorname{av},j-1,D)}w_{mn} \right )_{m\in G_j} \nonumber \cap  \left [\sum_{n\in \mathcal{U}(\operatorname{av},j,D)}w_{mn},1\right )_{m\in \mathcal{N}\setminus \mathcal{U}(\operatorname{av},1,D)} \nonumber \\ & \setminus \left ( \Lambda(W,\mathcal{N}\setminus \mathcal{U}(\operatorname{av})) + \left (\sum_{k\in \mathcal{U}(\operatorname{av},1,D)}w_{lk}\right )_{l\in \mathcal{N}\setminus \mathcal{U}(\operatorname{av})} \right )
    \end{align}
    #+END_prop



    #+BEGIN_proof
    For \(u \in R(W,\operatorname{av}) \) We have using lemma [[ref:lem:av_action]]
    \[T(a_1,u)_j = (F^{D-1} \circ E_{a_1}(u))_j = u_j + \delta_{a_1}^j \Delta U + \operatorname{act}(\operatorname{av},j,D) - \delta[j\in \mathcal{U}(\operatorname{av})]\]
    
    so \(T(a_1,\cdot)\) acts as a shift of \((\sum_{j\in \mathcal{U}(1,D)}w_{ij} + \delta[i=a_1] \Delta U - \delta[i\in \mathcal{U}(\operatorname{av})])_{i\in \mathcal{N}} \) on \(R(W,\operatorname{av}) \).
    #+END_proof

    
    The injectivity of \(\pi_DT(a_1,\cdot)\) on the regions
    \(\biguplus_{\operatorname{av} \in \Omega,\operatorname{av}_1 =
    \{a_1\}}R(W,\operatorname{av})\) remains to be shown. Since on
    each single \(R_{\operatorname{av}}\), \(T(a_1,\cdot)\) acts as a
    shift, it suffices to show that the images of different regions
    are pairwise disjoint. \\

    #+BEGIN_thm
    \label{thm:disjoint-images-r-av}
    Let \(\Omega_{a_1} = \{() \neq \operatorname{av} \in \Omega | av_1 = \{a_{1}\}\}\). It holds that
    \(T(a_1,R_{\operatorname{av1}}) \cap T(a_1,R_{\operatorname{av2}}) = \emptyset\) for \(\operatorname{av1},\operatorname{av2} \in \Omega_{a_1},\operatorname{av1} \neq \operatorname{av2}\) .
    #+END_thm

    #+BEGIN_proof
    For every \(\operatorname{av1}\) we show that its image is disjoint with the images of any \(\operatorname{av2}\)
    with \( |s_n'|  = |\mathcal{U}(\operatorname{av2})| \leq  |s_n| = |\mathcal{U}(\operatorname{av1}|)\).
    
    Since all \(R(W\operatorname{av1}),R(W,\operatorname{av2})\) with
    \(\operatorname{av1},\operatorname{av2} \in \Omega_{a_1}\) with
    the same \(\mathcal{U}(\operatorname{av1}) =
    \mathcal{U}(\operatorname{av2}) \) are shifted the same amount by
    \(T(a_1,\cdot)\), it is sufficient to show that the regions
    \(T(a_1,s_n) = \biguplus_{\operatorname{av} \in
    \Omega_{a_1},\mathcal{U}(\operatorname{av}) =
    s_n}T(a_1,\operatorname{av}) \) are pairwise disjoint.
    
    We now complete the proof by showing that \(T(a_1,s_n) \cap
    T(a_1,s_j) = \emptyset \) for all \(s_j,|s_j| \leq |s_n|\).
    
    Let \(M = s_n \setminus s_j\) be the nonempty set of indices occuring in
    \(s_n\) but not in \(s_j\), and let \(u^1 \in T(a_1,s_n)\),\(u^2
    \in T(a_1,s_j)\) be arbitrary.
    Then \(u^1_m < \sum_{i\in s_n}w_{mi}\) for all \(m\in M\), while 

    \begin{align*}
        u^2_m \in &\left [\sum_{n\in \mathcal{U}(\operatorname{av1},j,D)}w_{mn},1\right )_{m\in \mathcal{N}\setminus \mathcal{U}(\operatorname{av1})} \nonumber \\ 
        & \setminus \left ( \Lambda(W,\mathcal{N}\setminus \mathcal{U}(\operatorname{av1})) + \left (\sum_{l\in \mathcal{U}(\operatorname{av1})}w_{kl}\right )_{k\in \mathcal{N}\setminus \mathcal{U}(\operatorname{av1})} \right )  \text{ .}
    \end{align*}
    
    Now since \(M \subseteq \mathcal{N}\setminus
    \mathcal{U}(\operatorname{av1})\) we have
    \(\Gamma(W,M) \subseteq
    \Lambda(W,\mathcal{N}\setminus
    \mathcal{U}(\operatorname{av1}))\)
    so that for the component \(u^2_m \) it holds that \(u^2_m \geq
    \sum_{j\in M}w_{mj} + \sum_{l\in
    \mathcal{U}(\operatorname{av1},1,D)}w_{ml} \geq \sum_{i\in
    s_n}w_{mi} > u^1_m\) which proves that the intersection is the
    empty set.
    #+END_proof

    With the explicit construction of images for each
    \(\operatorname{av}\) it is now easy to show that the complement
    of the non-inhabited region is an invariant set for the dynamical system. \\

    #+BEGIN_thm
    \label{thm:invariant}
    For every \(a_1 \in \mathcal{M} \), \(\pi_{X} T\left (a_1,D \right )  =  D\)
    #+END_thm

    #+BEGIN_proof
    The key step is to show that the images
    \(\pi_DT(a_1,R(W,\operatorname{av}))\) lie in the complement of
    \(\Lambda(W,\mathcal{N})\). We show this by stating
    that for any avalanche \(\operatorname{av} =
    (G_i)_{i=1,\ldots,D}\) with \(G_1 = {a_1}\) and every \(I
    \subseteq \mathcal{N}\) the intersection
    \(\pi_DT(a_1,R(W,\operatorname{av})) \cap \Gamma(W,I) = \emptyset\).

    This is shown by a case distinction. First let \(I \subseteq
    \mathcal{U}(\operatorname{av})\). Let \(d_1\) be the first step of
    the avalanche in which at least one unit of \(I\) spikes during
    the avalanche and call this unit \(i_1\).
    
    Using proposition [[ref:prop:images-r-av]] and \(I \subset \mathcal{U}(\operatorname{av},d_1,D)\) we have

    \[\pi_DT(a_1,R(W,\operatorname{av}))_{i_1} \geq  \sum_{n\in \mathcal{U}(\operatorname{av},d_1,D)}w_{i_1n} \geq \sum_{n\in I}w_{i_1n} \text{, }\]

    while \(\Gamma(W,I)_{i_1} < \sum_{n\in I}w_{i_1n}\).

    If \(I \) is no subset of \(\mathcal{U}(\operatorname{av}))\) we
    have \(I' = I \setminus \mathcal{U}(\operatorname{av}) \neq
    \emptyset\). We have from [[ref:eq:def-R-av]] that
    \(R(W,\operatorname{av}) \cap \Lambda(W,I') = \emptyset\) and in particular
    \(R(W,\operatorname{av}) \cap \Gamma(W,I') = \emptyset\). \(T(a_1,\cdot) \) acts as a shift by \(s_{R_{\operatorname{av}}} = (\sum_{j\in
    \mathcal{U}(1,D)}w_{ij} + \delta[i=a_1] \Delta U - \delta[i\in
    \mathcal{U}(\operatorname{av})])_{i\in \mathcal{N}} \) on
    \(R_{\operatorname{av}} \). Applying this shift to \( \Gamma(W,I')\)
    we have that 

    \[\left ( \Gamma(W,I') + s_{R(W,\operatorname{av})} \right ) \cap \pi_DT(a_1,R(W,\operatorname{av}))  = \emptyset\text{ ,}\]
    
    while \( \Gamma(W,I) \subseteq \Gamma(W,I') + s_{R(W,\operatorname{av})} \) so 
    \(\pi_DT(a_1,R(W,\operatorname{av})) \cap \Gamma(W,I) = \emptyset\).

    Up to now we have shown that \(\pi_DT(a_1,D) \subseteq D\). For
    the other direction note that for \(u\in D\) if \(u_{a_1}-\Delta U
    \in D\) we have \(T^{-1}(a_1,u) = u-e_{a_1}\Delta U\). Since every
    point in the section \([1-\Delta_U,1]_{\{a_1\}}\) causes an
    avalanche and their images are disjoint as shown in
    [[ref:thm:disjoint-images-r-av]], every point \(u\in D\) with
    \(u-\Delta U e_{a_1} \) is part of exactly one
    \(T(a_1,R(W,\operatorname{av}))\) and thus has a unique preimage in \(D\).
    #+END_proof
    

    For the three dimensional case we can visualize the regions
    leading to avalanches and their images, as illustrated in figure [[ref:ref:fig:rav]].

    # #+ATTR_LATEX: :width 450px
    # #+CAPTION: Regions leading to avalanche patterns for the three-Dimensional EHE System with weight matrix blabla and bsdf.
    # [[/home/kima/Dropbox/uni/master//images/20171207_031447_2079_Br.png]]


    #+ATTR_LATEX: :width 450px
    #+CAPTION: Regions leading to avalanche patterns for the three dimensional EHE System.\label{ref:fig:rav}. The green region displays \(R(W,(\{1\}))\) and its image. The red region displays \(R(W,(\{1\},\{3\})\). In this region the  activation given to \(u_3\) from \(u_1\) is enough to make it spike itself, but the starting value of unit \(u_2\) is low enough that it is not pushed above the threshold. The yellow region represents \(R(W,(\{1\},\{2\})\), which behaves the same as the red region with the roles of \(u_2\) and \(u_3\) switched. The three blue regions from top left to bottom right are the regions \(R(W,(\{1 \},\{3 \},\{2 \})),R(W,(\{1 \},\{2,3 \}))\) and \(R(W,(\{1 \},\{2 \}))\).
    [[/home/kima/Dropbox/uni/master/images/Rav.pdf]]


*** Invariant measure 
     With this results we can proceed to show that the uniform Lebesque
     measure paired with the uniform Bernoulli measure on
     \(\Sigma_{N}^+\) is an invariant measure for the dynamical system. \\

     #+BEGIN_thm
     Let \(\mu \) be a uniform Bernoulli measure on
     \(\Sigma_{\mathcal{M}}^+ \) and \(\lambda \) a Lebesgue measure on
     \(D = [0,1]^N \setminus \Lambda^{\mathcal{N}}(W,1)\). Then \(\mu
     \times \lambda \) is a measure on \(\Sigma_{\mathcal{M}}\times D
     \), which is invariant under the transformation \(T\).
     #+END_thm

     #+BEGIN_proof
     The proof follows the same line of reasoning as the corresponding
     proof for the original EHE model in
     [[cite:levina2008mathematical][::Theorem 7.3.1]]. We have to prove
     that for any \(a \in \Sigma_{\mathcal{M}} \) and \(B \subseteq D
     \), \((\mu \times \lambda )(a,B) = (\mu \times \lambda
     )(T^{-1}(a,B)) \). We have that \(T^{-1}(a,B) =
     \bigcup_{i=1}^{|\mathcal{M}}(g_{m_i},B_i)\), where \(\mathcal{M} =
     \{m_1,m_2,\ldots \} \), \(g_{m_i} = \{m_i,a_1,a_2,\ldots\} \) and
     \(T(g_{m_i},B_i) = (a,B)\). Without loss of generality consider \(i=1\). Application of
     the external input to unit \(m_1\) causes either an empty
     avalanche or an avalanche \(\operatorname{av} \in \Omega_{m_1}\).
     This are the only possible choices according to Theorem
     [[ref:thm:at-most-once]]. From Threorems and Propositions
     [[ref:thm:R_av]],[[ref:prop:images-r-av]],[[ref:thm:disjoint-images-r-av]] we
     know that \(D \) can be decomposed into nonoverlapping
     hyperrectangles corresponding to the regions that produce the
     possible avalanche patterns, that on each of these regions \(T \)
     acts as a shift and that all the images are pairwise disjoint.
     Therefore, \(\pi_DT(m_1,\cdot)\) preserves the measure \(\lambda\).
     #+END_proof

** The EHE Model as random walk on the N-Torus \label{sec:random-walk-torus}


   In this section we show that the EHE Model can be written as a
   skewed random walk on the N-Torus. To do this, we identify ('glue
   together') the boundaries of the regions
   \(R(W,(\operatorname{av})\) and the corresponding boundaries of
   their images. This is illustrated in figure
   [[ref:fig:translation-surface]]. Instead of resetting after the system
   state crosses the boundary of \(D\), it enters a displaced copy of
   \(D\). In this way copies of \(D\) tesselate the plane and the
   system reduces to a random walk in the positive directions of the
   axes.

    #+ATTR_LATEX: :width 450px
    #+CAPTION: \label{fig:translation-surface} Copies of the 2 dimensional \(D\) tesselate the plane. In this tesselation, copies of \(D\) are displaced according to integer coefficients of the vectors \(S_1,S_2\) with \(S_i = \left (\delta[i=j] - w_{ji} \right )_{j\in \mathcal{N}}\). This lattice induces a quotient topology in which EHE model reduces to a random walk choosing one of the directions \(e_1,\ldots,e_{\mathcal{N}}\) and performing a shift of \(\Delta U \) in this direction.
    [[/home/kima/Dropbox/uni/master/images/surf_nontransformed.png]]



    We define for \(a,b \in \mathbb{R}^N\) 
    \begin{align}
    a &\sim_{W} b \text{ if and only if }  a-b =  \sum_{i=1}^NS_i z_i \text{, \(z_i \in \mathbb{Z} \) for all \(i=1,\ldots,N\)} \\
    S_i &= \left (\delta[i=j] - w_{ji} \right )_{j\in \mathcal{N}} \text{ .} \nonumber
    \end{align}
    
    In particular we show that\(\pi_DT(a_1,\cdot)\) for \(a_1 \in
    \mathcal{N} \) is just a shift by \(\Delta U e_{a_1} \) with
    respect to this quotient topology.     

    This quotient space is homeomorphic to the \(N-\)torus.\\
    #+BEGIN_prop
    \(\mathbb{R}^N / \sim_W\) is homeomorphic to the \(N-\)torus for
    all \(W \geq 0, \sum_{i=1}^Nw_{ji} + \Delta U < 1 \) for all \(j\in
    \mathcal{N}\).
    #+END_prop

    #+BEGIN_proof 
    We have to show that \((S_i)_{i\in \mathcal{N}}\) forms a basis of
    \(\mathbb{R}^N\).
    
    Note that \(S = I - W\). 
    
    The row-sum norm \(\|\cdot\|_{Z} \) gives an upper bound on the
    norm of the eigenvalues of W. for \(W v = \lambda v,v\neq 0\) we have

    \[|\lambda|\|v\|_{\infty} = \left\|Wv \right\|_{\infty} \leq \left\|W \right\|_Z \|v\|_{\infty} \implies \lambda \leq 1-\delta U \text{ .} \]

    It follows that \(S \) is invertible via a Neumann series cite:petersen2008matrix.
    The Inverse of \(S\) provides the homeomorphism to the N-Torus.
    #+END_proof

    Now we show that on \(\mathbb{R}^N / \sim_{W}\)
    \(\pi_DT(a_1,\cdot)\) can be seen as a shift \(\pi_DT(a_1,x) = x +
    \Delta U e_{a_1}\)\\

    #+BEGIN_prop 
    Let \(a_1\in \mathcal{N}\) be arbitrary and define
    \begin{align*}
    \tilde{T}(a_1,x) &: \mathbb{R}^N / \sim_W \longrightarrow \mathbb{R}^N / \sim_W \\
                   x &\mapsto  x + \Delta U e_{a_1} \text{ .}
    \end{align*}
    
    For \(u\in D\) it holds that
    
    \[\pi_DT(a_1,u) \sim_W \tilde{T}(a_1,u) \text{ .}\]
    #+END_prop

    #+BEGIN_proof
    For \(u_{a_1} \in [0,1 - \Delta U]\) the statement follows
    trivially. Every point with \(u_{a_1} \geq 1- \Delta U\) starts an
    avalanche \(\operatorname{av}\) upon external activation with \(a_1\) and belongs to
    exactly one \(R(W,\operatorname{av})\). We have using Proposition [[ref:prop:images-r-av]]
    \begin{align*}
    \pi_DT(a_1,u) &= u + \left (\sum_{j\in \mathcal{U}(1,D)}w_{ij} + \delta[i=a_1] \Delta U - \delta[i\in \mathcal{U}(\operatorname{av})]\right )_{i\in \mathcal{N}} \\
    &= u + \Delta U e_{a_1} - \sum_{i\in \mathcal{U}(\operatorname{av})} S_i \sim_W u + \Delta U e_{a_1} = \tilde{T}(a_1,u)\text{ .}
    \end{align*}
    #+END_proof

    In the following we will assume \(T\) defined on \(\mathbb{R}^N/\sim_W\) and use the quotient topology.

    Figure [[ref:fig:ehe-torus]] shows the 2 dimensional ehe model transformed to the torus by the inverse of \(S\).
   #+ATTR_LATEX: :width 450px
   #+CAPTION: The generalized EHE-model on the torus. \label{fig:ehe-torus}. At each step of the model the current state is shifted by one of the two vectors. When the trajectory of the system intersects one of the lines, an avalanche is fired. The black line represents firing of just \(u_1\), the red line firing of just \(u_2\), while the green and blue lines represent avalanches involving both units. Crossing the blue Line, \(u_2 \) starts the avalanche while crossing the green line the avalanche is started by \(u_1\).
   [[/home/kima/Dropbox/uni/master/images/surf_torus_better.png]]

   

** Topological transitivity and ergodicity of the homogeneous EHE-Model
   In this section we establish the ergodicity of the homogeneous EHE-Model. 
   
   We start by citing the key Lemma in the proof of the Topological
   transitivity in [[cite:levina2008mathematical][Chapter 7]] and a
   careful completion of the proof using this lemma. Using this almost
   sure topological transitivity in the phase space we show that this implies
   the ergodicity of the skew-product \(T\).
   
   We use the notation \(T^l(b,x)\) with \(b\) being a finite seuqence
   of length at least \(l\) as shorthand for \(T^l(ba,x)\) for an arbitrary \(a\in
   \Sigma^+_N\). 
   
   The main assumptions needed for the topological transitivity are cite:levina2008mathematical
   - \(\alpha \) and \(\delta \) are irrational and rationally independent
   - \(\alpha + \delta < 1\)
   - \(\delta < \frac{\alpha}{N} \)
   - \(\alpha \frac{N+1}{N} < 1\)

   #+ATTR_LATEX: :options [\cite{levina2008mathematical}{Lemma. 7.3.7}]
   #+BEGIN_lem
   For any \( y,z\) \in D and any \(\epsilon > 0 \)there exists a
   finite sequence \(b(y,z) = (b(y,z)_i)_{i=1}^{L(y,z)}\) of length
   \(L(y,z)\), such that the euclidean distance \(|z-\pi_D
   (T^{L(y,z)}(b(y,z),y))| < \epsilon\) and there is a uniform upper
   bound on \(L(y,z)\) not depending on \(y,z \), \(L(y,z) \leq M(N,\epsilon,\alpha,\delta)\).\\
   #+END_lem

   \\
   #+ATTR_LATEX: :options [\cite{levina2008mathematical}{Theorem 7.3.2}]
   #+BEGIN_thm
   For almost all (with respect to a uniform Bernoulli measure) sequences \(a \in A\), 
   \(\pi_DT(a,X) \)is topologically transitive on \(D \). \label{thm:almost-sure-top-transitivity}
   #+END_thm
   
   #+ATTR_LATEX: :options [proof extended from \cite{levina2008mathematical}{Page 99}]
   #+BEGIN_proof
   We have to prove that for almost all \(a\in A \) and for any open
   subsets \(U,V \subseteq D \), there exists an \(n\in \mathbb{N} \)
   such that \(\pi_D(T^n(a,U)) \cap V \neq \emptyset \).

   With [[cite:levina2008mathematical][Lemma 7.3.7]] we have a the
   existence of finite sequences with uniform upper bound on their
   length which move any point in \(D \) to the \(\epsilon\)
   Neighborhood of any other point in \(D\). To complete the proof we
   have to show that for any two points \(x\in U,z\in V\) the set of
   sequences eventually moving the image of \(x\) to the vicinity of
   \(z\) has full measure. However this alone is not enough to proof
   the almost sure topological transitivity since to directly use this
   for all open subsets \(U,V\) one would have to make an uncountable
   intersection of sets with probabilty one.
   
   We solve this issue by constructing a sequence of regular grids on
   \(D\) with distance between grid points converging to zero and
   showing that for each pair of points \(g_d^i,g_e^j\) from grids in
   the sequence that the set  
   \[A_{g_d^i,g_e^j} := \{a\in A| |\pi_D(T^n(a,g_d^i))-g_e^j| < \epsilon, \text{for some }n\in \mathbb{N}\}\]

   has full measure. Since we now have only countable many points on
   the sequence of grids the intersection of all \(A_{g_d^i,g_e^j}\)
   for pairs of grid points still has probability one.
   
   This is enough to prove the almost sure topological transitivty
   since for every pair of \(U,V \subseteq D\) threre exists a point
   in the grid sequence which is in \(U\) and a point in the grid
   sequence for which the \(\epsilon \) ball is fully contained in \(V\).
   
   We now complete the proof by showing that for any \(x,z\in D,
   \epsilon > 0\) \(A_{x,z}\) has probability one by showing that the complement
   \[A_{x,z}^c := \{a \in A| |T^n(a,x) - z| > \epsilon \forall n\in \mathbb{N}\} \]
   
   is a null set. With Lemma 7.3.7 we have for each image
   \(y_x^j=T^j(a,x)\) at least one completing sequence
   \(b(y_x^j,z)\)of length at most M(\epsilon,\delta,\alpha,N) such
   that if \(b(y_x^j,z) \) is a prefix of \(\sigma^j(a) = a_{j+1},a_{j+2},\ldots\) there
   exists a \(n \leq j + M\) with \(|T^n(a,x) - z | < \epsilon \).
   
   In particular we have 
   \begin{align*}
   A^c_{x,z} &\subseteq \{a \in A| b(y_x^j) \text{ is not prefix of } \sigma^j(a) \forall j \in \mathbb{N}\} \\
             &\subseteq \{a \in A| b(y_x^{pM}) \text{ is not prefix of } \sigma^{pM}(a) \forall p \in \mathbb{N}\} =: C_{x,z}^{\infty} \text{ .}
   \end{align*}

   Denote by \(C_{x,z}^n\) the set 
   \[C_{x,z}^n = \{a\in A|b(y_x^{pM},z)\text{ is not prefix of } \sigma^{pM}(a) \forall 0 \leq p < n \} \text{ .}\]

   Note that \(C_{x,z}^{n+1}\subseteq C_{x,z}^n\) and \(C_{x,z}^{\infty} = \bigcap_{n\in \mathbb{N}}C_{x,z}^n\).
   For the sets \(C_{x,z}^n\) we can inductively derive upper bounds on their probability:
   \[P(C_{x,z}^1) \leq 1-\frac{1}{N^M} \text{ and } P(C_{x,z}^{n+1}) \leq P(C_{x,z}^n)(1-\frac{1}{N^M})\]

   It now follows by continuity from above that \[P(A_{x,z}^c) \leq
   P(C^{\infty}_{x,z}) = \lim_{n\rightarrow \infty}C_{x,z}^n \leq
   (1-\frac{1}{N^M})^n = 0\] which completes the proof for a fixed
   \(\epsilon\). This is no problem however, since we can accomodate
   arbitrary small \(\epsilon\) values using a countable intersection
   of the sets constructed here with respect to the \(\epsilon\) levels
   \(\epsilon_n = \frac{1}{n}\).
   #+END_proof


   This almost sure topological transitivity paired with the uniform
   bernoulli shift is enough to ensure the ergodicity of the skew
   product dynamical system. \\

   #+BEGIN_thm 
   \(T\) is ergodic with respect to the product measure \(\mu \times
   \lambda\), where \(\mu\) is the uniform bernoulli measure on
   \(\Sigma_{N}\) and \(\lambda\) the uniform lebesgue measure on \(D \).
   The same holds when using the one sided shift space \(\Sigma^+_N \).
   #+END_thm


   #+BEGIN_proof 
   We have to show that for every two sets \(E,H \subset \Sigma_N\times D\) of positive measure, there exists an \(n > 0\) such that
    \((\mu \times \lambda)((T^{-n}E)\cap H)>0\). 

    Let \(m[i_0,\ldots,i_n] \) denote a zylinder set on \(\Sigma_N \)
    following the notation in cite:einsiedler2013dynamische.
    \[m[i_0,\ldots,i_n] := \{y = (y_m)_{m\in \mathbb{Z} }\in \Sigma_N | y_{k+m} = i_k \text{ for }k=0,\ldots,n\}\]

    For every set \(E,H\) of positive \(\mu \times \lambda\) measure
    we can find products of cylindersets on \(\Sigma_N\) and open sets on \(D\) contained in them.
    \(m_e[i^e_0,\ldots,i_{n_e}^e] \times U \subseteq E\),
    \(m_h[i^h_0,\ldots,i_{n_h}^h] \times V \subseteq H\).

    For all \(n > \max(m_e + n_e - m_h,0)\) we
    have \(\sigma^{-n}m_e[i^e_0,\ldots,i_{n_e}^e] \subseteq
    m_h[i^h_0,\ldots,i_{n_h}^h]\) and thus \(\pi_{\Sigma_N}(T^{-n}(E))
    \subseteq \pi_{\Sigma_N}(H) \text{ .}\)
    
    Since \(T \) preserves the \(\mu \times \lambda \) measure we have
    that \(\mu\times \lambda(T^{-n}(E)) = \mu \times
    \lambda(\sigma^{-n}m_e[i^e_0,\ldots,i_{n_e}^e]\times U') > 0\) and
    \(U'\) is a nonenpty open set.
    
    Thus we can apply Theorem [[ref:thm:almost-sure-top-transitivity]]
    with \(U'\) and \(V\) and there exists an \(n_2\) such that
    \(\pi_DT^{n_2}(a,V) \cap U') \neq \emptyset\) and by continuity of
    \(T(a_1,\cdot)\) for all \(a_1\in \mathcal{N}\) (with respect to
    the quotient topology) this intersection has positive measure for
    almost all \(a \in \sigma^{-n}m_e[i^e_0,\ldots,i_{n_e}^e] \). 

    Taken together we have \(\mu \times \lambda  \left(T^{-n-n_2}(E) \cap H \right ) > 0 \) proving the theorem.
   #+END_proof


   
** Probability Space for the avalanche patterns \label{sec:prob-space}

   In this section we will construct the probability space for the
   avalanches generated by the generalized EHE Model.

   The sample space of the avalanches is given by \(\Omega\). Since \(\Omega \) is a finite set we choose the powerset \(\mathcal{F} = 2^{\Omega}\) as the sigma algebra.

   In order to specify the probability measure it suffices to assign a
   probability to each elementary event \(\operatorname{av} \in
   \Omega\). Under the assumption of ergodicity, the propability of
   producing a specifc avalanche \(\operatorname{av}\)i s proportional
   to the volume of the region \(R(W,\operatorname{av})\) which can be
   directly calculated using [[eqref:eq:determinant-formula]]:

  \begin{align}
   \mathcal{V}(R(W,(G_i)_{i=1\ldots D})) &= \Delta U \prod_{i=2}^D\prod_{j\in G_i}\left (\sum_{k \in G_{i-1}}w_{j,k} \right ) \cdot \nonumber \nonumber \\ 
   &\Bigg ( \prod_{l\in \mathcal{N}\setminus \biguplus_{i=1}^DG_i}(1-\sum_{l\in \biguplus_{i=1}^DG_i}w_{li}) - \nonumber \\ 
   &\mathcal{V} \left (\Lambda \left (W,\mathcal{N}\setminus \biguplus_{i=1}^DG_i \right ) \cap \left [0,1-\sum_{l\in \biguplus_{i=1}^DG_i}w_{li} \right )_{l\in \biguplus_{i=1}^DG_i}\right ) \Bigg ) \text{ .} \label{eq:vol-r}
   \end{align}

   These volumes uniquely specify the probability space for all nonempty avalanches.
   The volume for the empty avalanche is given by 
   \begin{align} \label{eq:noavl}
   \sum_{i=1}^N \frac{1 - \Delta U - \mathcal{V}\left (\Lambda(W,\mathcal{N}) \cap [0,1-\delta[j=i]\Delta U]_{j\in \mathcal{N}}\right )}{N} \text{, }
   \end{align}
   since external input to unit \(u_{i}\) doesn't start an avalanche
   in the region \([0,1-\Delta U)_{{i}}\) which has volume \(1-\Delta
   U\). This formula follows after substracting the intersection of
   the non-inhabited volume with this region and averaging over all
   possible starting units \(i\).

*** Calculation of Regions leading to avalanche patterns       :implementation:
    :PROPERTIES:
    :header-args: :tangle src/ehe_ana.py
    :END:
    The next step is the calculation of the volumes leading to
    specific *avalanche patterns*. A avalanche pattern is defined by
    specifying which units will fire at wich position during the avalanche. 

    First one needs to generate all unique avalanche patterns for \(N \) units.
treffen wir uns da ja treffen wir uns da ja 
    #+begin_src ipython :session master
      from itertools import chain, combinations
      import numpy as np

      def spiking_patterns(N):
          """a avalanche pattern is expressed in the format [(i,),(i_2_1,i_2,2...),...,(...,i_D_k)]"""
          for avalanche_size in range(N,0,-1):
              #print("avalanche_size ", avalanche_size)
              if avalanche_size == 1:
                  for unit in range(N):
                      yield [(unit,)]
              else:
                  for length in range(avalanche_size,1,-1):
                      #print("combinations",range(N),length)
                      for units in combinations(range(N),avalanche_size):
                          #print("units ",units)
                          yield from sp_length_units(length,units)

      def group_sizes(N,length):
          """list possible sizes of groups where the first group has size 1 and the sum of all groups has to equal length.
             can be reduced to choosing length-2 spaces between the remaining N-1 elements"""
          if length == 2:
              yield [1,N-1]
          else:
              for sel_spaces in combinations(range(2,N),length-2):
                  yield np.diff((0,1)+sel_spaces+(N,))

      def unit_placements(units,gs):
          if len(gs) == 0:
              yield []
          else: 
              for sel_units in combinations(units,gs[0]):
                  for rest_placement in unit_placements(units-set(sel_units),gs[1:]):
                      yield [sel_units] + rest_placement

      def sp_length_units(length,units):
          for gs in group_sizes(len(units),length):
              yield from unit_placements(set(units),gs)

      def completing_patterns(pattern,N,set_of_patterns=None):
          if set_of_patterns==None:
              set_of_patterns = spiking_patterns(N)
          def is_prefix(pattern1,pattern2):
              fp1= np.array(reduce(lambda a,b:a+b,pattern1))
              fp2 = np.array(reduce(lambda a,b:a+b,pattern2))
              if len(fp2) <= len(fp1):
                  return False
              return (fp1 == fp2[:len(fp1)]).all()
          return [p for p in set_of_patterns if is_prefix(pattern,p)]
    #+end_src 

    #+RESULTS:

    After all avalanche patterns can be enumerated, for each of the
    avalanche pattern the volume of the region leading to this spiking
    pattern has to be calculated

     #+begin_src ipython :session master
       class SpikingPatternVolumes():
           def __init__(self,W,deltaU=0.022):
               self.N = W.shape[0]
               self.sps= list(spiking_patterns(self.N))
               self.volumes = {tuple(sp):None for sp in self.sps}
               self.W = W
               self.normalized_volumes={}
               self.NR = NoninhabitedRegion(W)
               self.deltaU = deltaU

           def get_probs(self):
               vol = 0
               for sp in self.sps:
                   vol +=self.volume(sp)
               self.normalized_volumes = {k:v/vol for k,v in self.volumes.items()}
               return self.normalized_volumes

           def volume(self,pattern):
               vol = self.volumes[tuple(pattern)]
               if vol is not None:
                   return vol
               # now calculate volume... oh my
               N = self.N;W = self.W;deltaU=self.deltaU
               i = pattern[0][0]
               ip = set(reduce(lambda a,b:a+b,pattern))
               ipc = set(range(self.N)) - ip
               area_along_i = deltaU
               vol = area_along_i
               if len(ipc) > 0:
                   upper_bounds = [1-sum(W[ic,i] for i in ip) for ic in ipc]
                   #nr = sym.symbols('NONINH_'+'_'.join(str(i) for i in ipc)+"__"+"_".join(str(ub).replace(" ","_") for ub in upper_bounds))
                   area_along_ipc = prod(upper_bounds) - self.NR.noninhabited_region(ipc,upper_bounds)
                   vol*=area_along_ipc
               if len(pattern) > 1:
                   area_along_p = prod(prod(sum(W[j,k] for k in pattern[d-1]) for j in pattern[d]) for d in range(1,len(pattern)))
                   vol *= area_along_p
               self.volumes[tuple(pattern)] = vol
               return vol

    #+end_src 

    #+RESULTS:

    To test the theoretical predictions against the simulation
    results, the avalanche pattern can be extracted from the detailed
    simulation output using the EHE.get_spiking_pattern method. The
    empirical distribution for the avalanche patterns and avalanche
    sizes can then be easily calculated.

    #+begin_src ipython :session master
      def empirical_sp_dist(emp_spiking_patterns):
          unique,counts = np.unique(emp_spiking_patterns,return_counts=True)
          norm_count = np.sum(counts)
          return {tuple(tuple(s for s in step) for step in sp):c/norm_count for sp,c in zip(unique,counts)}
    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master
      def avalanche_size_statistics(N,sp_probs):
          # group all avalanche patterns with the same avalanche size
          avs = [0 for i in range(N)]
          for sp,prob in sp_probs.items():
              size = sum(len(step) for step in sp)
              #print(sp,size)
              avs[size-1] += prob
          return avs
    #+end_src 

    #+RESULTS:

    Long running calculation to test the calculated statistics for
    random weight matrices.

    #+begin_src ipython :session master :tangle no :results output :eval no
      ehe = load_module('ehe_detailed')
      N = 4;
      alpha=(1-1/np.sqrt(N))/N;
      W = np.random.normal(alpha,0.01,(N,N))
      e=ehe.EHE();
      e.simulate_model_mat(np.random.random(N),15000000,W/N,deltaU);
      emp_sp = e.get_spiking_patterns()
      emp_probs = empirical_sp_dist(emp_sp)
      spv = SpikingPatternVolumes2(W/N)
      probs = spv.get_probs()


      emp_vs_theo = {k:(emp_probs[k],v_th) for k,v_th in probs.items()}

      print(emp_vs_theo)

      avs_stats = avalanche_size_statistics(N,probs)
      emp_avs_stats = avalanche_size_statistics(N,emp_probs)

      emp_vs_theo_avs_stats = list(zip(avs_stats,emp_avs_stats))

      print(emp_vs_theo_avs_stats)

      ##with constant matrix

      # ehe = load_module('ehe_detailed')
      # N = 4;
      # alpha=(1-1/np.sqrt(N))/N;
      # W = np.random.random((N,N))*alpha/N
      # e=ehe.EHE();
      # e.simulate_model_const(np.random.random(N),10000000,alpha,deltaU);
      # emp_sp = e.get_spiking_patterns()
      # emp_probs = empirical_sp_dist(emp_sp)
      # spv = SpikingPatternVolumes(N,np.ones((N,N))*alpha*N)
      # probs = spv.get_probs()


      # emp_vs_theo = {k:(emp_probs[k],v_th) for k,v_th in probs.items()}

      # print(emp_vs_theo)

      # avs_stats = avalanche_size_statistics(N,probs)
      # emp_avs_stats = avalanche_size_statistics(N,emp_probs)

      # emp_vs_theo_avs_stats = list(zip(avs_stats,emp_avs_stats))

      # print(emp_vs_theo_avs_stats)

    #+end_src 

    #+RESULTS:

    It works!
    #+begin_src ipython :tangle no :eval no :session master
    {((2,), (0,), (3,), (1,)): (7.3333338222222545e-06, 7.4023845221766183e-06), ((1,), (2,)): (0.0072782004852133654, 0.0072883647145364338), ((2,), (0, 3), (1,)): (1.5400001026666736e-05, 1.4784914630195591e-05), ((0,), (1, 3)): (0.00023200001546666769, 0.00023457475104576901), ((2,), (0, 1, 3)): (8.0000005333333682e-06, 7.3556367665006508e-06), ((3,), (1, 2), (0,)): (1.9133334608888975e-05, 1.8344958946962747e-05), ((0,),): (0.22704868180324544, 0.22709823822515504), ((3,), (2,), (0, 1)): (7.7333338488889231e-06, 7.1450549674775726e-06), ((0,), (1,), (2, 3)): (7.6000005066667005e-06, 8.1878560972712507e-06), ((0,), (1,), (3,)): (0.00021646668109777875, 0.00022063329876278933), ((1,), (3,), (0,), (2,)): (8.6666672444444837e-06, 7.6329124650450139e-06), ((1,),): (0.22441514829434323, 0.22447776760053223), ((0,), (3,), (1,)): (0.00025266668351111221, 0.00025699349159496321), ((1,), (2,), (0,)): (0.00019773334651555644, 0.00019515842009443495), ((1,), (0, 3), (2,)): (2.1533334768888983e-05, 1.96461246992291e-05), ((3,), (0, 1, 2)): (8.4000005600000368e-06, 8.4565635657594027e-06), ((2,), (3,), (0,), (1,)): (7.8000005200000348e-06, 7.9463512404856184e-06), ((2,), (0, 1)): (0.00018953334596888974, 0.00019126270731406719), ((3,), (0,), (2,), (1,)): (7.4666671644444779e-06, 7.9790938716993111e-06), ((0,), (3,), (1,), (2,)): (8.8000005866667054e-06, 9.5372082950052559e-06), ((0,), (3,), (1, 2)): (1.0533334035555603e-05, 9.6423295998628381e-06), ((3,), (0,), (1,)): (0.00020513334700888981, 0.00020572790761777638), ((2,), (1, 3)): (0.00025433335028889, 0.00025695449630125692), ((3,), (1,)): (0.007453933830262255, 0.0074508427189384436), ((1,), (0,)): (0.0081736005449067027, 0.0081666505360276937), ((1,), (0,), (3,), (2,)): (1.0800000720000048e-05, 1.0488295231283621e-05), ((2,), (3,), (1,), (0,)): (1.1333334088888939e-05, 1.1156026686200387e-05), ((0,), (1,)): (0.0069096671273111421, 0.0068529912939196864), ((3,), (2,), (1,), (0,)): (9.1333339422222623e-06, 1.031235821489122e-05), ((0,), (2,), (1,)): (0.0002524666834977789, 0.0002470228571318791), ((0,), (1,), (2,)): (0.00023600001573333438, 0.00023636105410043874), ((3,), (1,), (0, 2)): (1.0466667364444491e-05, 1.0718503124227781e-05), ((3,), (2,), (1,)): (0.00025546668369777893, 0.00024959824125279971), ((0,), (1,), (2,), (3,)): (9.6666673111111543e-06, 9.0900420898840084e-06), ((1,), (3,), (2,)): (0.0002461333497422233, 0.00023876915819591229), ((2,), (0,)): (0.005677267045151136, 0.0056854089911971082), ((0,), (2, 3)): (0.00025813335054222336, 0.00025214619009804112), ((2,), (1,), (0,)): (0.00027033335135555678, 0.00027604679879304886), ((3,), (0,), (1, 2)): (7.8000005200000348e-06, 7.653314617018567e-06), ((3,), (1,), (0,)): (0.00028900001926666794, 0.00028882514225986094), ((1,), (0,), (2,)): (0.00028406668560444569, 0.00028235593756994939), ((1,), (2,), (0, 3)): (8.8000005866667054e-06, 7.505459220451097e-06), ((2,), (0, 3)): (0.00020426668028444535, 0.00020385954748076545), ((1,), (3,), (2,), (0,)): (6.2000004133333613e-06, 6.835059218707558e-06), ((0,), (3,)): (0.0072254004816933656, 0.0072740070190894391), ((0,), (1,), (3,), (2,)): (8.6666672444444837e-06, 8.2781045316461235e-06), ((3,), (1, 2)): (0.00025413335027555669, 0.00026228797137259531), ((0,), (3,), (2,), (1,)): (8.7333339155555937e-06, 9.1758249419936485e-06), ((3,), (0, 2), (1,)): (1.713333447555563e-05, 1.576628544057955e-05), ((0,), (2,)): (0.0073704004913600326, 0.0073429158914006948), ((2,), (0,), (3,)): (0.00019273334618222308, 0.00019522955696092562), ((1,), (0, 3)): (0.00026786668452444566, 0.00026292679653706387), ((3,), (0,)): (0.0064003337600222511, 0.0063794855898293423), ((1,), (0,), (2,), (3,)): (1.1600000773333385e-05, 1.0858926681503309e-05), ((1,), (2, 3), (0,)): (1.4000000933333395e-05, 1.437489701376413e-05), ((0,), (1, 2)): (0.00023526668235111214, 0.00023693713517152409), ((0,), (2,), (3,), (1,)): (9.0666672711111522e-06, 9.9830666333164614e-06), ((0,), (1, 3), (2,)): (1.7800001186666744e-05, 1.7506417376329199e-05), ((0,), (2, 3), (1,)): (1.8866667924444528e-05, 1.8658363119887091e-05), ((3,), (1,), (2,)): (0.00026300001753333448, 0.00025942849083798177), ((3,), (1,), (2,), (0,)): (6.4000004266666947e-06, 7.4264578863347261e-06), ((3,), (0, 2)): (0.00022706668180444545, 0.00022303234226876635), ((2,), (1,), (0,), (3,)): (1.086666739111116e-05, 1.0166868446199118e-05), ((1,), (2,), (3,), (0,)): (8.0666672044444799e-06, 8.4533482305508169e-06), ((1,), (3,), (0,)): (0.00020300001353333423, 0.0002051794796167785), ((3,), (1,), (0,), (2,)): (1.093333406222227e-05, 1.074462725361846e-05), ((3,), (0, 1), (2,)): (1.5733334382222293e-05, 1.6749125535604259e-05), ((0,), (2,), (1,), (3,)): (8.5333339022222602e-06, 8.5571949009549921e-06), ((1,), (0, 2, 3)): (9.4666672977778191e-06, 9.7573973930225058e-06), ((1,), (0, 2)): (0.00027933335195555681, 0.00028166942673311972), ((3,), (0, 1)): (0.00022826668188444545, 0.00022538970226553711), ((1,), (0, 2), (3,)): (2.0000001333333422e-05, 2.1206475802338304e-05), ((2,), (1,)): (0.0071262671417511424, 0.0071428758570602274), ((2,), (1,), (3,)): (0.00022320001488000099, 0.00023145178190130423), ((1,), (0,), (3,)): (0.00027680001845333457, 0.00027954070481107826), ((1,), (2,), (3,)): (0.00026913335127555678, 0.00026218824484153041), ((0,), (3,), (2,)): (0.00025786668385777893, 0.00025430558628949945), ((2,), (0,), (1,), (3,)): (6.8000004533333633e-06, 6.355073452258784e-06), ((1,), (3,), (0, 2)): (8.0000005333333682e-06, 7.6982812183874472e-06), ((2,), (3,)): (0.0076454671763644781, 0.0076621175183758661), ((1,), (2,), (0,), (3,)): (6.600000440000029e-06, 7.1877304570943125e-06), ((2,), (1,), (0, 3)): (9.8000006533333777e-06, 9.562622206234607e-06), ((0,), (1, 2, 3)): (1.0266667351111156e-05, 8.7264503476967776e-06), ((3,),): (0.22618734841248989, 0.22613197396670071), ((2,), (0,), (1,)): (0.0001788666785911119, 0.00018345362231783716), ((2,), (3,), (0,)): (0.00023200001546666769, 0.00022960563695845974), ((1,), (2, 3)): (0.00023120001541333435, 0.00023616608129324576), ((3,), (2,), (0,)): (0.00020353334690222314, 0.00019802332804534693), ((2,), (3,), (0, 1)): (8.5333339022222602e-06, 8.7057986489513941e-06), ((2,), (0,), (1, 3)): (7.2000004800000319e-06, 6.7566400053870695e-06), ((3,), (2,)): (0.0074066004937733661, 0.007442761595487706), ((1,), (0,), (2, 3)): (1.0000000666666711e-05, 1.039923550944353e-05), ((3,), (0,), (2,)): (0.00021766668117777875, 0.00022113849794747425), ((0,), (1, 2), (3,)): (1.6000001066666736e-05, 1.7320009482231525e-05), ((2,), (1, 3), (0,)): (1.9066667937777862e-05, 1.8900892197918601e-05), ((2,), (3,), (1,)): (0.00027313335154222345, 0.00027001822301168856), ((1,), (3,)): (0.0067976671198444749, 0.0067827412539778918), ((2,),): (0.22770928184728545, 0.22765108560344119), ((3,), (2,), (0,), (1,)): (6.000000400000027e-06, 6.8533287740793675e-06), ((2,), (1,), (3,), (0,)): (7.2666671511111436e-06, 7.4623578649598886e-06), ((0,), (2,), (1, 3)): (1.0266667351111156e-05, 9.5000768084999718e-06), ((2,), (0, 1), (3,)): (1.3800000920000062e-05, 1.3669840811814834e-05), ((3,), (0,), (1,), (2,)): (6.7333337822222525e-06, 7.6347066023705735e-06), ((0,), (2,), (3,)): (0.00026213335080889008, 0.000263292141889528)}
[(0.90535906539582911, 0.90536046035736395), (0.085471162979840531, 0.085464805697653717), (0.0086175547506850379, 0.0086199339079955951), (0.0005522168736453065, 0.00055480003698666919)]
    #+end_src
    
    #+RESULTS:


    #+begin_src ipython :session master :tangle no :results output 
      ehe = load_module('ehe_detailed')
      
      N = 3;
      alpha=(1-1/np.sqrt(N))/N;
      W = np.random.normal(alpha,0.05,(N,N))
      e=ehe.EHE();
      e.simulate_model_mat(np.random.random(N),5000000,W/N,deltaU);
      emp_sp = e.get_spiking_patterns()
      emp_probs = empirical_sp_dist(emp_sp)
      spv = SpikingPatternVolumes(W/N)
      probs = spv.get_probs()


      emp_vs_theo = {k:(emp_probs[k],v_th) for k,v_th in probs.items()}

      print(emp_vs_theo)
      norm_emp_vs_theo = sum([np.abs(x[0]-x[1]) for (k,x) in emp_vs_theo.items()])
      print(norm_emp_vs_theo)


      # avs_stats = avalanche_size_statistics(N,probs)
      # emp_avs_stats = avalanche_size_statistics(N,emp_probs)

      # emp_vs_theo_avs_stats = list(zip(avs_stats,emp_avs_stats))

      # print(emp_vs_theo_avs_stats)

      ##with constant matrix

      # ehe = load_module('ehe_detailed')
      # N = 4;
      # alpha=(1-1/np.sqrt(N))/N;
      # W = np.random.random((N,N))*alpha/N
      # e=ehe.EHE();
      # e.simulate_model_const(np.random.random(N),10000000,alpha,deltaU);
      # emp_sp = e.get_spiking_patterns()
      # emp_probs = empirical_sp_dist(emp_sp)
      # spv = SpikingPatternVolumes(N,np.ones((N,N))*alpha*N)
      # probs = spv.get_probs()


      # emp_vs_theo = {k:(emp_probs[k],v_th) for k,v_th in probs.items()}

      # print(emp_vs_theo)

      # avs_stats = avalanche_size_statistics(N,probs)
      # emp_avs_stats = avalanche_size_statistics(N,emp_probs)

      # emp_vs_theo_avs_stats = list(zip(avs_stats,emp_avs_stats))

      # print(emp_vs_theo_avs_stats)

    #+end_src 

    #+RESULTS:
    : {((2,), (0,), (1,)): (0.00037360007472001493, 0.00038077502913431071), ((2,), (0, 1)): (0.00077040015408003082, 0.00076071944303957224), ((2,), (1,), (0,)): (0.00068900013780002752, 0.00070867424338149905), ((1,), (0, 2)): (0.00089640017928003591, 0.00091167179476491032), ((0,), (1, 2)): (0.00047740009548001908, 0.00048772197071480692), ((2,), (0,)): (0.015828603165720632, 0.015802738144991987), ((0,),): (0.30196266039253206, 0.30201539933152788), ((1,), (2,)): (0.017315003463000692, 0.017394156513037193), ((2,),): (0.3024276604855321, 0.30236854453275575), ((1,), (0,)): (0.014008002801600561, 0.013972426969645371), ((0,), (2,)): (0.020267604053520811, 0.020241197560376088), ((1,),): (0.30149966029993208, 0.30147669157922657), ((0,), (1,)): (0.0065332013066402616, 0.0065361092150597556), ((1,), (2,), (0,)): (0.0009802001960400391, 0.00097862518135163152), ((1,), (0,), (2,)): (0.0010484002096800419, 0.0010426171584774927), ((0,), (1,), (2,)): (0.00042560008512001704, 0.00042646753007321208), ((2,), (1,)): (0.013513602702720541, 0.013521083768215812), ((0,), (2,), (1,)): (0.00098300019660003935, 0.00097438003422609088)}
    : 0.000391181917231



    symbolic 3 dim avalanche patterns
    #+begin_src ipython :session master :results output :cache yes :tangle no
      W = symbolic_matrix(3);
      spv = SpikingPatternVolumes(W,deltaU=sym.symbols('deltaU'))
      spv.get_probs();
      print('unnormalized volumes')
      for k,v in spv.volumes.items():
          spv.volumes[k] = sym.simplify(v)
          print(k,spv.volumes[k])
      print('normalized volumes')
      for k,v in spv.get_probs().items():
          print(k,sym.simplify(v))
    #+end_src 

    #+RESULTS[f21e5debf1f499922bf05f2a5043ed86da4cb6cf]:
    #+begin_src ipython :session master :eval no :tangle no
    unnormalized volumes
    ((2,), (0,), (1,)) deltaU*w02*w10
    ((2,), (0, 1)) deltaU*w02*w12
    ((2,), (1,), (0,)) deltaU*w01*w12
    ((1,), (0, 2)) deltaU*w01*w21
    ((0,), (1, 2)) deltaU*w10*w20
    ((2,), (0,)) deltaU*w02*(-w10 - w11 - w12 + 1)
    ((0,),) deltaU*(w10*w20 + w10*w22 - w10 + w11*w20 + w11*w22 - w11 - w12*w21 - w20 - w22 + 1)
    ((1,), (2,)) deltaU*w21*(-w00 - w01 - w02 + 1)
    ((2,),) deltaU*(w00*w11 + w00*w12 - w00 - w01*w10 + w02*w11 + w02*w12 - w02 - w11 - w12 + 1)
    ((1,), (0,)) deltaU*w01*(-w20 - w21 - w22 + 1)
    ((0,), (2,)) deltaU*w20*(-w10 - w11 - w12 + 1)
    ((1,),) deltaU*(w00*w21 + w00*w22 - w00 + w01*w21 + w01*w22 - w01 - w02*w20 - w21 - w22 + 1)
    ((0,), (1,)) deltaU*w10*(-w20 - w21 - w22 + 1)
    ((1,), (2,), (0,)) deltaU*w02*w21
    ((1,), (0,), (2,)) deltaU*w01*w20
    ((0,), (1,), (2,)) deltaU*w10*w21
    ((2,), (1,)) deltaU*w12*(-w00 - w01 - w02 + 1)
    ((0,), (2,), (1,)) deltaU*w12*w20
    normalized volumes
    ((2,), (0,), (1,)) w02*w10/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((2,), (0, 1)) w02*w12/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((2,), (1,), (0,)) w01*w12/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((1,), (0, 2)) w01*w21/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((0,), (1, 2)) w10*w20/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((2,), (0,)) w02*(w10 + w11 + w12 - 1)/(-w00*w11 - w00*w22 + 2*w00 + w01*w10 + w02*w20 - w11*w22 + 2*w11 + w12*w21 + 2*w22 - 3)
    ((0,),) (w10*w20 + w10*w22 - w10 + w11*w20 + w11*w22 - w11 - w12*w21 - w20 - w22 + 1)/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((1,), (2,)) w21*(w00 + w01 + w02 - 1)/(-w00*w11 - w00*w22 + 2*w00 + w01*w10 + w02*w20 - w11*w22 + 2*w11 + w12*w21 + 2*w22 - 3)
    ((2,),) (w00*w11 + w00*w12 - w00 - w01*w10 + w02*w11 + w02*w12 - w02 - w11 - w12 + 1)/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((1,), (0,)) w01*(w20 + w21 + w22 - 1)/(-w00*w11 - w00*w22 + 2*w00 + w01*w10 + w02*w20 - w11*w22 + 2*w11 + w12*w21 + 2*w22 - 3)
    ((0,), (2,)) w20*(w10 + w11 + w12 - 1)/(-w00*w11 - w00*w22 + 2*w00 + w01*w10 + w02*w20 - w11*w22 + 2*w11 + w12*w21 + 2*w22 - 3)
    ((1,),) (w00*w21 + w00*w22 - w00 + w01*w21 + w01*w22 - w01 - w02*w20 - w21 - w22 + 1)/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((0,), (1,)) w10*(w20 + w21 + w22 - 1)/(-w00*w11 - w00*w22 + 2*w00 + w01*w10 + w02*w20 - w11*w22 + 2*w11 + w12*w21 + 2*w22 - 3)
    ((1,), (0,), (2,)) w01*w20/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((1,), (2,), (0,)) w02*w21/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((0,), (1,), (2,)) w10*w21/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
    ((2,), (1,)) w12*(w00 + w01 + w02 - 1)/(-w00*w11 - w00*w22 + 2*w00 + w01*w10 + w02*w20 - w11*w22 + 2*w11 + w12*w21 + 2*w22 - 3)
    ((0,), (2,), (1,)) w12*w20/(w00*w11 + w00*w22 - 2*w00 - w01*w10 - w02*w20 + w11*w22 - 2*w11 - w12*w21 - 2*w22 + 3)
#+end_src

#   symbolic volumes with noninhabited region replaced by symbol
#  #+begin_src ipython :session master :results output :cache yes :tangle no
#       W = symbolic_matrix(4); 
#       SpikingPatternVolumes(W,deltaU=sym.symbols('deltaU'))
#       spv.get_probs();
#       print('unnormalized volumes')
#       for k,v in spv.volumes.items():
#           spv.volumes[k] = sym.simplify(v)
#           print(k,spv.volumes[k])
#       print('normalized volumes')
#       for k,v in spv.get_probs().items():
#           print(k,sym.simplify(v))
#     #+end_src 
*** 2d phase space density visualization for different weight matrices :implementation:

    With strictly positive weight matrices, the central feature of the
    ehe model of having an equilibrium density that is uniform in the
    inhabited volume is preserved. The analytical reason for this
    extends from the case of the original EHE Model because given the
    direction of the external input the resulting internal activation acts
    bijectively on the inhabited phase space.

     #+begin_src ipython :session master :file ./images/20170622_123227_1871Jvwe3r.png :tangle no :cache yes
      ehe = load_module('ehe_detailed')
      N = 2;
      alpha=(1-1/np.sqrt(N))/N;
      e=ehe.EHE();
      e.record_units=True;
      e.simulate_model_mat(np.random.random(2),2800000,alpha*np.array([[1.5,1],[1,1.3]]),deltaU*2);
      uh = e.unit_hist;
      plt.hist2d(uh[:,0],uh[:,1],bins=1000,range=[[0,1],[0,1]])
    #+end_src 

    #+RESULTS[0df226598a61391812c7243d74d2c015d5359540]:
    [[file:./images/20170622_123227_1871Jvwe3r.png]]


    This is no longer the case when weights become negative
    #+begin_src ipython :session master :file ./images/20170622_123227_1871JvP234.png :tangle no :cache yes
      ehe = load_module('ehe_detailed') 
      N = 2;
      alpha=(1-1/np.sqrt(N))/N;
      e=ehe.EHE();
      e.record_units=True; 
      e.simulate_model_mat(np.random.random(2),2800000,alpha*np.array([[-1.5,1],[1,1.3]]),deltaU*2);
      uh = e.unit_hist;
      plt.hist2d(uh[:,0],uh[:,1],bins=1000,range=[[0,1],[0,1]])
    #+end_src 

    #+RESULTS[9456fe7286bdc196455e7bfbe90a05ba0c3235fa]:
    [[file:./images/20170622_123227_1871JvP234.png]]

    #+begin_src ipython :session master :file ./images/20170622_123227_1871JvP.png :tangle no :cache yes
      ehe = load_module('ehe_detailed')
      N = 2;
      alpha=(1-1/np.sqrt(N))/N;
      e=ehe.EHE();
      e.record_units=True;  
      e.simulate_model_mat(np.random.random(2),2800000,alpha*np.array([[1.5,-1],[1,1.3]]),deltaU*2);
      uh = e.unit_hist;
      plt.hist2d(uh[:,0],uh[:,1],bins=1000,range=[[0,1],[0,1]])
    #+end_src 

    #+RESULTS[599cc51bc83a876874406b35ab9baa2706c6c9f4]:
    [[file:./images/20170622_123227_1871JvP.png]]


    Now with new version without clamping to zero.


     #+begin_src ipython :session master :file ./images/20170622_123227_1871Jvwe3342r.png :tangle no :cache yes
      ehe = load_module('ehe_detailed')
      N = 2;
      alpha=(1-1/np.sqrt(N))/N;
      e=ehe.EHE();
      e.record_units=True;
      e.simulate_model_mat(np.random.random(2),2800000,alpha*np.array([[1.5,-1],[1,1.3]]),deltaU*2);
      uh = e.unit_hist;
      plt.hist2d(uh[:,0],uh[:,1],bins=1000)#,range=[[0,1],[0,1]])
    #+end_src 

    #+RESULTS[604ff18fb6d0b9cdda055f946a1cf2631c025520]:
    [[file:./images/20170622_123227_1871Jvwe3342r.png]]

     #+begin_src ipython :session master :file ./images/20170622_123227_1871Jvwe3342r42.png :tangle no :cache yes
      ehe = load_module('ehe_detailed')
      N = 2;
      alpha=(1-1/np.sqrt(N))/N;
      e=ehe.EHE();
      e.record_units=True;
      e.simulate_model_mat(np.random.random(2),2800000,alpha*np.array([[-1.5,1],[1,1.3]]),deltaU*2);
      uh = e.unit_hist;
      plt.hist2d(uh[:,0],uh[:,1],bins=1000,range=[[min(uh[:,0].min(),0),1],[min(uh[:,1].min(),0),1]])
    #+end_src 

    #+RESULTS[65a8c38c1308e98ed27fa700368e40f7822e3408]:
    [[file:./images/20170622_123227_1871Jvwe3342r42.png]]



     #+begin_src ipython :session master :file ./images/20170622_123227_1871Jvwe3r21.png :tangle no :cache yes
      ehe = load_module('ehe_detailed')
      N = 2;
      alpha=(1-1/np.sqrt(N))/N;
      e=ehe.EHE();
      e.record_units=True;
      e.simulate_model_mat(np.random.random(2),2800000,alpha*np.array([[1.5,1],[1,1.3]]),deltaU*2,np.array([0]));
      uh = e.unit_hist;
      plt.hist2d(uh[:,0],uh[:,1],bins=1000,range=[[0,1],[0,1]])
    #+end_src 

    #+RESULTS[3022f97efd9a3385bb09f3849e291f95e9eb9f00]:
    [[file:./images/20170622_123227_1871Jvwe3r21.png]]


     #+begin_src ipython :session master :file ./images/20170622_123227_1871Jvwe3r21321.png :tangle no :cache yes
      ehe = load_module('ehe_detailed')
      N = 2;
      alpha=(1-1/np.sqrt(N))/N;
      e=ehe.EHE();
      e.record_units=True;
      e.simulate_model_mat(np.random.random(2),2800000,alpha*np.random.normal(1,1,(2,2)),deltaU*2,np.array([1]));
      uh = e.unit_hist;
      plt.hist2d(uh[:,0],uh[:,1],bins=1000,range=[[0,1],[0,1]])
    #+end_src 

    #+RESULTS[ff058dbe93c5670d33f3d7123495cce586646f47]:
    [[file:./images/20170622_123227_1871Jvwe3r21321.png]]

** Avalanche size distribution for the homogeneous network
    
   As an application of the framework derived in this chapter we see
   how the formula for the probability distributions reduce to the
   avalanche size distribution of the homogeneous EHE model cite:eurich2002finite.

   For the case of \(W^{\text{hom}}_{i,j} = \frac{\alpha}{N}\), \(W\) has rank one
   and the formula for volume of the noninhabited region becomes very
   simple: 
   \[\mathcal{V}(\Lambda(W^{\text{hom}},H) \cap [0,U_i)_{i\in H}) =  \sum_{i=1}^{|H|}\frac{\alpha}{N}\prod_{j\in H,j\neq i}U_j\]

   In particular we have for \(\Lambda(W^{\text{hom}},\mathcal{N}) \)
   \[\mathcal{V}(\Lambda(W^{\text{hom}},\mathcal{N}) =  \alpha \text{ .}\]

   
   Formula [[eqref:eq:vol-r]] for the volume of the region \(R(W^{\text{hom}},\operatorname{av}\) given an avalanche \(\operatorname{av} = (G_i)_{i=1,\ldots,D}\) 
   simplifies to 

   \[\mathcal{V}(R(W^{\text{hom}},(G_i)_{i=1,\ldots,D})) = \Delta U \underbrace{\prod_{i=2}^D \left (\frac{\alpha n_{i-1}}{N}\right )^{n_i}}_{I_1}\underbrace{\left ( \left (1- \frac{\alpha n}{N}\right)^{N-n} - \frac{N-n \alpha}{N}\left(1 -\frac{n\alpha}{N}\right )^{N-n-1}\right )}_{I_2(n)} \text{ , }\]
   where \(n_i = |G_i|, n = \operatorname{size}(\operatorname{av}) = \sum_{i=1}^Dn_i\).
    
   In order to find a closed form expression for 
   \[P(\operatorname{size}(\operatorname{av}) = n) \sim \sum_{\operatorname{av} \in \Omega,\operatorname{size}(\operatorname{av}) = n}\operatorname{Vol}(R_{av}) \]
   note that only the factor \(I_1 \) in \(\operatorname{Vol}(R_{av}) \) depends on the structure \((n_i)_{i=1,\ldots,D} \) of \(\operatorname{av}\). Comparing with the recursion formula derived in cite:eurich2002finite we get 
   \[\sum_{\operatorname{av} \in \Omega,\operatorname{size}(\operatorname{av}) = n}\prod_{i=2}^{D}n_{i-1}(\operatorname{av})^{n_i(\operatorname{av})} = {N \choose n} n^{n-1}\] 
   and 
   \[P(\operatorname{size}(\operatorname{av}) = n) \sim \Delta U I_{2}(n) \left (\frac{\alpha}{N}\right )^{n-1}{N \choose n} n^{n-1} \] \\

   The possibility that the avalanche \(\operatorname{av} \) occurs
   given that the current point in phase space is inside
   \(R(W^{\text{hom}},\operatorname{av})\) is \(1/N\) due to the uniform bernoulli
   measure of the external input. Taken together, this results in
   the (unnormalized) avalanche size distribution for size \(n \geq 1 \):
    
   \begin{align*}
   P(\operatorname{size}(\operatorname{av}) = n) & \sim \frac{1}{N}{N \choose n}
   n^{n-1} \Delta U (\frac{\alpha}{N})^{n-1} \left ( \left (1-
   \frac{n \alpha}{N}\right)^{N-n} - \frac{N-n \alpha}{N}(1 -
   \frac{n \alpha}{N})^{N-n-1}\right ) \\
   &= \frac{1}{N}{N \choose n}
   n^{n-1} \Delta U (\frac{\alpha}{N})^{n-1} \left (1-
   \frac{n \alpha}{N}\right)^{N-n-1}\left (1 - \frac{n\alpha}{N} - \frac{(N-n)\alpha}{N} \right)\\
   &= \frac{1}{N}{N \choose n}
   n^{n-1} \Delta U (\frac{\alpha}{N})^{n-1} \left (1-
   \frac{n \alpha}{N}\right)^{N-n-1}(1 - \alpha) \text{ .}
   \end{align*}
    

   In order to compute the normalization constant explicitely we
   compute the volume of the region not leading to an avalanche at all
   upon receiving external input and the total volume of inhabited
   phase space. For \(W^{\text{hom}}\) equation [[eqref:eq:noavl]] simplifies to

      \(P[\operatorname{size}(\operatorname{av}) = 0] \sim (1-\Delta U) - \frac{N-1}{N}\alpha (1-\Delta U) - \frac{\alpha}{N} \).

   Using this expressions we can normalize the avalanche size distribution for size \(n \geq 1 \) to get
   \begin{align*}
   P(\operatorname{size}(\operatorname{av}) = n) & = \frac{\frac{1}{N}{N \choose n}
   n^{n-1} \Delta U (\frac{\alpha}{N})^{n-1} \left (1-
   \frac{n \alpha}{N}\right)^{N-n-1}(1 - \alpha)}{(1-\alpha) - (1-\Delta U) + \frac{N-1}{N}\alpha (1-\Delta U) + \frac{\alpha}{N}} \\
   & = \frac{\frac{1}{N}{N \choose n}
   n^{n-1} \Delta U (\frac{\alpha}{N})^{n-1} \left (1-
   \frac{n \alpha}{N}\right)^{N-n-1}(1 - \alpha)}{(1-\alpha) - (1-\Delta U) + \alpha - \frac{N-1}{N}\alpha \Delta U} \\
   & = \frac{\frac{1}{N}{N \choose n}
   n^{n-1} \Delta U (\frac{\alpha}{N})^{n-1} \left (1-
   \frac{n \alpha}{N}\right)^{N-n-1}(1 - \alpha)}{\Delta U(1 - \frac{N-1}{N}\alpha)} \\
   & = \frac{1}{N}{N \choose n}
   \left(\frac{n \alpha}{N}\right )^{n-1} \left (1-
   \frac{n \alpha}{N}\right)^{N-n-1}\frac{(1 - \alpha)}{(1 - \frac{N-1}{N}\alpha)} \text{, }
   \end{align*}
    
   which is equal to the formula [[cite:eurich2002finite][Equation C21]]. 
    
** Avalanche size statistics for simple overlap network      :implementation:
    :PROPERTIES:
    :header-args: :tangle src/ehe_ana.py
    :END:
    
   Symbolic algebra implementation: 

   #+begin_src ipython :session master
     import sympy as sym

     def overlap_av_vol(av_ovl,alpha,N,N_sub,deltaU=deltaU):
         D = len(av_ovl)
         ns1 = [step[0] for step in av_ovl]
         ns2 = [step[1] for step in av_ovl]
         no  = [step[2] for step in av_ovl]
         n_s1,n_s2,n_o = sum(ns1),sum(ns2),sum(no)
         N_s1,N_s2,N_o = N
         vol_av = deltaU*prod((alpha/N_sub)**(ns1[i]+ns2[i]+no[i])*(ns2[i-1]+no[i-1])**ns1[i]*
                           (ns2[i-1]+no[i-1])**ns2[i]*(ns1[i-1]+ns2[i-1]+no[i-1])**no[i]for i in range(1,D))
         vol_avc = ((1-(alpha/N_sub)(n_s1+n_o))**(N_s1-n_s1)*
                    (1-(alpha/N_sub)(n_s2+n_o))**(N_s2-n_s2)*
                    (1-(alpha/N_sub)(n_s1+n_s2+n_o))**(N_o-n_o)
                    -(1-(alpha/N_sub)(n_s1+n_o))**(N_s1-n_s1-1)*
                     (1-(alpha/N_sub)(n_s2+n_o))**(N_s2-n_s2-1)*
                     (1-(alpha/N_sub)(n_s1+n_s2+n_o))**(N_o-n_o-1)
                     ,*((alpha/N_sub)*(sum(N)-N_s1+N_s2+N_o)+
                       (alpha/N_sub)**2*(N_s1-n_s1)*(N_s2-n_s2)-
                       (alpha/N_sub)**3*(N_s1-n_s1)*(N_s2-n_s2)*(N_o-n_o))
                     -(alpha/N_sub)*((N_s1-n_s1)*(1-(alpha/N_sub)*(n_s2+n_o))*(1-(alpha/N_sub)*(n_s1+n_s2+n_o))+
                                     (N_s2-n_s2)*(1-(alpha/N_sub)*(n_s1+n_o))*(1-(alpha/N_sub)*(n_s1+n_s2+n_o))+
                                     (N_o-n_o)*(1-(alpha/N_sub)*(n_s1+n_o))*(1-(alpha/N_sub)*(n_s2+n_o))))
         vol_avc = 1
         return vol_av*vol_avc

     def homogen_av_vol(av_hom,alpha,N,deltaU=deltaU):
         D = len(av_hom)
         n = sum(av_hom)
         vol_av = deltaU*prod((alpha/N)**av_hom[i]*av_hom[i-1] for i in range(1,D))
         return vol_av

     def av_hom_patterns(size):
         return {k:v for k,v in  zip(*np.unique([tuple([len(step) for step in sp]) for sp in spiking_patterns(size) ],return_counts=True))}

     class S_bar():
         def __init__(self,N,alpha,k):
             self.s_bars = defaultdict(lambda:None);
             self.N = N
             self.alpha = alpha
             self.k = k

         def s_bar(self,m,l,j):
             m,l,j = tuple(m),tuple(l),tuple(j)
             res = self.s_bars[(m,l,j)]
             if res is not None:
                 return res
             # now calculate s_bar by recursion formula
             js1,js2,jo = j
             ms1,ms2,mo = m
             ls1,ls2,lo = l
             ks1,ks2,ko = self.k
             N,alpha = self.N,self.alpha
             if sum(j) == 0:
                 ns1,ns2,no = ks1+ls1,ks2+ls2,ko+lo
                 res = ((1-(alpha/N)*(ns1+no))**ms1*(1-(alpha/N)*(ns2+no))**ms2*(1-(alpha/N)*(ns1+ns2+no))**mo -
                        (1-(alpha/N)*(ns1+no))**(ms1-1)*(1-(alpha/N)*(ns2+no))**(ms2-1)*(1-(alpha/N)*(ns1+ns2+no))**(mo-1)
                        ,*((alpha/N)*(ms1+ms2+mo)+(alpha/N)**2*ms2*ms2-(alpha/N)**3*ms1*ms2*mo)
                        -(alpha/N)*(ms1*(1-(alpha/N)*(ns2+no))*(1-(alpha/N)*(ns1+ns2+no))
                                    +ms2*(1-(alpha/N)*(ns1+no))*(1-(alpha/N)*(ns1+ns2+no))
                                    +mo*(1-(alpha/N)*(ns1+no))*(1-(alpha/N)*(ns2+no))))
             else:
                 print([(is1,is2,io) for is1 in range(js1+1) for is2 in range(js2+1) for io in range(jo+1) if is1+is2+io > 0])
                 res = sum(sym.binomial(ms1,is1)*sym.binomial(ms2,is2)*sym.binomial(mo,io)*(alpha/N)**(js1+js2+jo)
                           ,*(ls1+lo)**is1*(ls2+lo)**is2*(ls1+ls2+lo)**io
                           ,*self.s_bar((ms1-js1,ms2-js2,mo-jo),(is1,is2,io),(js1-is1,js2-is2,jo-io))
                           for is1 in range(js1+1) for is2 in range(js2+1) for io in range(jo+1) if is1+is2+io > 0)
             self.s_bars[(m,l,j)] = res
             #(ks1+ls1,ks2+ls2,ko+lo)
             return res


     class orig_S_bar():
         def __init__(self,N,alpha,k):
             self.N = N;
             self.alpha = alpha
             self.k = k

         def s_bar(self,m,l,j):
        


   #+end_src 

   #+RESULTS:


   symbolic calculations to probe for closed form solutions
   #+begin_src ipython :session master
     alpha,N,deltaU = sym.symbols('alpha,N,deltaU')

     size = 5

     def factor_hom(s):
         avp = av_hom_patterns(s)
         print(set(avp))
         summed = sym.simplify(sum(homogen_av_vol(avh,alpha,N,deltaU) for avh in avp))
         print(summed)
         return summed/(deltaU*(alpha/N)**(s-1))


     factors = [factor_hom(s) for s in range(1,10)]
     print(factors)
   #+end_src 

   
** Avalanche Statistics                                      :implementation:

   One can show analytically that the EHE Model produces power law
   distributed avalanche sizes. They show up as straight line in a log
   log plot. However, determining the goodness of fit and the exponent
   of a power law from the log log plot alone is not sufficient.
   Instead, the goodness of fit and the best fitting power law
   exponent should be determined by statistical tests.

*** Create avalanche plot                                    :implementation:

    As the avalanche sizes are discrete, the observed measurements can
    simply be transformed to their normalized counts in order to obtain the pdf
   #+begin_src ipython :session master
     def points2pdf(points,lower_limit=1,upper_limit=None,return_unique=True):
         if upper_limit is None:
             upper_limit = max(points)
         unique,counts = np.unique(points,return_counts=True)
         pdf = np.zeros(upper_limit - lower_limit + 1)
         for u,c in zip(unique,counts):
             pdf[u - lower_limit] = c
         if return_unique:
             return (pdf/np.sum(pdf),np.array(list(range(lower_limit,upper_limit+1))))
         return pdf/np.sum(pdf)
   #+end_src

   #+RESULTS:


   #+begin_src ipython :session master
     import matplotlib.pyplot as plt

     def loglogplot(pdf,ax = None, figsize=(6,4),from_pdf=False,label=None):
         if from_pdf:
             norm_counts,unique = pdf
         else:
             points = pdf
             unique,counts = np.unique(points,return_counts=True)
             norm_counts = counts/np.sum(counts)
         if ax is None:
             f,ax = plt.subplots(figsize=figsize)
         ax.loglog(unique,norm_counts,label=label)
         #ax.set_xscale('log',nonposx='mask');
         #ax.set_yscale('log',nonposy='mask');
         #ax.scatter(unique,norm_counts,facecolors='none',edgecolors='b')
         return ax

   #+end_src

   #+RESULTS:


   #+begin_src ipython :session master :file ./images/20170515_115426_5863WcW2341.png :tangle no
     avspdf = pickle.load(open('../avalanches/const_crit_avalanches_10000/avspdf.pickle','rb'))
     #plt.figure(figsize=(10,8))
     #plt.loglog(avspdf[1],avspdf[0])
     loglogplot(avspdf,figsize=(12,8),from_pdf=True)
   #+end_src 

   #+RESULTS:
   [[file:./images/20170515_115426_5863WcW2341.png]]



   #+RESULTS:

** ignore                                             :implementation:ignore:

   Figure [[ref:fig:ehe_crit_avs]] shows the avalanche statistics for the critical ehe model of 10000 units and \(2\times10^8\)  avalanches
   
   #+CAPTION: \label{fig:ehe_crit_avs} avalanche statistics for critical ehe model N=1e4, averaged over 2e8 avalanches
   [[file:./images/20170515_115426_5863WcW2341.png]]
   
*** Recreate EHE Paper plot                                  :implementation:

    The four different dynamical regimes of the model can be
    differentiated with their avalanche distributions.

    The calculation of these distributions takes long time, fortunately python multiprocessing threading
    pool can be used since the c++ extension releases the GIL!

    The distributions will be calculated using the apply_async function which calls a specified callback on completion.
    This helper function stores the value from the callback under a given key in a dict.
    #+begin_src ipython :session master
      def storekey(res,key):
         def inner(value):
             res[key] = value
         return inner

    #+end_src 

    #+RESULTS:

 
    #+begin_src ipython :session master :eval no :tangle no
      #long running (async) calculation
      N = 10000
      recreate_ehe_plot_res = {}
      ehe_arma = load_module('ehe_detailed').EHE()
      futures = [pool.apply_async(ehe_arma.simulate_model_const,(np.random.random(N),10000000,alpha/N,deltaU),
                                   callback=storekey(recreate_ehe_plot_res,key))
                     for alpha,key in [[(0.8,'subcrit'),(0.99,'crit'),(0.999,'supcrit'),(0.9999,'supsupcrit'),(0.99999,'supsupsupcrit')][4]]]
      #avs = np.load("../avalanches/const_supsupcrit_avalanches_10000/avsconcatenated.npy")
      #recreate_ehe_plot_res['supsupcrit'] = (avs,)
      pickle.dump(recreate_ehe_plot_res,open('../avalanches/recreate_ehe_plot_res_supsupsupcrit.pickle','wb'))
    #+end_src 

    #+RESULTS:

    The supsupcritical case takes very long to calculate, therfore, it is evaluated on the cluster
    #+begin_src ipython :session master :tangle no :eval no
      qsub("""
      from ehe import *;
      ehe_arma = load_module('ehe_arma');
      N = 10000
      avs,avd = ehe_arma.simulate_model_const(np.random.random(N),int(1e7),0.99/N,deltaU)
      np.save(outdir+"/avs"+str(task_id),avs)
      np.save(outdir+"/avd"+str(task_id),avd)
      """, name="const_crit_avalanches_10000",servername="server",
           post_command="post_command_concat(outdir+'/avs',range(1,201))",task_ids='1-200')
    #+end_src 


    #+begin_src ipython :session master :tangle no :eval no
      qsub("""
      from ehe import *;
      ehe_arma = load_module('ehe_arma');
      N = 10000
      avs,avd = ehe_arma.simulate_model_const(np.random.random(N),int(1e5),0.9999/N,deltaU)
      np.save(outdir+"/avs"+str(task_id),avs)
      np.save(outdir+"/avd"+str(task_id),avd)
      """, name="const_supsupcrit_avalanches_10000",servername="server",
           post_command="post_command_concat(outdir+'/avs',range(1,101))",task_ids='1-100')
    #+end_src 


    #+begin_src ipython :session master :file :file ./images/20170527_125133_75693WX.png :cache yes :tangle no
    recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else
          pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
    f,axs = plt.subplots(2,2,figsize=(16,12))
    for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])):
       avalanches = recreate_ehe_plot_res[k][0]  
       loglogplot(points2pdf(avalanches),ax,from_pdf=True) 
       ax.set_title(str(k)) 
    #+end_src 

    #+RESULTS[b957441b46584167d9754acc5a5f42516a4e646e]:
    [[file:./images/20170527_125133_75693WX.png]]
    

    
*** Avalanches with constant matrix connectivity

** ignore                                             :implementation:ignore:
   The different dynamical regimes of the model are shown in figure [[ref:fig:recreate-ehe-plot]]
   #+CAPTION: \label{fig:recreate-ehe-plot} Recreation of the plot in \cite{eurich2002finite}.
   [[./images/20170527_125133_75693WX.png]]

* Summary and outlook
  In this thesis we proposed a link between critical dynamics and
  feature integration in the brain. In particular this proposal
  considered networks in which subnetworks representing perceptual
  figures are embedded in such a way that upon activation by external
  input they display critical dynamics, whereas randomly chosen
  subnetworks or the system as a whole stay subcritical.

  We were able to construct an algorithm that generates such a network
  using a generalization of the EHE model. We demonstrated that with
  multiple overlapping networks runaway activity quickly leads to a
  supercritical system showing 'epileptic' oscillatory states. This
  runaway activity could however be successfully contained when
  assigning negative weights to connections between units that do not
  share a common figure network and it was possible to embed a large
  number of figures. On average, a neuron could be connected to
  several figure networks. We analyzed the transition boundary to
  supercriticality and found that the probability of two units sharing
  a common subnetwork correlates very well with the phase transition.

  In this thesis we only considered the problem of embedding multiple
  critical subnetworks of the same size. In this case we could use the
  known critical coupling strength from the homogeneous EHE model
  tuned to the figure size. However, extending this embedding scheme
  alone will be insufficient and strategies have to be devised if two
  figures with different sizes overlap. The framework derived in
  chapter [[ref:chap:gen-ehe]] could in future be applied to this problem.

  Afterwards we tested the separability of the dynamics of activated
  figure (target) networks from activated randomly chosen (background)
  networks in a simulated 2-alternative forced choice task. We found
  that the avalanche size distributions were clearly separate with
  critical distributions arising in the target stimulus. In addition,
  also the temporal dynamics were clearly separate between these two
  activation schemes even when the global firing rates in the two
  stimuli were made equal. A simple coincidence detector could
  distinguish with high accuracy between the target and distractor
  stimuli. These results were checked for robustness against higher
  level of background noise and lower levels of inhibition.

  These results give a clear indication that critical subnetworks
  representing a figure offer good computational abilities for feature
  integration. Again we give as an outlook to go beyond the 2-AFC
  paradigm and to devise mechanism how not just the presence or
  absence of an activated figure network can be detected but also
  readout mechanism that can detect and recognize figure networks
  which become activated.

  At the same time we deepened the mathematical understanding of the
  EHE model in this thesis, proving the ergodicity of the homogeneous
  network assumed in the derivations of the avalanche statistics from
  the volumes of phase space regions where they originate from
  citep:eurich2002finite. We were able to generalize this analysis for
  a large class of non-negative coupling matrices, in effect
  generalizing from one degree of freedom in the coupling matrix to
  \(N^2\). Still we could perfectly identify the non-inhabited region,
  prove that the system acts bijectively on its complement and
  identify the region leading to avalanches. We were able to derive a
  surprisingly structured formula for the volume of the non-inhabited
  region as an alternating sum of subdeterminants which could
  potentially lead to studies of the dependency of the network
  structure on criticality for networks with known spectral graph
  properties.

  We also showed that there is a much simpler point of view to look at
  the generalized EHE model. We were able to completely transform away
  the internal spreading of activation by identifying the boundaries
  of regions leading to avalanches with the assoziated boundaries of
  their images.

  It turned out that this identification leads to a quotient space
  homeomorphic to the N-torus. With respect to this topology the EHE
  model is just a shift in a random positive direction at each time
  step. In future, this connection to a random walk on the torus could
  be helpful to gain deeper insights into the generalized EHE system
  for example about the ergodicity in the non-homogeneous case.

  Even if all regions leading to avalanches and their volume are
  known, it is a long way to go before one can use this to predict
  observed avalanche statistics in big networks. The number of
  possible avalanches in the system scales massively (quicker than the
  factorial function) so that closed form solutions for aggregate
  statistics of interest, for example avalanche size distributions
  have to be found for special cases. A particular simple case would
  be the derivation of the excitatory 2-ovl network, which would give
  theoretical insight into the expected crosstalk between overlapping
  subnetworks.


* Statistical fitting of Power Laws                          :implementation:
  
** Deviation from power law with symmetric KL divergence
  :PROPERTIES:
  :header-args: :tangle src/utils.py
  :END:
    
   cite:eurich2002finite recommend using the symmetric KL divergence
    
   \begin{align}
   \label{eq:symkl}
   K(\alpha) = \sum_L (p(l) - \tilde{p}(l))(\ln(p(l)) - \ln(\tilde{p}(l)))
   \end{align}
   where \(p(l) \) is the empirical pdf of the observed avalanche sizes
   and \(\tilde{p}(l) = \frac{l^{-\frac{3}{2}}}{\sum_{L = 1}^N L^{-\frac{3}{2}}}\).

    
   :implementation:
   #+begin_src ipython :session master
     def sym_kl(pdf,pl_exp,upper_limit=None,lower_limit=None):
         pdf_emp,unq_points = pdf
         lower_limit = unq_points[0] if lower_limit is None else lower_limit
         upper_limit = unq_points[-1] if upper_limit is None else upper_limit
         pdf = np.zeros(upper_limit - lower_limit + 1)
         for pe,up in zip(pdf_emp,unq_points): 
             if 0 <= up - lower_limit < len(pdf):
                 pdf[up - lower_limit] = pe
         pdf[pdf != 0] /= np.sum(pdf)        
         pdf_pl,unq_true = discrete_power_law_dist(pl_exp,
                                                   lower_limit=lower_limit,
                                                   upper_limit=upper_limit)
         tmp = (pdf - pdf_pl)*(np.log(pdf)-np.log(pdf_pl))
         tmp[~np.isfinite(tmp)] = 0
         return np.nansum(tmp)

     def sym_kl2(pdf1,pdf2,upper_limit=None,lower_limit=None): 
         pdf1_emp,unq_points1 = pdf1
         pdf2_emp,unq_points2 = pdf2         
         lower_limit = min(unq_points1[0],unq_points2[0]) if lower_limit is None else lower_limit
         upper_limit = max(unq_points1[-1],unq_points2[-1]) if upper_limit is None else upper_limit
         # populate distribution vectors
         pdf1 = np.zeros(upper_limit - lower_limit + 1)
         for pe1,up1 in zip(pdf_emp1,unq_points1):
             if 0 <= up1 - lower_limit < len(pdf):
                 pdf1[up1 - lower_limit] = pe         
         pdf2 = np.zeros(upper_limit - lower_limit + 1)
         for pe2,up2 in zip(pdf_emp2,unq_points2):
             if 0 <= up2 - lower_limit < len(pdf):
                 pdf2[up1 - lower_limit] = pe
         tmp = (pdf1 - pdf2)*(np.log(pdf1)-np.log(pdf2))
         tmp[~np.isfinite(tmp)] = 0
         return np.nansum(tmp)    

     def sym_kl_old(pdf,pl_exp,N):
         pdf,unq_points = pdf
         pdf_pl,unq_true = discrete_power_law_dist(pl_exp,lower_limit=1,upper_limit=unq_points[-1])
         tmp = (pdf - pdf_pl)*(np.log(pdf)-np.log(pdf_pl))
         tmp[~np.isfinite(tmp)] = 0
         return np.nansum(tmp)

     def discrete_power_law_dist(pl_exp,lower_limit=1,upper_limit=10000):
         """limits are inclusive"""
         unique = list(range(lower_limit,upper_limit+1))
         pdf_pl = np.array([np.power(l,pl_exp) for l in unique])
         return (pdf_pl/np.sum(pdf_pl),np.array(unique))
   #+end_src 

   #+RESULTS:
   :END:
    
*** Exploration                                                 :exploration:
    On the 12e6 sampled critical points this produces
    #+begin_src ipython :session master :file ./images/20170527_125353_7569Ehd.png :tangle no :cache yes
    avs = avs if 'avs' in globals() else pickle.load(open('../avalanches/ehe_const_crit_10000_2','br'))
    exps = -3/2 + np.linspace(-1/10 ,1/10,10) # 
    avs_pdf = points2pdf(avs)
    dists = [sym_kl(avs_pdf,exp,10000) for exp in exps]
    plt.plot(exps,dists)
    #+end_src 

    #+RESULTS[4b847f164c1d5eb217e96e2935f0b296dbd7d8c1]:
    [[file:./images/20170527_125353_7569Ehd.png]]


    For reference, the generated power laws 

*** Comparison with Normal KL Divergence                        :exploration:

    #+begin_src ipython :session master
      def kl(pdf,pl_exp,N):
          pdf,unq_points = pdf
          pdf_pl,unq_true = discrete_power_law_dist(pl_exp,lower_limit=1,upper_limit=unq_points[-1])
          tmp = pdf_pl*(np.log(pdf)-np.log(pdf_pl))
          tmp[~np.isfinite(tmp)] = 0
          return -np.nansum(tmp)

    #+end_src 

    #+RESULTS:

    #+begin_src ipython :session master :file ./images/20170527_130157_7569e1p.png :tangle no :cache yes
    avs = avs if 'avs' in globals() else pickle.load(open('../avalanches/ehe_const_crit_10000_2','br'))
    exps = -3/2 + np.linspace(-1/10 ,1/10,10) 
    avs_pdf = points2pdf(avs)
    dists = [kl(avs_pdf,exp,10000) for exp in exps]
    plt.plot(exps,dists)
    #+end_src 

    #+RESULTS[db14fe4b875a9e2bf4a79f8551a27cb04cc33562]:
    [[file:./images/20170527_130157_7569e1p.png]]

    Doesn't show much difference
  
*** Symmetric KL distances on the different regimes          :implementation:

    #+begin_src ipython :session master :file ./images/20170604_235118_1882Jr134.png :tangle no :cache yes
      recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
                              pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
      f,axs = plt.subplots(2,2,figsize=(16,12))
      for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])):
          avalanches = np.array(recreate_ehe_plot_res[k][0])
          dist = sym_kl(points2pdf(avalanches),-3/2,10000)
          loglogplot(avalanches,ax)
          ax.set_title('sym kl distance: '+str(dist))
    #+end_src 

    #+RESULTS[169112fe091bfcafbe56518c7eb1fe9f98e34a64]:
    [[file:./images/20170604_235118_1882Jr134.png]]

    #+begin_src ipython :session master :file ./images/20170604_235118_1882Jr1.png :tangle no :cache yes
      recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
                              pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
      f,axs = plt.subplots(2,2,figsize=(16,12))
      for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])):
          avalanches = np.array(recreate_ehe_plot_res[k][0])
          dist = sym_kl(points2pdf(avalanches),-3/2-0.1,10000)
          loglogplot(avalanches,ax)
          ax.set_title('sym kl distance: '+str(dist))
    #+end_src 

    #+RESULTS[854541a6e9bda334b2ce3e61579114dd2e618ca3]:
    [[file:./images/20170604_235118_1882Jr1.png]]

** ignore                                                            :ignore:
   
   See figure [[ref:fig:sym-kl-distances]] for values of the symmetric KL
   distance for the different regimes. This distance is only small (<
   0.005) for the critical case, and this only for exponent 1.5. With exponent
   1.6, the sym kl distance for the critical case is short below 0.05.

   While the symmetric KL distance gives a measure of the divergence
   between probability distributions it doesn't tell from which value
   on the fit can be considered good enough. An alternative is to use
   a non parametric goodness of fit procedure (like a Kolmogorov-Sinai
   test) after fitting the power law parameters with the maximum
   likelihood method and calculating p values using synthetic data sets.
   This approach will be followed in the next sections.

   #+CAPTION: \label{fig:sym-kl-distances} Symmetric KL distances from discrete power law from 1 to 10000 for the different regimes
    [[file:./images/0170604_235118_1882Jr134.png]]
** Approaches to estimate and fit power laws from data
*** Kappa measure
    
    Instead of applying a KS distance to the pdf,
    cite:shew2009neuronal recommend using
    \begin{align}
    \label{eq:kappa}
    \kappa = 1 + \frac{1}{m}\sum_{k=1}^m(F^{\text{NA}}(\beta_k) - F(\beta_k) \text{, }
    \end{align}
    where \(\beta \) are the avalanche sizes, \(F\) the empirical cdf
    of observed avalanche sizes and and \(F^{\text{NA}} \) the
    theoretical reference CDF 
    \[F^{\text{NA}}(\beta) = \frac{1 - \sqrt{\frac{l}{\beta}}}{1- \sqrt{\frac{l}{L}}}\]
    corresponding a power law distribution between cut offs \(l,L \).
    \[p^{\text{NA}}(\beta) = \mathds{1}[l \leq \beta \leq L] \frac{\sqrt{l}\beta^{-\frac{3}{2}}}{2(1 - \sqrt{\frac{l}{L}})}\]
    
    However, this kappa measure is not a mathematical sound way to
    test for power laws as it can be zero also for distributions which
    have great deviations from a power law but cancel out. It could be
    used however to indicate after the goodness of fit test rejected a
    power law fit whether the sampled distribution represents the sub-
    or supercritical dynamical regime.

  
*** Method from Clauset et al
    A more principled mathematical Method to estimate power law
    parameters and evaluate the goodness of fit is the method by TODO
    ZITE: Clauset et al. It works by first estimating the lower cutoff
    via maximum likelihood and testing the (continuous) power law with
    the nonparametric Kolmogorov Smirnov test. As the slope and lower
    cutoff of the null hypothesis distribution is estimated from the
    data, the distribution of the KS-distance is approximated by a
    particular semi-parametric bootstrap method. 

    However, it has been shown that with this method the goodness of
    fit of data generated from subcritical distributions can be better
    than data sampled from a real power law with upper cutoff, which
    is the case for the avalanche distributions studied in this work.
*************** TODO Cite papers
*************** END

    The Method from Clauset et al. has recently been extended to
    estimate an upper cutoff as well and the bootstrap process to
    estimate the KS-distance modified by the method from Corral and
    Deluca, which will be used (with slight generalizations) in this
    work.

** Method from Corral and Deluca
   :PROPERTIES:
   :header-args: :tangle src/plfit_cd.py
   :END:
   
   
   :implementation:
   #+begin_src ipython :session master
   import matplotlib.pyplot as plt
   import numpy as np
   import pickle
   #+end_src

   #+RESULTS:

   :END:
 

    Systems of finite size can never show perfect power laws, which
    are only expected in the limit of large system sizes. Therefore,
    one has to deal with lower and upper cut-offs for region where the
    finite size system exhibits power law behavior. A method to
    statistically test for power laws in a certain range and to fit the
    lower and upper cutoff as well as the exponent was described by cite:deluca2013fitting.

    The probability density function for a power law in the range \([l,L]\) and exponent \(\alpha > 0 \) is given by

    \begin{align}
    f(x) = \begin{cases}
             \frac{\alpha - 1}{l^{1 - \alpha} - L^{1 - \alpha}} x^{-\alpha} &\mbox{ if } \alpha \neq 1 \\
             \frac{1}{x\ln{\frac{L}{l}}} &\mbox{ if } \alpha = 1 \end{cases} 
    \end{align}

    :implementation:
    #+begin_src ipython :session master
      def f_pl(alpha,l,L):
          def f_not_1(x):
              range_mask = ((x >= l) & (x <= L))
              ret = np.zeros(x.shape)
              ret[range_mask] = ((alpha - 1)/(np.power(l,1-alpha) - 
                      np.power(L,1-alpha)) * np.power(x[range_mask],-alpha))
              return ret
          def f1(x):
              range_mask = ((x >= l) & (x <= L))
              ret = np.zeros(x.shape)
              ret[range_mask] = 1/x[range_mask]*np.log(L/l)
              return ret
          return f_not_1 if alpha != 1 else f1

    #+end_src

    #+RESULTS:

    :END:
 

    with complementary cumulative density function \(S(x) = P[X > x] \)
    
    \begin{align}
    S(x) = \begin{cases}
           \frac{x^{1-\alpha} - L^{1 - \alpha}}{l^{1 - \alpha} - L^{1 - \alpha}} &\mbox{ if } \alpha \neq 1 \\
           \frac{\ln(\frac{L}{x})}{\ln(\frac{b}{l})} &\mbox{ if } \alpha = 1
           \end{cases}
    \end{align}

    :implementation:
    #+begin_src ipython :session master
      def S_pl(alpha,l,L):
          def S_not_1(x):
              range_mask = ((x >= l) & (x <= L))
              ret = np.zeros(x.shape)
              ret[range_mask] = ((np.power(x[range_mask],1-alpha)-np.power(L,1-alpha))/(np.power(l,1-alpha)-np.power(L,1-alpha)))
              ret[x < l] = 1
              return ret
          def S1(x):
              range_mask = ((x >= l) & (x <= L))
              ret = np.ones(x.shape)
              ret[range_mask] = (np.log(L) - np.log(x))/(np.log(L) -np.log(l))
              ret[x > L] = 0
              return ret
          return S_not_1 if alpha != 1 else S1
    #+end_src

    #+RESULTS:

    :END:

    Sample draws from this distribution can be created with the
    inversion method by transforming uniformely distributed values \(U
    \sim \operatorname{UNI}[0,1]\) by 
    \begin{align}
    I(u) = \frac{l}{(1 - (1 - (l/L)^{\alpha - 1})u)^{\frac{1}{\alpha-1}}} \text{ .}
    \end{align}
    I(U) then follows the truncated power law distirbution
    
    :implementation:
    #+begin_src ipython :session master
    def draw_sample(l,L,alpha,N=10000):
          u = np.random.random(N)
          return l/np.power(1 - (1 - np.power(l/L,alpha-1))*u,1/(alpha-1))
    #+end_src 

    #+RESULTS:
    :END:

    If the observable one measures has only integer values, it should
    be tested if the follow the  power-law distribution

    \begin{align}
    f(n) = \mathds{1}_{l \le n \le L}\frac{n^{-\alpha}}{\sum_{k = 1}^L(k+l)^{-\alpha}}\text{, }
    \end{align}
    with lower and upper cutoffs \(l,L\). For \(b = \infty \) the
    normalizing factor is known as the Hurwitz zeta function \(\zeta(\alpha,a) \).
    
    :implementation:
    #+begin_src ipython :session master
      from scipy.special import zeta

      def hurwitz_zeta(alpha,l,L,return_cumsum=False):
          """l==np.Inf is the actual hurwitz zeta"""
          if (L-l) < 1e3 or return_cumsum:
              if L == np.Inf:
                  L = 1e5
              return (np.cumsum(np.power(np.arange(l,L+1),-alpha)) if return_cumsum else
                      np.sum(np.power(np.arange(l,L+1),-alpha)))
          else: # use scipy.special implementation which uses euler-mclaurin
              return zeta(alpha,l) - zeta(alpha,L) #zeta handles np.Inf


      def discrete_f_pl(alpha,l,L):
          def d_f_pl(x):
              x = np.array(x)
              # x has to be vector of ints
              range_mask = ((x >= l) & (x <= L))
              ret = np.zeros(x.shape)
              norm_const = 1/hurwitz_zeta(alpha,l,L)
              ret[range_mask] = norm_const*np.power(x[range_mask],-alpha)
              return ret
          return d_f_pl
    #+end_src

    #+RESULTS:

    :END:

    The discrete power law comparative cumulative density function is given by
    :implementation:
    #+begin_src ipython :session master
      def discrete_S_pl(alpha,l,L):
          def d_S_pl(x):
              x = np.array(x)
              range_mask = ((x > l) & (x <= L)) # > important for cumsum index
              ret = np.ones(x.shape)
              hurwitz_cumsum = hurwitz_zeta(alpha,l,L,return_cumsum=True)
              zeta = hurwitz_cumsum[-1]
              hinds = (x[range_mask]-l-1).astype(int)
              ret[range_mask] = (zeta - hurwitz_cumsum[hinds])/zeta
              ret[x > L] = 0
              return ret;
          return d_S_pl
    #+end_src

    #+RESULTS:

    :END:

    :exploration:
    #+begin_src ipython :session master :file ./images/20170528_175936_432273V.png :cache yes :tangle no
      plt.figure(figsize=(12,4))
      plt.subplot(121)
      x = np.arange(-1,1005)
      plt.loglog(x,discrete_f_pl(3/2,1,1000)(x))
      plt.subplot(122)
      plt.plot(x,discrete_S_pl(3/2,1,1000)(x))
      plt.xlim([0,100])
    #+end_src 

    #+RESULTS[01430681a836b9210ddcd079014e727efe09734c]:
    [[file:./images/20170528_175936_432273V.png]]

    :END:

    
*************** TODO Zite results Hurwitz zeta
*************** END


    Discrete power laws are simulated with an extension to the
    independence sampler described in link-to-cd-arxiv.
    
*************** TODO Zite arxiv paper from corral deluca
*************** END

    
    :implementation:
    #+begin_src ipython :session master
      def draw_sample_discrete(l,L,alpha,N=1000):
          ret = np.zeros(N,dtype='i')
          beta = alpha-1
          umax = np.power(1/l,beta)
          umin = np.power(1/L,beta)
          f = discrete_f_pl(alpha,l,L)
          q = lambda y: np.power(l/y,beta) - np.power(l/(y+1),beta)
          #print(umax)
          for i in range(N):
              numit = 0
              while(numit<100):
                  u = np.random.rand()*(umax-umin) + umin
                  v = np.random.rand()
                  y = np.floor(1/np.power(u,1/beta))
                  tau = np.power(1 + 1/y,beta)
                  b = np.power(l + 1,beta)
                  #print(v*y*(tau - 1)/(b - np.power(l,beta)),(l*tau)/b)
                  if v <= (np.power(y,-alpha)*q(l))/(np.power(l,-alpha)*q(y)):
      #            if v*y*(tau - 1)/(b - np.power(l,beta)) <= (l*tau)/b:
                      break
                  numit += 1
              if(numit == 100):
                  print('did not break')
              ret[i] = y
              # print(numit)
          return ret

    #+end_src 

    #+RESULTS:


    #+begin_src ipython :session master :file ./images/20170603_132944_1873r323.png :tangle no
      l=1;L=10000;alpha=3/2;N=1000;
      sample = draw_sample_discrete(l,L,alpha,1000000)[10000:]
      unique,counts = np.unique(sample,return_counts=True)
      norm_counts = counts/np.sum(counts)
      plt.loglog(unique,norm_counts)
      #loglogplot(sample)
    #+end_src 

    #+RESULTS:
    [[file:./images/20170603_132944_1873r323.png]]

    Check for convergence of the sampled discrete power laws
    #+begin_src ipython :session master :eval no :tangle no
      qsub("""
      np.save(outdir+"/samples"+str(task_id),draw_sample_discrete(1,10000,3/2,int(1e7)))
      """,name='sample_convergence',task_ids='1-100')
    #+end_src 

    #+begin_src ipython :session master :file ./images/20170606_124407_3928Pq0.png :tangle no :eval no
    samples = np.load('../avalanches/sample_convergence/samples1.npy')
    #p,dists = KS_p_value(0.2,3/2,1,10000,10000,sample_func=draw_sample_discrete,num_samples=1000,
    #                     null_ccdf=discrete_S_pl,return_dists=True)
    sliced_p_values = [dist_KS(3/2,1,10000,samples[(10000*i):(10000*(i+1))],null_ccdf=discrete_S_pl) for i in range(0,99)]
    max_dev,S,S_e,unique_s = dist_KS(3/2,1,10000,samples,null_ccdf=discrete_S_pl,return_ccdf=True)
    plt.plot(sliced_p_values)
    plt.plot(list(range(10)),np.ones((10))*np.mean(dists))
    #+end_src 


    #+RESULTS:
    [[file:./images/20170606_124407_3928Pq0.png]]

    
    #+begin_src ipython :session master :file ./images/20170606_142409_3928qUk.png :tangle no
    samples = np.load('../avalanches/sample_convergence/samples1.npy')
    plt.figure(figsize=(12,8))
    ax = plt.gca()
    loglogplot(samples,ax=ax)
    loglogplot((discrete_f_pl(3/2,1,10000)(np.arange(1,10001)),np.arange(1,10001)),from_pdf=True,ax=ax)

    #+end_src 

    #+RESULTS:
    [[file:./images/20170606_142409_3928qUk.png]]

    :END:

    Figure [[ref:fig:discrete-sample-1e7]] compares the analytical density
    $f(x)$ against the empirical density of 10 million samples from a
    truncated power law with parameters \(\alpha=\frac{3}{2},l=1,L=10000\).

    #+CAPTION: \label{fig:discrete-sample-1e7} Comparison of the empirical pdf of a truncated discrete power law \(\alpha=\frac{3}{2},l=1,L=10000 \) with the analytical pdf
    [[file:./images/20170606_142409_3928qUk.png]]


    In order to assess whether the given data follow this
    distribution, a nonparametric goodness of fit test is used. These
    statistical tests measure the deviation between the empirical
    process \(S_n(x) = \frac{\#\text{measurements} >
    x}{\#\text{measurements}}\) and \(S(x)\). But before these tests
    can be applied, the parameters \(l,L,\alpha \) of \(S\) have to be fitted.

*** Maximum likelihood estimation
    This is achieved by deriving a maximum likelihood estimator, which
    is guaranteed to exist and to be unique in regular exponential
    families, of which a truncated power law is an example cite:deluca2013fitting.
    
*************** TODO Cite also other sources about the theory of statistics blabla maybe dickhaus
*************** END
    
    Assuming for the moment that the parameters \(l,L \) are already
    determined and that the datapoints were drawn independently and
    identically distributed with \(f_{l,L,\alpha}\), the log
    likelihood of this sample in dependence of \(\alpha \) is given by
    
    \begin{align}
    \label{eq:log-like}
    l(\alpha) = 
\begin{cases}
\ln{\frac{\alpha - 1}{1 - r^{\alpha - 1}}} - \alpha \ln{\frac{g}{l} - \ln{l}} &\mbox{if } \alpha \neq 1 \\
-\ln \ln \frac{1}{r} - \ln g &\mbox{if } \alpha = 1 \text{ ,}
\end{cases}
    \end{align}
    
    :implementation:
    #+begin_src ipython :session master
      def log_likelihood(l,L,x,dist_to_1=1e-6):
          r = l/L
          range_mask = ((x >= l) & (x <= L))
          samples_in_range = x[range_mask]
          log_g = np.sum(np.log(samples_in_range))/np.shape(samples_in_range)[0]
          def ll(alpha):
              if np.abs(alpha-1) > dist_to_1:
                  return (np.log(alpha-1) - np.log(1 - np.power(r,alpha-1)) -
                          alpha*(log_g - np.log(l)) - np.log(l))
              else:
                  return -np.log(-np.log(r)) - log_g
          return ll
    #+end_src
    #+RESULTS:

    :END:

    where the data enter via their geometrical mean \(\ln g =
    \frac{\sum_1^N \ln x_i}{N} \) and \(r = \frac{l}{L}\) and \(l \neq
    0 \). The parameter \(\alpha_e \) of the maximum likelihood
    estimator is the unique solution to \(\frac{dl(\alpha)}{d\alpha} =
    0\). For a non-truncated power law (\(r = 0 \)) the solution is
    \[\alpha_{e } = 1 + \frac{1}{\ln(\frac{g}{l})}\]

    while for general
    truncated power laws, no closed form solution is known. Instead,
    \(\alpha_e \) is obtained by numerically maximizing \(l(\alpha) \).

    For the discrete case, the log likelihood s given by
    \begin{align}
    l(\alpha) = \frac{1}{N}\sum_{i=1}^N \ln{f(n_i)} = -\ln{\zeta(\alpha,a)} - \alpha*\ln{g}
    \end{align}
 
    

    :implementation:
    #+begin_src ipython :session master
      def discrete_log_likelihood(l,L,x):
          samples_in_range = x[(x >= l) & (x <= L)]
          log_g = np.sum(np.log(samples_in_range))/np.shape(samples_in_range)[0]
          def ll(alpha):
              return - np.log(hurwitz_zeta(alpha,l,L)) - alpha * log_g
          return ll
    #+end_src 

    #+RESULTS:
    :END:

*** Exploration                                  :exploration:implementation:
    The cutted of power law distributions look like 
    #+begin_src ipython :session master :tangle no :file ./images/20170520_165758_1869VZP3.png :tangle no :cache yes
    r = np.linspace(1,100,5e4)
    pl_density = f_pl(3/2,1,100)
    probs = pl_density(r)
    assert(abs(np.trapz(probs,dx = r[1]-r[0])-1) < 1e-6)
    loglogplot((probs,r))
        #+end_src 

        #+RESULTS[382e1638ec70eeecdb8960cf6a8837afa748bf49]:
        [[file:./images/20170520_165758_1869VZP3.png]]


    with complementary cumulative density 
    #+begin_src ipython :session master :file ./images/20170520_170218_1869ijV22.png :tangle no  :eval no
    pl_ccdf = S_pl(3/2,1,100)
    plt.plot(r,pl_ccdf(r))
    #+end_src 

    #+RESULTS:
    [[file:./images/20170520_170218_1869ijV22.png]]

    and log likelihood
    #+begin_src ipython :session master :file ./images/20170520_170505_1869vtb4323.png :tangle no
    sample = draw_sample(1,100,3/2)
    log_like = log_likelihood(1,100,sample )
    alphas = np.linspace(1,2,100)
    res = [log_like(alpha) for alpha in alphas]
    plt.plot(alphas,res)
    #+end_src 


    #+RESULTS:
    [[file:./images/20170520_170505_1869vtb4323.png]]

    #+begin_src ipython :session master :file ./images/20170520_170505_1869vtbkj4.png :cache yes :tangle no
    sample = draw_sample(1,100,3/2)
    avs = avs if 'avs' in globals() else pickle.load(open('../avalanches/ehe_const_crit_10000_2','br'))
    log_like = log_likelihood(10,10000,np.array(avs))
    alphas = np.linspace(1.47,1.78,1000)
    res = [log_like(alpha) for alpha in alphas]
    plt.plot(alphas,res)
    from scipy.optimize import minimize_scalar
    min_r = minimize_scalar(lambda x:-log_like(x))
    plt.title(alphas[np.argmax(res)])
    
    #+end_src 

    #+RESULTS[daa8640ebf181991954b1e251ae6226df3816961]:
    [[file:./images/20170520_170505_1869vtbkj4.png]]

    Log likelihood for the discrete case: 

    #+begin_src ipython :session master :file ./images/20170603_133440_1873s_x.png :tangle no
    sample = draw_sample_discrete(1,10000,6.0,N=100000)
    log_like = discrete_log_likelihood(1,10000,sample)
    alphas = np.linspace(1,7,1000)
    res = [log_like(alpha) for alpha in alphas]
    plt.plot(alphas,res)
    #+end_src 

    #+RESULTS:
    [[file:./images/20170603_133440_1873s_x.png]]

*************** TODO debug why mlfit doesn't work with discrete_log_likelihood
*************** END

    #+begin_src ipython :session master :file ./images/20170603_133523_1873eJB.png :tangle no
      sample = np.array(avs if 'avs'  in globals() else pickle.load(open('../avalanches/ehe_const_crit_10000_2','br')))
      log_like = discrete_log_likelihood(1,10000,sample)
      alphas = np.linspace(1,2,100)
      res = [log_like(alpha) for alpha in alphas]
      plt.plot(alphas,res)
    #+end_src 

    #+RESULTS:
    [[file:./images/20170603_133523_1873eJB.png]]


*** Computing the maximum likelihood exponent                :implementation:
    
    #+begin_src ipython :session master
      from scipy.optimize import minimize_scalar
      def ml_fit(sample,l,L,log_like=log_likelihood,bounds=[1,10]):
          log_like = log_like(l,L,sample)
          res = minimize_scalar(lambda x:-log_like(x),method='Bounded',bounds=bounds)
          assert(res.success)
          return res.x
    #+end_src 

    #+RESULTS:


    To test the maximum likelihood estimation, it is ested against
    simulated data with the same exponent
    #+begin_src ipython :session master :results output silent :tangle no :exports code
      l = 1;L=10000
      for i in np.linspace(1.3,10,30):
         fitted = ml_fit(draw_sample(l,L,i,N=L*10),l,L)
         print(fitted,i)
         assert(abs(fitted-i) < 0.1)
    #+end_src 


    The same goes for discrete power laws
*************** TODO Here 2.6183 is the biggest exponent that comes out of discrete_maximum likelihood
*************** END

        #+begin_src ipython :session master :results output :tangle no :exports code :cache yes
          l = 1;L=1000
          for i in np.linspace(1.05,2,10):
             fitted = ml_fit(draw_sample_discrete(l,L,i,N=L*10),l,L,log_like=discrete_log_likelihood)
             assert(abs(fitted-i) < 0.1)
    #+end_src 

    #+RESULTS[455a0a176f2277f7ecf6aec342815ed0b5ee4b07]:

*** Testing Goodness of fit   
    As apparent in equation [[eqref:eq:log-like]] , every sample with the
    same geometric mean would lead to the same log likelihood
    function. To assess that the sample distribution realy resembles a
    power law distribution in the given range, the goodness of fit is
    measured by the Kolmogorov-Smirnov test.

    It quantifies the deviation of the (complementary) cumulative
    density function estimated from the samples from the ccdf of the
    power law distribution. 

    \begin{align}
    d_e = \max_{l \leq x \leq L}{\left | S(x) - S_e(x) \right |}
    \end{align}
    The empirical cumulative density function
    is an unbiased estimator of the true cumulative density function
    of the data generating proces and is simply given by \(S_e(x) = \frac{\#\text{measurements} >
    x}{\#\text{measurements}}\).
    
    The KS-distance \(d_e \) is easy to evaluate on empirical data
    because \(S_e(x)\) is constant between values occuring in the
    sample and the maximal deviation from \(S(x)\) therefore can only
    occur at a value of \(x \) that are in the sample.

    :implementation:
    #+begin_src ipython :session master
      def dist_KS(alpha,l,L,sample,null_ccdf=S_pl,return_ccdf=False):
          ccdf_pl = null_ccdf(alpha,l,L)
          sample = sample[((sample >= l) & (sample <= L))]
          print('start building empirical ccdf')
          unique,count = np.unique(sample,return_counts=True)
          sort_idx = np.argsort(unique)
          unique_s,count_s = (unique[sort_idx],count[sort_idx])
          cum_count = np.cumsum(count_s)
          S_e = np.ones(cum_count.shape[0])
          S_e[1:] = 1 - cum_count[1:]/cum_count[-1] # < und <= verwechselt kann sein
          print('build null ccdf')
          S = ccdf_pl(unique_s)
          print('calc max dev')
          max_dev = np.max(np.abs(S - S_e))
          if return_ccdf:
              return (max_dev,S,S_e,unique_s)
          else:
              return  max_dev
    #+end_src 

    #+RESULTS:
    :END:

    
    Although there are results for the distribution of the
    Kolmogorov-Smirnov distance that can be used to derive confidence
    intervals and \(p-\)values, these results are no longer valid in
    the case considered here, where the parameters of the true
    distribution are also estimated from the sample. Nonetheless, this
    distribution can be efficiently estimated numerically using a
    monte carlo technique.

    To test whether the KS-distance obtained from the sample with
    estimated parameters \(l,L,\alpha \) and sampling size \(N \),
    generate many synthetic samples of size \(N\) according to null
    hypothesis distribution \(\operatorname{pl}(l,L,\alpha)\) and
    calculate the KS-distance for each sample. The probability \(p\)
    under the null hypothesis, that the KS-distance is bigger than
    \(d_e \) is then estimated simply by the fraction of KS-distances
    on the synthetic data sets that are bigger than \(d_e \). with the
    \(p \) value defined in this way, the bigger the \(p \) value, the
    better the fit to the power law distribution.
    cite:deluca2013fitting propose to reject the null hypothesis if \(p < 0.05 \).

    :implementation:
    #+begin_src ipython :session master
      def KS_p_value(d_e,alpha,l,L,N,sample_func=draw_sample,num_samples=1000,null_ccdf=S_pl,return_dists=False,use_dists=None):
          distances = (np.array([dist_KS(alpha,l,L,sample_func(l,L,alpha,N),null_ccdf=null_ccdf) 
                       for _ in range(num_samples)]) if use_dists is None else use_dists)
          print('distances_calculated')
          if not(return_dists):
              return len(distances[distances > d_e])/num_samples
          else:
              return (len(distances[distances > d_e])/num_samples,distances)

      def p_value(d,distances):
          return len(distances[distances <= d])/len(distances)
    #+end_src 

    #+RESULTS:
    :END:


    :exploration:
    
    First, do the lengthy calculation of the p value
    #+begin_src ipython :session master :tangle "1e7_p_value.py" :eval no
      import pickle
      from plfit_cd import *
      p,distances = KS_p_value(0.2,3/2,1,10000,int(1e7),return_dists=True)
      pickle.dump(distances,open("../avalanches/distances1e7l1L1e4e1,5.pickle","wb"))
    #+end_src 

    #+END_SRC
    
    #+begin_src ipython :session master :file ./images/20170521_151136_1869oME32.png :cache yes :tangle no
    avs = avs if 'avs' in globals() else pickle.load(open('../avalanches/ehe_const_crit_10000_2','br')) 
    alpha,l,L,N = (3/2,2,10000,len(avs))
    d,S,S_e,unique_s = dist_KS(alpha,l,L,np.array(avs),return_ccdf=True)
    plt.figure(figsize=(20,10))
    plt.plot(unique_s,S)
    plt.plot(unique_s,S_e)
    plt.xlim([0,100])
    #p = KS_p_value(d,alpha,l,L,N)
    plt.title("d_e "+str(d))#+" with p value " + p)
    #+end_src 

    #+RESULTS[bb565cbe0c0738a19ba83d135c1ddd6e9b2cfd37]:
    [[file:./images/20170521_151136_1869oME32.png]]


    #+begin_src ipython :session master :file ./images/20170521_152601_1869CoE.png :cache yes :tangle no
      alpha,l,L,N = (3/2,1,100,100)
      d,S,S_e,unique_s = dist_KS(alpha,l,L,draw_sample(l,L,alpha,N=100),return_ccdf=True)
      p = KS_p_value(d,alpha,l,L,N)
      plt.figure(figsize=(16,8))
      plt.subplot(121) 
      plt.title("d_e "+ str(d) + " with p value " + str(p))
      plt.plot(unique_s,S)
      plt.plot(unique_s,S_e)
      plt.xlim([0,100])
      plt.subplot(122)
      d,S,S_e,unique_s = dist_KS(alpha,l,L,draw_sample_discrete(l,L,alpha,N=100),return_ccdf=True)
      p = KS_p_value(d,alpha,l,L,N,sample_func=draw_sample_discrete,null_ccdf=discrete_S_pl)
      plt.title("d_e "+ str(d) + " with p value " + str(p))
      plt.plot(unique_s,S)
      plt.plot(unique_s,S_e)
      plt.xlim([0,100])
    #+end_src 

    #+RESULTS[6df935125f338939c71d05035887b294aab17a0b]:
    [[file:./images/20170521_152601_1869CoE.png]]


    :END:


    

    Figure [[ref:fig:ds-synthetic]] shows the KS distance for a syntethic
    sample of size 100 drawn from a \(\frac{3}{2}\) power law between
    \(1 \leq x \leq 100 \) and compares the empirical comparative
    cumulative density function with the analytical ccdf. The left
    plot shows the continuous case while the discrete power law ccdf
    and the sampled ccdf are compared in the right plot. The resulting
    \(p-\)value was calculated using 1000 generated samples.

    #+CAPTION: \label{fig:ds-synthetic} KS-Distance and comparison of the ccdf for continuous and discrete power laws.
    [[./images/20170521_152601_1869CoE.png]]



    
    
   :implementation:
   Completing this section, here is the plfit_cd method which returns
   for a given cdf the best matching power law exponent and it's p
   value. Instead of estimating the best matching exponent it can be
   specified by the user and the p value used to assess goodness of fit.

   #+begin_src ipython :session master
     def plfit_cd(sample,l,L,fix_exponent=None,use_dists=None,num_samples=1000,return_all=False,
                  log_like=log_likelihood,bounds=[1,10],null_ccdf=S_pl,sample_func=draw_sample):
         exponent = (ml_fit(sample,l,L,log_like=log_like,bounds=bounds)
                     if fix_exponent is None else fix_exponent)
         d_ks,S,S_e,unique_s = dist_KS(exponent,l,L,sample,null_ccdf=null_ccdf,return_ccdf=True)
         print('distks',d_ks)
         p_value = KS_p_value(d_ks,exponent,l,L,len(sample),num_samples=num_samples,use_dists=use_dists,
                              sample_func=sample_func,null_ccdf=null_ccdf)
         return ((exponent,p_value) if not return_all else
                 (exponent,d_ks,S,S_e,unique_s,p_value))


     def plfit_cd_discrete(sample,l,L,fix_exponent=None,use_dists=None,num_samples=1000,return_all=False):
         return plfit_cd(sample,l,L,fix_exponent=fix_exponent, use_dists=use_dists,
                         num_samples=num_samples,return_all=return_all, log_like=discrete_log_likelihood,
                         bounds=[1,10],null_ccdf=discrete_S_pl,sample_func=draw_sample_discrete)
   #+end_src 

   #+RESULTS:

   :END:

*** Exploration                                                 :exploration:

**** critical ehe statistics don't fit for continous power laws
    however, the created avalanche distributions, even in the critical
    case don't a power law with exponent 1.5. In fact the KS distance
    to the empirical /continuos/ power law distribution is so big that
    none of the 1000 synthetid datasets have shown a smaller value!
    #+begin_src ipython :session master :file ./images/20170518_111501_1884MK23.png :cache yes :tangle no
      recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
            pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
      f,axs = plt.subplots(2,2,figsize=(16,12))
      dists = np.array(pickle.load(open("../avalanches/distances1e7l1L1e4e1,5.pickle",'br')))
      for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])):
         avalanches = np.array(recreate_ehe_plot_res[k][0])
         alpha,p_value = plfit_cd(avalanches,1,10000,fix_exponent=3/2,use_dists=dists)
         loglogplot(avalanches,ax)
         ax.set_title(str(k)+" p value " + str(p_value))
     #+end_src

     #+RESULTS[08be7a7646e9aace465cc9af6b564a272456d3e9]:
     [[file:./images/20170518_111501_1884MK23.png]]


     calculate Avalanche statistics between 5 and 1000 for exponent 3/2
     #+begin_src ipython :session master
       import os
       def calc_dists(l,L,alpha,tid,outdir,distfile="../avalanches/distancesl5L1e3N10002.dat",
                      sep=10,null_ccdf=S_pl,sample_func=draw_sample):
           # if not os.path.isfile(distfile):
           #     np.memmap(distfile, dtype='float64', mode='w+', shape=(1000))
           # distances = np.memmap(distfile,dtype='float64',mode='r+',shape=(1000))#np.load(distfile,mmap_mode='r+')
           print('start calculating distances')
           print('tid ',tid,sep*(tid-1),(sep*tid))
           dists = KS_p_value(0.2,3/2,5,1000,int(1e7),num_samples=sep,return_dists=True,sample_func=sample_func)[1]
           print('dists',dists)
           np.save(os.path.join(outdir,"dists"+str(tid)),dists)
     #+end_src 

     #+RESULTS:


     #+begin_src ipython :session master :eval no :tangle no
       qsub("""
       from ehe import *
       from plfit_cd import *
       calc_dists(5,1000,3/2,task_id,outdir,sep=100,null_ccdf=S_pl,sample_func=draw_sample)
       """, post_command="from utils import *;post_command_concat(outdir+'/dists',range(1,201))",
            queue='long',servername="server",name="distancesl5L1e3N1000_pp",task_ids='1-200')#,execute=False) 
     #+end_src 


      #+begin_src ipython :session master :file ./images/20170518_111501_1884MK232.png :cache yes :tangle no
      recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
            pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
      f,axs = plt.subplots(2,2,figsize=(16,12))
      dists = np.load("../avalanches/distancesl5L1e3N1000_pp/distsconcatenated.npy") 
      for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])):
         avalanches = np.array(recreate_ehe_plot_res[k][0])
         alpha,p_value = plfit_cd(avalanches,5,1000,fix_exponent=3/2,use_dists=dists)
         loglogplot(avalanches,ax)
         ax.set_title(str(k)+" p value " + str(p_value))
     #+end_src

     #+RESULTS[56cc6726f295950562efec72abc7424aec3cf610]:
     [[file:./images/20170518_111501_1884MK232.png]]

**** Discrete power laws do fit!

     create surrogate distances 
         #+begin_src ipython :session master :eval no :tangle no
           qsub("""
           from ehe import *
           from plfit_cd import *
           calc_dists(1,10000,3/2,task_id,outdir,sep=10,null_ccdf=discrete_S_pl,sample_func=draw_sample_discrete)
           """, post_command="from utils import *;post_command_concat(outdir+'/dists',range(1,101))",
                queue='long_64gb',servername="server",name="discrete_distancesl1L1e4N1000",task_ids='1-100')#,execute=False) 
     #+end_src 


         #+begin_src ipython :session master :eval no :tangle no
           qsub("""
           from ehe import *
           from plfit_cd import *
           calc_dists(5,10000,3/2,task_id,outdir,sep=10,null_ccdf=discrete_S_pl,sample_func=draw_sample_discrete)
           """, post_command="from utils import *;post_command_concat(outdir+'/dists',range(1,101))",
                queue='long_64gb',servername="server",name="discrete_distancesl5L1e4N1000",task_ids='1-100')#,execute=False) 
     #+end_src 




     Fit only good for l=5 not for l=1
     
*************** TODO Investigate what goes wrong for l=1
                 - biggest deviation always seems to be at index 1....
*************** END
 
      #+begin_src ipython :session master :file ./images/20170518_111501_1884MK23223423.png :cache yes :tangle no
        recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
              pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
        f,axs = plt.subplots(2,2,figsize=(16,12))
        dists = np.load("../avalanches/discrete_distancesl5L1e4N1000/distsconcatenated.npy") 
        for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])): 
           avalanches = np.array(recreate_ehe_plot_res[k][0])
           alpha,p_value = plfit_cd_discrete(avalanches,5,10000,fix_exponent=3/2,use_dists=dists)
           loglogplot(avalanches,ax)
           ax.set_title(str(k)+" p value " + str(p_value))
     #+end_src

     #+RESULTS[1b90b526023a33762d68e9816f21a28cca1eee2b]:
     [[file:./images/20170518_111501_1884MK23223423.png]]

     This result looks promising! The critical EHE avalanche statistics
     had a smaller KS-distance than all 1000 surrogate discrete power
     laws.

*************** TODO Check sampling of discrete power laws for convergence...
*************** END

     The subcritical and supercritical distributions however are
     easily identified and have a bigger KS-distance than all the
     surrogate datasets. At least with 1e7 samples, identification of
     the different regimes is very clear

    
*************** TODO Applicabililty [1/2]
                 CLOSED: [2017-06-04 So 21:21]
              - [-] Check to what extand small deviations from the
                critical alpha value are recognizable using this
                procedure
                - [X] basic checks -> not so much deviations recognizable
                - [ ] More Systematic checks needed ?
              - [X] how does this change with system size and number of avalanches observed
*************** END

     With a smaller number of avalanches, the p values are not black
     and white for the critical case, but the subcritical and critical
     case can be safely discarded.
      #+begin_src ipython :session master :file ./images/20170518_111501_1884MK23223423332.png :cache yes :tangle no
        recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
              pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
        f,axs = plt.subplots(2,2,figsize=(16,12))
        dists = np.load("../avalanches/discrete_distancesl5L1e4N1000/distsconcatenated.npy") 
        for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])):
           avalanches = np.array(recreate_ehe_plot_res[k][0])[:1000]
           alpha,p_value = plfit_cd_discrete(avalanches,5,10000,fix_exponent=3/2)
           loglogplot(avalanches,ax)
           ax.set_title(str(k)+" p value " + str(p_value))
     #+end_src

     #+RESULTS[92c508407680386f9b69ca29c8a6c535932858f5]:
     [[file:./images/20170518_111501_1884MK23223423332.png]]



     #+begin_src ipython :session master :tangle no :eval no
       qsub("""
       def p_values(task_id,outdir,exponent=2.0,recreate_ehe_plot_res=pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb'))):
           resdict = dict()
           for k in recreate_ehe_plot_res.keys():
               avalanches = np.array(recreate_ehe_plot_res[k][0])[100000:100000+task_id]
               alpha,p_value = plfit_cd_discrete(avalanches,5,10000,fix_exponent=exponent)
               resdict[k] = p_value
           pickle.dump(resdict,open(outdir+"/p_value"+str(exponent)+"num_samples"+str(task_id)+".pickle",'wb'))
       p_values(task_id,outdir)
       """,name='p_value_num_samples',task_ids='100000',servername='server')
     #+end_src 


        #+begin_src ipython :session master :file ./images/20170518_111501_1884MK2322342333233.png :cache yes :tangle no
        recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
              pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
        f,axs = plt.subplots(2,2,figsize=(16,12))
        dists = np.load("../avalanches/discrete_distancesl5L1e4N1000/distsconcatenated.npy") 
        for k,ax in zip(recreate_ehe_plot_res.keys(),np.reshape(axs,[-1])):
           avalanches = np.array(recreate_ehe_plot_res[k][0])[:2000]
           alpha,p_value = plfit_cd_discrete(avalanches,5,10000,fix_exponent=2.0)
           loglogplot(avalanches,ax)
           ax.set_title(str(k)+" p value " + str(p_value))
     #+end_src

     #+RESULTS[1ee37d9fba237aed9072a282ea8990c5ccda5317]:
     [[file:./images/20170518_111501_1884MK2322342333233.png]]


***** Dependence on power law exponent
 
     To check the dependence on the chosen power law exponent, this analysis is repeated with exponents 1.55 1.45 and 2
     #+begin_src ipython :session master :eval no :tangle no
       for exp in [1.49,1.51,1.45,1.55,2]:
           qsub("""
       from ehe import *
       from plfit_cd import *
       calc_dists(5,10000,{0},task_id,outdir,sep=10,null_ccdf=discrete_S_pl,sample_func=draw_sample_discrete)
       """.format(exp), post_command="from utils import *;post_command_concat(outdir+'/dists',range(1,101))",
                queue='long_64gb',servername="server",name="discrete_distancesl5L1e4N1000"+str(exp),task_ids='1-100')
      #+end_src 
   


       #+begin_src ipython :session master :file ./images/20170518_111501_1884MK232234233424.png :cache yes :tangle no
         recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
               pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
         f,axs = plt.subplots(2,2,figsize=(16,12))
         for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])):
            avalanches = np.array(recreate_ehe_plot_res[k][0])
            title = str(k)
            for exp in [1.45,1.49,1.55,2]:
               dists = np.load("../avalanches/discrete_distancesl5L1e4N1000"+str(exp)+"/distsconcatenated.npy") 
               alpha,p_value = plfit_cd_discrete(avalanches,5,10000,fix_exponent=float(exp),use_dists=dists)
               title += " exp: " + str(exp) + " p: " +str(p_value)
            loglogplot(avalanches,ax)
            ax.set_title(title)
      #+end_src

      #+RESULTS[0aabca8a0e197c60348fe26f63fe1a857f80e237]:
      [[file:./images/20170518_111501_1884MK232234233424.png]]


      #+begin_src ipython :session master :results output :tangle no :eval no
        for num_samples in list(range(1000,10001,1000))+[100000]:
            res_dict = pickle.load(open('../avalanches/p_value_num_samples/p_value2.0num_samples'+str(num_samples)+'.pickle','rb'))
            print('num_samples ',num_samples)
            print('res_dict ',res_dict)
      #+end_src 

      #+RESULTS:
      #+begin_example
      num_samples  1000
      res_dict  {'subcrit': 1.0, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  2000
      res_dict  {'subcrit': 0.994, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  3000
      res_dict  {'subcrit': 0.982, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  4000
      res_dict  {'subcrit': 0.791, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  5000
      res_dict  {'subcrit': 0.907, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  6000
      res_dict  {'subcrit': 1.0, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  7000
      res_dict  {'subcrit': 1.0, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  8000
      res_dict  {'subcrit': 1.0, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  9000
      res_dict  {'subcrit': 1.0, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  10000
      res_dict  {'subcrit': 1.0, 'crit': 0.0, 'supcrit': 0.0}
      num_samples  100000
      res_dict  {'subcrit': 1.0, 'crit': 0.0, 'supcrit': 0.0}
#+end_example

           #+begin_src ipython :session master :file ./images/20170518_111501_1884MK2322342332423.png :cache yes :tangle no
         recreate_ehe_plot_res = (recreate_ehe_plot_res if 'recreate_ehe_plot_res' in globals() else 
               pickle.load(open('../avalanches/recreate_ehe_plot_res.pickle','rb')))
         f,axs = plt.subplots(2,2,figsize=(16,12))
         dists = np.load("../avalanches/discrete_distancesl5L1e4N1000/distsconcatenated.npy") 
         for k,ax in zip(['crit','subcrit','supcrit','supsupcrit'],np.reshape(axs,[-1])): 
            avalanches = np.array(recreate_ehe_plot_res[k][0])
            dks,S,S_e,unique_s = dist_KS(3/2,5,10000,avalanches,null_ccdf=discrete_S_pl,return_ccdf=True)
            ax.plot(S)
            ax.plot(S_e)
            if k == 'crit':
                ax.set_xlim(0,100)
            ax.set_title(str(k)+" KS_dist " + str(dks))
      #+end_src

      #+RESULTS[2c1f2cbc73bfb59b38c540b0d5c2cadd8d3ae84f]:
      [[file:./images/20170518_111501_1884MK2322342332423.png]]

** ignore                                                            :ignore:

   For the avalanche sizes, one has to use the null hypothesis of
   discrete power laws, with a large sample size of \(10^7 \) no
   continous synthetic sample shows a larger deviation from the
   continous truncated power law than the avalanches from the EHE
   model, even in the critical case. 

   Testing explicitly against the discrete power law statistics, the
   situation looks different. Figure [[ref:fig:ehe-dpl-fit]] shows that
   there is absolutely no evidence against the hypothesis that the
   avalanche distribution in the critical case follows a discrete
   power law, while the deviations in the other regimes clearly
   indicate differently.

   #+CAPTION: \label{fig:ehe-dpl-fit} Goodness of fit p values against a discrete power law with exponent 1.5 and cutoffs 5 and 10000 for the different regimes.
   [[file:./images/20170518_111501_1884MK23223423.png]]

   
   Even for fewer points, this can be clearly seen in figure [[ref:fig:gof-few-data]].
   
   #+CAPTION: \label{fig:gof-few-data} Fit to discrete power law with sample size of 1000
   [[file:./images/20170518_111501_1884MK23223423332.png]]

   However, the goodness of fit for power laws truncated to this size
   are not very sensitive to changes in the exponent. Figure [[ref:fig:gof-exponent]] 
   shows that the fit is considered very good for an exponent in the range \(1.5 \pm 0.05 \).

   #+CAPTION: \label{fig:gof-exponent} Goodness of fit p value in dependence on exponent
   [[file:./images/20170518_111501_1884MK232234233424.png]]

** Finite Size Scaling
   The upper cutoff of the avalanche distributions of networks with
   varying sizes should scale with some exponent under the hypotheses
   that it is really a power law. In particular rescaled avalanche
   distributions should then lie on one curve! For a truncated power law with
   exponent \(\frac{3}{2} \) the rescaled curves are given by
  
   \[\tilde{f}(x) = N^{\frac{3}{2}}f(\frac{x}{N}) \text{ .}\]

   This finite size scaling relation can be seen for the normal EHE Model in figures [[ref:fig:avs-distr-different-n]]
   and [[ref:avs-sizes-rescaled]].
   
   
   :implementation:
   #+begin_src ipython :session master :eval no :tangle no
     qsub("""
     ehe_arma = load_module('ehe_arma')
     Ns = [100,300,500,700,1000,3000,5000,7000,10000]
     N = Ns[task_id-1]
     avs,avd = ehe_arma.simulate_model_const(np.random.random(N),int(1e8),(1-1/np.sqrt(N))/N,deltaU)
     np.save(outdir+"/avs"+str(N),avs)
     np.save(outdir+"/avd"+str(N),avd)
     """,name='finite_size_scaling',servername='server',task_ids='6-7',queue='long')
   #+end_src 

   #+begin_src ipython :session master
     def scale_avalanches(exponent,avalanches,N):
         """returns pdf of scaled avalanches that can be plotted with loglogplot from_pdf=True"""
         unique,counts = np.unique(avalanches,return_counts=True);
         norm_counts = counts/np.sum(counts)
         norm_counts = np.power(N,exponent)*norm_counts
         unique = unique/N
         return (norm_counts,unique)
   #+end_src 

   #+RESULTS:

   #+begin_src ipython :session master :eval no
     Ns = [700,1000,3000,7000,10000]
     #avalanches = [np.load("/media/selfmount/avalanches/finite_size_scaling/avs"+str(N)+".npy") for N in Ns]
     f,ax = plt.subplots()
     # for avs in avalanches:
     #     loglogplot(avs,ax=ax)
     #plt.savefig('../images/avalache_different_sizes.png')

     for (avs,N) in zip(avalanches,Ns):
         pdf = scale_avalanches(1.5,avalanches,N)
         loglogplot(pdf,ax=ax,from_pdf=True)
         ax.set_label(str(N))
     #plt.savefig('../images/finite_size_scaling.png')
   #+end_src
   :END:

   #+CAPTION:\label{fig:avs-distr-different-n} Distribution of avalanche sizes in deendence of system size.
   [[file:images/avalache_different_sizes.png]]
#+CAPTION: \label{avs-sizes-rescaled} Rescaled by \(N^{1.5}f(\frac{x}{N})\), the avalanche distributions collapse to one line.
   
[[file:images/finite_size_scaling.png]]
* Alternative Investigation: EHE Model for general subnetworks  :exploration:
  :PROPERTIES:
  :header-args: :tangle src/ehe_subnetworks.py
  :END:

  Replacing the constant coupling weights \(\frac{\alpha}{N}\) by a
  general weight matrix \(W \) subnetworks of different coupling
  characteristics can be embedded in a network of EHE model units.

  An avalanche of the EHE model gives rise to avalanches in the
  subnetworks if units of the subnetworks are active during the
  avalanche. An avalanche in a subnetworks of units \(U\) of size \(M \)
  and duration \(D\) is observed is for the D consecutive steps of the
  avalanche always at least one element of the subnetwork \(U \) is
  active and the sum of active elements of \(U \) over this time steps
  is \(M \). Note that when no element of \(U \) spikes in a time step
  during the avalanche between two steps that have an active unit in \(U
  \), this gives rise to two separate avalanches in the subnetwork (causal avalanches).

*************** TODO Avalanches Causal or not?
             - logically causal avalanches are what is happening in a subnetwork
             - But, for coincidence detection, noncausal avalanches are more appropriate!
*************** END
  

  To systematically study different strategies for embedding critical
  subnetworks, the possible subnetwork structures have to  be specified. 
  The parameters of interests are 

  1. The total Number \(N\) of units.
  2. The Number \(M \) of subnetworks
  3. The sizes and overlaps of the subnetworks. In this thesis, only
     pairwise overlaps will be important. Thus this gives rise to a
     symmetric overlap matrix \(O \in \mathbb{R}^{M\times M}\) with
     The number of units in each subnetwork along the diagonal and
     \(O_{i,j} \) denoting number of units that are in both subnetworks \(i,j
     \leq M \). Note that \(\sum_{j=1,j\neq i}^MO_{i,j} \leq O_{i,i}\).

  
  :implementation:
  The first step is generating an assignment to subnetworks given the parameters
  \((N,O)\).

  #+begin_src ipython :session master
    from collections import defaultdict  
    import matplotlib.pyplot as plt
    import numpy as np
    from ehe import *
    from utils import *
    def subnetwork_assignments(N,O):
          """generates subnetwork assignment of N units to M subnetworks with
             overlap matrix O of shape (M,M). Returns a vector A of tuples
             with A[i] := units belonging to subnetwork i and a dictionary
             U with U[i] := list of subnetworks that unit i belongs to
          """
          O = np.array(O)
          assert ((O.sum(axis=1) - 2*np.diag(O)) <= 0).all(),'Not implemented yet'
          A = [()]*O.shape[0]
          U = defaultdict(lambda: ())
          remaining_units = np.arange(N)
          for i in range(O.shape[0]):
                #print("i",i)
                sel = []
                # choose number of units overlapping with network j < i
                for j in range(i):
                      #print("select ",O[i,j], " units from ",A[j])
                      sel.extend(np.random.choice(A[j],size=O[i,j],replace=False))
                      # choose remaining units from units not chosen jet
                #print('select ',O[i,i]-len(sel),' units from remaining ',remaining_units)
                sel.extend(np.random.choice(remaining_units,size=O[i,i]-len(sel),replace=False))
                #print("sel for i ",i," is ",sel)
                A[i] = sel
                remaining_units = np.setdiff1d(remaining_units,sel)
          return A
  #+end_src 

  #+RESULTS:

  #+begin_src ipython :session master :results output
print(subnetwork_assignments(10,np.array([[4,2,2],[2,4,2],[2,2,5]])))
  #+end_src 

  #+RESULTS:
  : [[3, 5, 7, 6], [3, 7, 2, 4], [7, 5, 2, 4, 9]]

  :END:

  Given a  iSubnetwork assignment, the task is to construct a weight
  matrix so that under the dynamics of the EHE Model the avalanche
  statistics an activated network becomes critical without spreading
  to overlapping not activated networks and being robust to randomly
  selected background units that also fire. 


:implementation:
  #+begin_src ipython :session master
    def subnetwork_connectivity_matrix(N,O,A=None,c=1):
        """Generates from the subnetwork assignment A a connectivity matrix
           TODO Works only for subnetworks of same size in the moment"""
        W = np.zeros((N,N))
        O = np.array(O)
        if A is None:
            A = subnetwork_assignments(N,O)
        # first recurrent critical weights for all subnetworks
        for i,S_i in enumerate(A):
            W[np.ix_(S_i,S_i)] = ehe_critical_weights(O[i,i])
        # Now construct inhibitory connections between pairwise overlapping subnetworks
        for i in range(len(A)):
            for j in range(len(A)):
                if i==j:
                    continue
                elif O[i,j] > 0:
                    units_not_in_j = np.setdiff1d(A[i],A[j])
                    units_not_in_i = np.setdiff1d(A[j],A[i])
                    #print(i,j,units_not_in_j)
                    W[np.ix_(units_not_in_i,units_not_in_j)] = - c*(O[i,j]/(O[i,i]-O[i,j]))*(1 - 1/np.sqrt(O[i,i]))/O[i,i]
        return W
  #+end_src 

  #+RESULTS:

  #+begin_src ipython :session master
    def subnetwork_connectivity_matrix_2(N,O,A=None,c=1):
        """Generates from the subnetwork assignment A a connectivity matrix
           TODO Works only for subnetworks of same size in the moment"""
        class Wrapper():
            def __init__(self,t):
                self.t = t
            def __repr__(self):
                return t.__repr__()
            def __add__(self,other):
                if type(other) == type(self):
                    return Wrapper(self.t + other.t)
                return Wrapper(self.t + (other,))
            def extract_mean(self):
                if len(self.t) == 0:
                    return 0
                else:
                    return np.mean(self.t)

        vmean = np.vectorize(lambda x:x.extract_mean())
        W = np.full((N,N),Wrapper(()))
        O = np.array(O)
        if A is None:
            A = subnetwork_assignments(N,O)
            # first recurrent critical weights for all subnetworks
        for i,S_i in enumerate(A):
            W[np.ix_(S_i,S_i)] += ehe_critical_weights(O[i,i])
            # Now construct inhibitory connections between pairwise overlapping subnetworks
        W = vmean(W)
        for i in range(len(A)):
            for j in range(len(A)):
                if i==j:
                    continue
                elif O[i,j] > 0:
                    units_not_in_j = np.setdiff1d(A[i],A[j])
                    units_not_in_i = np.setdiff1d(A[j],A[i])
                    #print(i,j,units_not_in_j)
                    overlap = list(set(A[i]).intersection(set(A[j])))
                    W[np.ix_(units_not_in_i,units_not_in_j)] += - c*(O[i,j]/(O[i,i]-O[i,j]))*np.mean(W[np.ix_(overlap,overlap)])
        return W

  #+end_src 

  #+RESULTS:



  #+begin_src ipython :session master
    def simulate_subnetwork(W,A,activated=[0],num_background=30,num_avs=10000,deltaU=deltaU,background_contrast=1):
        """ A is the subnetwork assignment """
        e = load_module('ehe_detailed').EHE()
        activated_inds = np.hstack(A[i] for i in activated)
        background_inds = np.random.choice(np.setdiff1d(np.arange(W.shape[0]),activated_inds),num_background)
        external_weights = np.zeros(W.shape[0],dtype=int)
        external_weights[activated_inds ] = 1
        external_weights[background_inds] = background_contrast
        print('external_weights ',external_weights,flush=True)
        e.simulate_model_mat(np.random.random(W.shape[0]),num_avs,W,deltaU,external_weights)
        return {'ehe':e,'activated_inds':activated_inds,'background_inds':background_inds}

  #+end_src 

  #+RESULTS:
:END:

** First example results for simple connectivity matrix      :implementation:
   #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iBs.png :tangle no
     f,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,0],[0,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix(N_tot,O,A)
     activated = [0]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax1)
     ax1.set_title('avs distribution in activated subnetwork')
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax2)
     ax2.set_title('avs distribution in non activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax3)
     ax3.set_title('corresponding avs distribution')
   #+end_src 

   #+RESULTS[a9ee269e1eba48a0cf87891a42a52d96e1615e6d]:
   [[file:./images/20170721_165930_2464iBs.png]]

  

   #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iB34s.png :tangle no
     f,ax = plt.subplots(2,2,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,100],[100,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix(N_tot,O,A)
     activated = [0]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax[0,0])
     ax[0,0].set_title('avs distribution in activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[0,1])
     ax[0,1].set_title('corresponding avs distribution')
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1])-set(A[0]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax[1,0])
     ax[1,0].set_title('avs distribution in non activated subnetwork')
     E3 = load_module('ehe_detailed').EHE()
     N=O[1,1];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_2),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[1,1])

   #+end_src 

   #+RESULTS[90c695468be15bf098ac22c23f5c5c512d0a94ad]:
   [[file:./images/20170721_165930_2464iB34s.png]]

   #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iB34s2342.png :tangle no
     f,ax = plt.subplots(2,2,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,50],[50,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix(N_tot,O,A)
     activated = [0]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs,num_background=0)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax[0,0])
     ax[0,0].set_title('avs distribution in activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[0,1])
     ax[0,1].set_title('corresponding avs distribution')
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1])-set(A[0]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax[1,0])
     ax[1,0].set_title('avs distribution in non activated subnetwork')
     E3 = load_module('ehe_detailed').EHE()
     N=O[1,1];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_2),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[1,1])

   #+end_src 

   #+RESULTS[cc70beb7a61240f6dfd4742539799741a48fed8f]:
   [[file:./images/20170721_165930_2464iB34s2342.png]]

   Test with scale parameter of the crosstalk inhibition
     #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iB34s2342234.png :tangle no
     f,ax = plt.subplots(2,2,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,50],[50,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix(N_tot,O,A)
     activated = [0]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs,num_background=0)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax[0,0])
     ax[0,0].set_title('avs distribution in activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[0,1])
     ax[0,1].set_title('corresponding avs distribution')
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1])-set(A[0]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax[1,0])
     ax[1,0].set_title('avs distribution in non activated subnetwork')
     E3 = load_module('ehe_detailed').EHE()
     N=O[1,1];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_2),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[1,1])

   #+end_src 
   
   #+RESULTS[5fd8d7593997e90df9acc054611a6f47aece29c7]:
   [[file:./images/20170721_165930_2464iB34s2342234.png]]
  

   #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iB34s2342434.png :tangle no
     f,ax = plt.subplots(2,2,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,50],[50,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix(N_tot,O,A,c=2)
     activated = [0,1]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs,num_background=0)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax[0,0])
     ax[0,0].set_title('avs distribution in activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[0,1])
     ax[0,1].set_title('corresponding avs distribution')
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax[1,0])
     ax[1,0].set_title('avs distribution in other activated subnetwork')
     E3 = load_module('ehe_detailed').EHE()
     N=O[1,1];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_2),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[1,1])

   #+end_src 

   #+RESULTS[725e165002cd5a7c46903460b1ceee88339affe0]:
   [[file:./images/20170721_165930_2464iB34s2342434.png]]

   #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iB34s2342434234.png :tangle no
     f,ax = plt.subplots(2,2,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,100],[100,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix_2(N_tot,O,A,c=2)
     activated = [0]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs,num_background=0)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax[0,0])
     ax[0,0].set_title('avs distribution in activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[0,1])
     ax[0,1].set_title('corresponding avs distribution')
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax[1,0])
     ax[1,0].set_title('avs distribution in non activated subnetwork')
     E3 = load_module('ehe_detailed').EHE()
     N=O[1,1];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_2),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[1,1])

   #+end_src 

   #+RESULTS[acdc47d570505dfe4dac7f9b62ac2c440dc38ab7]:
   [[file:./images/20170721_165930_2464iB34s2342434234.png]]

   
     #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iB34s2342434234234.png :tangle no
     f,ax = plt.subplots(2,2,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,100],[100,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix(N_tot,O,A,c=1)
     activated = [0]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs,num_background=0)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax[0,0])
     ax[0,0].set_title('avs distribution in activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[0,1])
     ax[0,1].set_title('corresponding avs distribution')
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax[1,0])
     ax[1,0].set_title('avs distribution in non activated subnetwork')
     E3 = load_module('ehe_detailed').EHE()
     N=O[1,1];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_2),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[1,1])

   #+end_src 

   #+RESULTS[0dbcbe6cc7a7ae60d22ebd9d0333de91eddaa3c2]:
   [[file:./images/20170721_165930_2464iB34s2342434234234.png]]


   #+begin_src ipython :session master :cache yes :file ./images/20170721_165930_2464iB34s2342434234.png :tangle no
     f,ax = plt.subplots(2,2,figsize=(16,12))
     N_tot = 1000
     num_avs = 10000
     O = np.array([[500,350],[350,400]])
     A = subnetwork_assignments(N_tot,O)
     W = subnetwork_connectivity_matrix(N_tot,O,A)
     activated = [0,1]
     res =  simulate_subnetwork(W,A,activated=activated,num_avs=num_avs,num_background=0)
     E1 = res['ehe']
     (sub_avs_detailed_1,sub_avs_indices_1) = E1.subnetwork_avalanches(set(A[0]))
     (sub_avs_sizes_1,sub_avs_durations_1) = E1.get_avs_size_and_duration(sub_avs_detailed_1,sub_avs_indices_1)
     loglogplot(sub_avs_sizes_1,ax=ax[0,0])
     ax[0,0].set_title('avs distribution in activated subnetwork')
     E2 = load_module('ehe_detailed').EHE()
     N=O[0,0];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_1),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[0,1])
     ax[0,1].set_title('corresponding avs distribution') 
     (sub_avs_detailed_2,sub_avs_indices_2) = E1.subnetwork_avalanches(set(A[1]))
     (sub_avs_sizes_2,sub_avs_durations_2) = E1.get_avs_size_and_duration(sub_avs_detailed_2,sub_avs_indices_2)
     loglogplot(sub_avs_sizes_2,ax=ax[1,0])
     ax[1,0].set_title('avs distribution in non activated subnetwork')
     E3 = load_module('ehe_detailed').EHE()
     N=O[1,1];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_2),(1-1/np.sqrt(N))/N,deltaU)
     loglogplot(E2.avs_sizes,ax=ax[1,1])

   #+end_src 

   #+RESULTS[8c173460ebb992be9879c4deafc903bc0be80797]:
   [[file:./images/20170721_165930_2464iB34s2342434234.png]]


   See how avalanches proceed! Turn the avs format into indicators to which network the units belong
   #+begin_src ipython :session master
     def subnet_avs_inspector(A,avs):
         res = np.empty(avs.shape,dtype=object)
         for i,a in enumerate(avs):
             if a == np.iinfo(avs.dtype).max:
                 res[i] = a
             else:
                 res[i] = tuple(k for k in range(len(A)) if a in A[k])
         return res
   #+end_src 

   #+RESULTS:

   #+begin_src ipython :session master
     import matplotlib.pyplot as plt
     from ehe import ehe_critical_weights,deltaU
     from utils import load_module
     def test_subnet_connectivity(N_tot,num_avs,O,activated,num_background,c=1):
         f,ax = plt.subplots(O.shape[0],2,figsize=(8*O.shape[0],12))
         A = subnetwork_assignments(N_tot,O)
         W = subnetwork_connectivity_matrix(N_tot,O,A,c)
         res = simulate_subnetwork(W,A,activated=activated,num_avs=num_avs,num_background=num_background)
         res['A'] = A
         res['W'] = W
         for i in range(O.shape[0]):
             E1 = res['ehe']
             (sub_avs_detailed_i,sub_avs_indices_i) = E1.subnetwork_avalanches(set(A[i]))
             (sub_avs_sizes_i,sub_avs_durations_i) = E1.get_avs_size_and_duration(sub_avs_detailed_i,sub_avs_indices_i)
             loglogplot(sub_avs_sizes_i,ax=ax[i,0])
             ax[i,0].set_title('avs distribution in '+ ('activatet' if i in activated else 'non-activated') +' subnetwork '+str(i))
             E2 = load_module('ehe_detailed').EHE()
             N=O[i,i];E2.simulate_model_const(np.random.random(N),len(sub_avs_sizes_i),(1-1/np.sqrt(N))/N,deltaU)
             loglogplot(E2.avs_sizes,ax=ax[i,1])
             ax[0,1].set_title('corresponding avs distribution')
         return f,res
   #+end_src 

   #+RESULTS:



   #+begin_src ipython :session master :tangle no
     f,res = test_subnet_connectivity(100,10000,np.array([[30,10],[10,60]]),[0],10)

   #+end_src 


   #+RESULTS:
   : <matplotlib.figure.Figure at 0x7f2ddcfed438>


   #+begin_src ipython :session master :eval no :tangle no
     qsub("""
     import pickle
     f,res = test_subnet_connectivity(10000,100000,np.array([[1000,250,300],[250,2000,500],[300,500,3000]]),
                                     [[0],[0,1],[0,2],[0,1,2]][task_id-1],1000,c=2)
     f.savefig(outdir+'/avalanches'+str(task_id)+'.png')
     res['figure'] = f
     res['avs'] = res['ehe'].avs
     res['avs_inds'] = res['ehe'].avs_inds
     del res['ehe']
     pickle.dump(res,open(outdir+'/results'+str(task_id)+'.pickle','wb'))
     """,servername='server_inline',name='test_subnet_connectivity_background_1000,c=2',queue='long_64gb',task_ids='1-4')
   #+end_src 
* Old cluster jobs                                       :exploration:ignore:
  

   - Check whether simulate_model_mat is consisent whith simulate_model_const
      #+begin_src ipython :session master :eval no :tangle no
      qsub("""
      from ehe import *;
      import numpy as np;
      ehe_arma = load_module('ehe_arma')
      N = 10000;
      avs,avd = ehe_arma.simulate_model_mat(np.random.random(N),1000*N,ehe_critical_weights(N),deltaU)
      np.save(outdir+"/avs"+str(task_id),avs)
      np.save(outdir+"/avd"+str(task_id),avd)
      """,queue='maximus@fatbastard',name='ehe_mat_crit',servername='server')
    #+end_src 


       #+begin_src ipython :session master :tangle no :eval no
         qsub("""
         from ehe import *;
         ehe_arma = load_module('ehe_detailed');
         N = 10000
         avs,avd = ehe_arma.simulate_model_const(np.random.random(N),int(1e7),0.99/N,deltaU)
         np.save(outdir+"/avs"+str(task_id),avs)
         np.save(outdir+"/inds"+str(task_id),avd)
         """, name="const_crit_avalanches_detailed_10000",servername="server_inline")
    #+end_src 



    switch backend to qt for better inspection of figures
     #+begin_src ipython :session master :tangle no :exports none
    %matplotlib qt
    #+end_src

    #+RESULTS:

* ignore                                                             :ignore:
\clearpage
\addcontentsline{toc}{section}{References}   
[[bibliographystyle:plainnat]]
[[bibliography:/home/kima/Dropbox/emacs/bibliography/references.bib]]
* Further explorations                                          :exploration:
#+begin_src ipython :session master :tangle no :eval no
  esrap = pickle.load(open('/home/kima/Dropbox/uni/master/bernstein_results/avalanche_distributions/k100/evaluated_sim_res_avalanche_plots100,54,100.pickle','rb'))
  esrap['all_avalanche_figure'].show()
  ax = plt.gca()
  avs_figure = ax.lines[0].get_xydata()
  avs_background = ax.lines[1].get_xydata()
  avs_ref = ax.lines[2].get_xydata()
  savemat('/home/kima/Dropbox/uni/master/bernstein_results/avalanche_distributions/k100/avalanche_plot100,54,100',{'avs_figure':avs_figure,'avs_background':avs_background,'avs_ref':avs_ref})
#+end_src 


#+begin_src ipython :session master :tangle no :eval no
  from scipy.io import savemat
  plt.ioff()
  esr = pickle.load(open('/home/kima/Dropbox/uni/master/bernstein_results/2_afc_classifier_accuracy/background_contrast/k100/evaluated_sim_res100,54,100.pickle','rb'))
  ath = np.array(esr['accuracies_thresholds'])
  savemat('/home/kima/Dropbox/uni/master/bernstein_results/2_afc_classifier_accuracy/background_contrast/k100/accuracies_thresholds100,54,100',{'accuracies_thresholds':ath})
#+end_src 

#+RESULTS:


#+begin_src ipython :session master :tangle no :eval no
  Nu = 1000
  Ns = 100
  Ne = 50
  K  = 100
  ens = embed_ensembles(N_tot=Nu,N_subunits=Ns,N_ensembles=Ns)
  W = simple_submatrix(Nu,ens)
  from scipy.io import savemat

  savemat('ssm1000_100_50_100',{'W':W})
#+end_src 


#+begin_src ipython :session master :tangle no
  num_avs_a = np.zeros((10,10),dtype=int)
  num_avs_b = np.zeros((10,10),dtype=int)
  for ens in range(10):
      for act in range(10):
          num_avs_a[ens,act] = len(sim_res['sim_res'][ens]['sim_res_a'][act][1])
          num_avs_b[ens,act] = len(sim_res['sim_res'][ens]['sim_res_b'][act][1])

  ens_idx = 6
  act_idx = 0

  ens = sim_res['ensembles'][ens_idx]
  num_ovl = [sum(1 if i in ens[j] else 0 for j in range(len(ens))) for i in range(1000)]


  act = sim_res['sim_res'][ens_idx]['figure_units'][act_idx]
  background = sim_res['sim_res'][ens_idx]['background_units'][act_idx]
  ovl_act = np.array([len((set(ens[i])&(set(act) | set(background)))) for i in range(len(ens))])
  pw_ovl = np.array([[len((set(ens[i])&set(ens[j]))) for j in range(len(ens))] for i in range(len(ens))])

  def input_to_firing_units(W,avs):
      """avs is only one avalanche"""
      step_seps = np.where(avs == 65535)[0]
      inp = []
      for step in np.split(avs,step_seps):
          arrint = np.array(step[1:] if step[0] == 65535 else step)
          Wsub = W[np.ix_(arrint,arrint)]
          inp.append([Wsub @ np.ones((Wsub.shape[0],))])
      return inp

  def split_in_steps(avs):
      step_seps = np.where(avs == 65535)[0]
      return [s[1:] if s[0] == 65535 else s for s in np.split(avs,step_seps)]
#+end_src 

#+RESULTS:



#+begin_src ipython :session master :tangle no
params = np.array([[(Ns,Ne,100) for Ns in range(80,120,4)] for Ne in range(40,71,2)],dtype='i8,i8,i8')
exp_ovl = np.array([[params[i,j][0]*params[i,j][1]/1000 for j in range(params.shape[1])] for i in range(params.shape[0])])
#+end_src 

#+RESULTS:


#+begin_src ipython :session master :tangle no
  from scipy.io import savemat
  plt.ioff()
  esr = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/evaluated_sim_res100,50,100.pickle','rb'))
  ath = np.array(esr['accuracies_thresholds'])
  savemat('/home/kima/Dropbox/uni/master/bernstein_results/2_afc_classifier_accuracy/background_contrast/k100/accuracies_thresholds100,50,100_T1000',{'accuracies_thresholds_biggerT':ath})
#+end_src 


#+begin_src ipython :session master :file ./images/20171010_123800_2039uDA.png :tangle no
plt.figure(figsize=(10,10))
plt.imshow(Wsub2)
plt.colorbar()
#+end_src 

#+RESULTS:
[[file:./images/20171010_123800_2039uDA.png]]


#+begin_src ipython :session master :file ./images/20171018_113707_18489_1.png :tangle no
plt.imshow(psm) 
#+end_src 

#+RESULTS:
[[file:./images/20171018_113707_18489_1.png]]


#+begin_src ipython :session master
%matplotlib qt
#+end_src 

#+RESULTS:


  #+begin_src ipython :session master :file ./images/20170622_123227_1871Jvwe3r234242222.png :tangle no :cache yes
    ehe = load_module('ehe_detailed')
    N = 2;
    alpha=(1-1/np.sqrt(N))/N;
    e=ehe.EHE();
    e.record_units=True;
    e.simulate_model_mat(np.random.random(2),2800000,alpha*np.array([[1.5,1],[1,1.3]]),deltaU*2,[1,10]);
    uh = e.unit_hist;
    plt.hist2d(uh[:,0],uh[:,1],bins=1000,range=[[0,1],[0,1]])
    plt.colorbar()
  #+end_src 

  #+RESULTS[955c155c9b89f02a45ac57db3ecf0824ca45923c]:
  [[file:./images/20170622_123227_1871Jvwe3r234242222.png]]







  #+begin_src ipython :session master :file ./images/20171116_181046_1902xKs.png :tangle no
  plt.figure(figsize=(20,20))
  plt.imshow(psm,origin='lower')
  #+end_src 

  #+RESULTS:
  0[[file:./images/20171116_181046_1902xKs.png]]
  #+begin_src ipython :session master :file ./images/20171116_181046_1902xKsadfa.png :tangle no
  plt.figure(figsize=(20,20))
  plt.imshow(psm_beta3,origin='lower')
  #+end_src 

  #+RESULTS:
  
[[file:./images/20171116_181046_1902xKsadfa.png]]
  plt.figure(figsize=(20,20))
  plt.imshow(psm,origin='lower')
  #+end_src 



  #+begin_src ipython :session master
  plt.imshow(psm)
  #+end_src 

  #+RESULTS:
  : <matplotlib.figure.Figure at 0x7f73b833d128>


  #+begin_src ipython :session master :file ./images/20171128_082731_29324r4v.png :tangle no
  plt.loglog(pdfs[100][0])
  #+end_src 

  #+RESULTS:
  [[file:./images/20171128_082731_29324r4v.png]]

  #+begin_src ipython :session master
  %matplotlib qt
  #+end_src

  #+RESULTS:


  #+begin_src ipython :session master :file ./images/20171130_181000_135669cD.png :tangle no
  plt.imshow(statpsm[0],origin='lower');plt.colorbar()
  #+end_src 

  #+RESULTS:
  [[file:./images/20171130_181000_135669cD.png]]


 #+begin_src ipython :session master :file ./images/20171130_181000_1356692cD.png :tangle no
     psm[psm==0]=1
     plt.imshow(psm[:-1,:]);plt.colorbar()
  #+end_src 

  #+RESULTS:
  [[file:./images/20171130_181000_1356692cD.png]]

  

  #+begin_src ipython :session master
  %matplotlib qt
  #+end_src 

  #+RESULTS:



  #+begin_src ipython :session master
  f,ax = plt.subplots(1,1)
  sim_res = pickle.load(open('/home/kima/Dropbox/uni/master/avalanches/sim_res100,50,100.pickle','rb'))
  avs_sizes_ens = coactivated_avs_distribs(sim_res,9,9)
  avs_sizes_ens.sort(key=lambda x: x[1],reverse=True)
  #+end_src

  #+RESULTS:

  #+begin_src ipython :session master
  for (avs_sizes,ovl) in avs_sizes_ens: 
      loglogplot(avs_sizes,from_pdf=False,ax=ax,label=str(ovl))
  plt.legend()    
  #+end_src 

  #+RESULTS:
  : <matplotlib.legend.Legend at 0x7f4a2fbc5630>

  
  #+begin_src ipython :session master
  def mean_pdf(pdf):
      norm_counts,unique = pdf
      return norm_counts.dot(unique)
  #+end_src 

  #+RESULTS:



  #+begin_src ipython :session master
  f,ax1 = plt.subplots(1,1)
  ovl = [x[1] for x in avs_sizes_ens]
  means = [mean_pdf(points2pdf(x[0])) for x in avs_sizes_ens]
  ax1.scatter(ovl,means)
  ax1.set_xlabel('overlap with figure network')
  ax1.set_ylabel('mean of avalanche size distribution')
  #+end_src 

  #+RESULTS:
  : <matplotlib.collections.PathCollection at 0x7f4a3198f1d0>

